[{"title":"监控驱动的故障排查：利用PMQL定位集群问题","url":"/2025/09/13/监控驱动的故障排查：利用PMQL定位集群问题/","content":"\n## 背景\n\n> 起因是钉钉群内发现大量告警\n\n查看集群状态，发现node状态在ready和notready之间反复横跳，有问题的node都是某地区IDC机房的节点，<font style=\"color:rgb(44, 44, 54);\">集群跟该地区IDC节点组网使用基于WireGuard协议的应用，节点间通信依赖公网出口。</font>\n\n## 问题定位\n\n### agent组网节点检查\n\n检查组网节点是否工作正常\n\n发现是agent节点出现无法连接中继节点或者直连master节点，所以有可能是网络问题\n\n![](./tailscaled.png)\n\n### 机房公网流量检查\n\n查看机房公网流量出口，发现公网的出流量把带宽都占满了，可以确定是有人把公网出流量占满了，导致集群组网出现问题，从而出现节点notready的情况\n\n![](./outbound.png)\n\n## 排查\n\n集群内的公网流量分为：\n\n+ 内部流量：各个pod、node之间相关访问的流量（因为有组网，所以这些内部流量也是走公网出口）\n+ 外部流量：集群中的工作负载通过网关访问公网\n\n### 内部流量排查\n\n集群中内部组网使用了agent节点，所以该机房中的内部流量都会走该agent出去，查看其流量趋势\n\n![](./grafana.png)\n\n发现流量数据还算正常，排除集群内大流量的问题\n\n### 外部流量排查\n\n集群使用kube-ovn CNI，外部公网流量分为集中式网关和分布式网关两种方式。\n\n+ 集中式网关用的是 vpc-nat-gateway + multus-cni ，它会把节点上面的网卡加入到对应pod上面，所以可以通过查看对应节点的网卡流量情况得出该网关的流量情况。\n\n![](./kube-ovn-1.png)\n\n+ 分布式网关是pod访问公网时，流量会经过pod所在节点的网卡，走节点自身的网络。\n\n![](./kube-ovn-2.png)\n\n因为集群里面的节点都安装了node-exporter，所以可以使用pmql来定位哪个节点的网卡出流量最多\n\n```plain\n# 3小时内，平均发送数据(MB)top5的网卡信息\ntopk(5, sum by (instance,device) (rate(node_network_transmit_bytes_total{device!~\"lo|veth.*|docker.*|br.*\"}[3h]))) / 1024 / 1024\n```\n\n![](./pmql.png)\n\n结果发现有一张网卡的流量十分高，定位到是一个集中式网关的节点，上面的网卡被用来做网关的公网出口网卡了，所以可以确定大流量的应用就在这个网关后面。\n\n在集群中使用集中式网关的都会给它分配一个EIP资源，同时集群里面也有对应的exporter来收集EIP的流量使用情况，所以可以继续使用对应的pmql来定位对应的EIP资源。\n\n```plain\n# 内部查看eip流量的pmql，仅供参考\ntopk(5, sum by (eipName) (rate(xxx_eip_network_traffic_egress_bytes{xxx}[3h]))) / 1024 / 1024\n```\n\n在成功定位到有问题的EIP后，后台马上找到了对应的资源，是一台裸金属。\n\n这台裸金属在1个月前因初始配置不当导致未授权访问，并被修改了密码，在修改密码后就可以登录回去了。但是因为上面有一些虚拟机应用在跑，所以没有选择重装系统，而是重装了部分核心软件，并检查和清除相关入侵痕迹。所以可以确定是裸金属被黑了导致出流量激增，不是因为相关业务。\n\n#### 处理\n\n这里处理分为两步：\n\n+ 对应的同学备份虚拟机和系统上面的重要数据，确保系统可以重装\n+ 对裸金属进行临时处理，并且看一下是否可以找到对应的问题\n\n### 裸金属处理\n\n#### 流量查看\n\n通过`iftop`查看实时流量，发现随机向多个IP发送大流量数据\n\n![](./iftop.png)\n\n使用tcpdump进行抓包查看相关数据，发现流量会访问目标ip的80或443端口，并且会一直发送SYN包\n\n![](./tcpdump.png)\n\n综上所述，这台服务器是被人拿来做肉鸡，对公网服务发起SYN洪水攻击\n\n#### 临时处理\n\n为避免进一步影响业务，临时用tc给系统限速，确保不要再发送大流量把系统公网带宽占满。要求：业务ip流量不限制（ssh登录ip、虚拟机的业务流量），只限制异常流量（所有ip的80、443端口出流量）。\n\n```plain\n# 清空对应网卡旧规则\n# tc qdisc del dev enp1s0 root\n\n# 添加qdisc，并且默认归类为 1:20 的规则（没有匹配到规则的流量走 1:20 ）\n# tc qdisc add dev enp1s0  root handle 1: htb default 20\n\n# 添加class规则\n# tc class add dev enp1s0  parent 1: classid 1:1   htb rate 20Mbit ceil 20Mbit burst 15k\n# 异常流量限速规则\n# tc class add dev enp1s0 parent 1:1 classid 1:10 htb rate 1Mbit ceil 1Mbit burst 1k \n# 其它流量规则\n# tc class add dev enp1s0 parent 1:1 classid 1:20 htb rate 20Mbit ceil 20Mbit burst 15k\n\n# 为了避免一个会话永占带宽,添加随即公平队列sfq\n# tc qdisc add dev enp1s0  parent 1:10 handle 10: sfq perturb 10 \n# tc qdisc add dev enp1s0  parent 1:20 handle 20: sfq perturb 10  \n\n# 添加filter来应用class规则，这里是目标端口为80、443的都到 1:10的class 中\n# tc filter add dev enp1s0  protocol ip parent 1:0 prio 3 u32   match ip dport 443 0xffff   flowid 1:10 \n# tc filter add dev enp1s0  protocol ip parent 1:0 prio 2 u32   match ip dport 80 0xffff   flowid 1:10\n\n# 规则检验\n# tc qdisc show dev enp1s0\n# tc class show dev enp1s0\n# tc filter show dev enp1s0 \n\n# 数据统计 查看有多少流量命中那些规则\n# tc -s class show dev enp1s0\n# tc -s filter show dev enp1s0 \n```\n\n限速后，公网流量恢复正常，集群节点状态恢复。\n\n#### 排查\n\n直接说结论，还没有排查出来问题系统就被重装了，~~所以怀疑是有问题的进程被替换成正常的进程程序了~~。😥\n\n这里说一下排查思路和过程：\n\n+ 重新安装基础命令工具（ps、ss、lsof等）\n+ 使用 iftop 持续监控流量\n+ 使用 ss、ps、lsof 定位可疑进程：\n  - 未发现可疑进程与攻击IP的连接\n  - 发现几个异常公网ip的VNC-console连接，禁止相关IP访问后重启libvirt服务，问题仍存在\n+ 检查系统日志、登录日志、操作日志、安全日志及内核日志\n+ 检查iptables配置，确认无异常丢弃ACK/FIN包规则\n+ 检查定时任务、启动脚本、可疑用户及登录信息\n+ 检查虚拟机内部流量，确认正常\n+ 使用Rootkit工具进行全面检查\n\n值得注意的是，上述排查后发现，自上次修改密码后，系统无任何可疑IP登录记录，怀疑攻击者在初次入侵时植入了持久化后门或恶意程序。\n\n最后在其他同学备份好相关文件后，对系统进行了重装。\n\n## 总结\n\n在经历了上述事件后，还是要加强以下两点：\n\n+ 系统暴露在公网环境下，最好是不要用弱密码（用密钥或者强密码）并且要开启防火墙，限制可以登录的ip白名单。\n+ 要精细化监控告警，像上述的问题都是通过<font style=\"color:rgb(44, 44, 54);\">PMQL</font>来定位的，这样完全可以编写对应的告警规则，在出现相关问题的时候，第一时间可以定位到对应的资源，加快故障恢复速度。\n","tags":["排障","k8s"]},{"title":"阿里云Prometheus成本优化实战","url":"/2025/09/03/阿里云Prometheus成本优化实战/","content":"\n## 背景\n\n> 近日，团队发现阿里云上某个项目的Prometheus监控实例成本异常高昂，月均费用达到2-3千元，而其他类似规模的项目仅需几百元。我对此进行了成本优化。\n\n架构\n\n在我们项目的集群中是有自建的Prometheus实例的，同时采用了混合监控方案：\n\n- 集群内部署了自建Prometheus实例，负责基础监控数据的采集和存储\n- 通过Prometheus的remote_write功能，将监控数据同步至阿里云ARMS Prometheus\n- 告警功能完全依赖阿里云Prometheus的高级规则配置实现\n\n这种架构设计初衷是为了利用阿里云Prometheus更强大的告警管理能力，但同时也带来了额外的成本。\n\n![降本-Prometheus-架构图.drawio.png](./架构图.png)\n\n## 问题诊断\n\n首先，我登录阿里云控制台查看该Prometheus实例的指标数据，发现基础指标和自定义指标数量均异常高。然而，当我检查集群内自建Prometheus的数据存储量时，仅显示400多GB，与阿里云上显示的规模明显不符。\n\n![image.png](./before.png)\n\n因此直接给阿里云提上工单询问为什么本地看的Prometheus存储数据和阿里云上面看到的不一致。\n\n在客服询问后得知，阿里云上面的指标数据是未经过压缩的，而Prometheus本地查看的数据是经过了压缩的，所以存储占用看起来会少很多。\n\n看来只能做相关远程写的指标优化，首先要确定阿里云Prometheus的收费标准\n\n在查询文档和询问客服后得知得知阿里云Prometheus的收费是分两种，计费方式为里面任意一种。\n\n- 按写入量收费，分为基础指标（免费）和自定义指标（收费）\n- 按上报量收费，也是分为基础指标（免费）和自定义指标（收费）\n\n[Prometheus 实例的计费方式、计费项、计费单价和开通方式_应用实时监控服务(ARMS)-阿里云帮助中心](https://help.aliyun.com/zh/arms/prometheus-monitoring/product-overview/billing-description/?spm=a2c4g.11186623.0.0.460f611641GFxf)\n\n并且远程写里面也是可以区分基础指标（免费）和自定义指标（收费）\n\n特别关键的是，通过remote_write同步的数据也可以区分基础指标和自定义指标。这意味着我们可以通过配置relabel规则，精确控制哪些指标需要同步到阿里云，从而大幅降低费用。\n\n![image.png](./参考.png)\n\n## 优化方案\n\n基于以上分析，我制定了以下优化策略：\n\n1. **保留所有免费的基础指标**：确保Kubernetes核心组件监控不受影响\n2. **精简自定义指标**：仅保留需要告警的关键业务指标\n3. **停用阿里云Grafana**：改用集群内自建的Grafana实例，避免额外费用\n\n### 配置\n\n集群中使用的Prometheus安装方的是 [prometheus-operator](https://github.com/prometheus-operator/prometheus-operator) \n\n所以可以按照文档里面的方式对remove wirte配置进行修改 [相关文档](https://github.com/prometheus-operator/prometheus-operator/blob/v0.79.2/Documentation/api.md) \n\n这里是直接修改集群中Prometheus对象\n\n在spec.remoteWrite.writeRelabelConfigs 中进行配置\n\n这里我配置是以job为最小单位，虽然告警用的是特定的指标数据，但是在查看相关自定义job的数据后，发现它们的占用并不高，同时因为每次修改完后remoteWrite的相关配置后，都要重启Prometheus，所以也是为了避免后续因调整告警规则而不断重启Prometheus，所以考虑使用了job的范围来做relabel的配置\n\n```yaml\n    remoteWrite\n      writeRelabelConfigs:\n        - sourceLabels: [job]\n          regex: '^(apiserver|node-exporter|kube-state-metrics|kubelet|metallb-metrics|kube-dns|xxx-.*|ceph-.*|.*etcd.*)$'\n          action: keep\n```\n\n配置好后，重启对应的statefulset\n\n```bash\nKubectl rollout restart statefulsets.apps prometheus-k8s -n monitoring\n```\n\n### 验证\n\n在配置好后，到指标中心查看up的job\n\n```promql\nsum(up{}) by (job)\n```\n\n同时逐一验证每个告警规则的PromQL表达式，确保所有关键告警仍能正常触发\n\n## 优化效果\n\n在配置好并过了一天后查看对应数据\n\n![image.png](./before.png)\n\n![image.png](./after.png)\n\n从上面可以看出对应的自定义指标数据有大幅下降，同时在查看成本后，发现对应的Prometheus的成本也是下降了。\n\n原先是每天160G x 0.4(我们的Prometheus是0.4每G) = 64 下降成 9G x 0.4 = 3.6 \n\n## 总结\n\n通过合理配置Prometheus的remote wirte规则，成功将阿里云Prometheus实例的成本降低了90%以上，同时完整保留了告警功能。唯一的变动是将Grafana查看地址切换回集群内部署的实例。\n\n这次优化实践表明，云服务的成本控制不仅需要关注资源规格，更需要深入理解服务的计费模型，通过技术手段精准控制资源消耗。\n\n对于使用阿里云Prometheus的团队，建议定期审查指标摄入情况，避免不必要的费用支出。","tags":["优化","Prometheus"]},{"title":"Linux网络丢包告警的排查实践","url":"/2025/08/14/Linux网络丢包告警的排查实践/","content":"\n> 某天晚上，发现在告警群里面收到一条关于网络丢包的告警\n\n![](./alert_dingtalk.png)\n\n这里看到有两张网卡都出现了丢包，且丢包数量一致，那是因为这个boud4网卡是由enp26s0f0np0和enp26s0f1np1聚合出来了（为了做高可用），所以只要看enp26s0f0np0这张网卡就好了。\n\n# 查看节点流量状态\n\n![](./grafana.png)\n\n在grafana中查看节点网络状态，确实是有大流量在跑\n\n# 登录到节点进行排查\n\n先用 `ip link` 来查看网卡相关信息\n\n```shell\n# ip -s link show enp26s0f0np0 \n4: enp26s0f0np0: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 9000 qdisc mq master bond4 state UP mode DEFAULT group default qlen 1000\n    link/ether 0c:42:a1:32:11:d6 brd ff:ff:ff:ff:ff:ff\n    RX:      bytes     packets errors dropped  missed   mcast           \n    25911288114346 25333203540      0  574380       0 9523278 \n    TX:      bytes     packets errors dropped carrier collsns           \n    30755111916060 25795803740      0       0       0       0\n```\n\n可以发现在网卡接受包的时候出现了丢包原因\n\n使用 <font style=\"color:rgb(51, 51, 51);\">ethtool –S 查看网卡包信息，主要关注rx的信息</font>\n\n<font style=\"color:rgb(51, 51, 51);\">其中关于丢包的信息要关注以下几个字段</font> （或者是带err、drop字样的字段）\n\n<font style=\"color:rgb(51, 51, 51);\">rx_err_lane_xx_phy  物理层（PHY层）接收过程中，特定传输通道上检测到的错误（可能是网线存在问题）</font>\n\n<font style=\"color:rgb(51, 51, 51);\">rx_out_of_buffer  网卡没有可用的接收缓冲区来存储传入数据包而导致丢包  </font>\n\n<font style=\"color:rgb(51, 51, 51);\">rx_buff_alloc_err   网卡在接收数据包时尝试分配内存缓冲区时发生而导致丢包</font>\n\n<font style=\"color:rgb(51, 51, 51);\">rx_oversize_pkts_sw_drop 数据包大小超过了软件层面允许（MTU）的最大值而被丢弃的接收数据包数量    </font>\n\n<font style=\"color:rgb(51, 51, 51);\">rx_wqe_err  接收工作队列项（Work Queue Entry）错误 , 意味着网卡驱动与硬件之间的通信问题  </font>\n\n```shell\n# ethtool -S enp26s0f0np0 | grep -i rx\n...\nrx_out_of_buffer  572782\n...\n```\n\n<font style=\"color:rgb(51, 51, 51);\">我这里发现rx_out_of_buffer的数量一直在增加，说明网卡的缓冲区处理速度更不上网络传输速度，在上图中也看到网络确实有大量流量进来</font>\n\n<font style=\"color:rgb(51, 51, 51);\">查看网卡缓冲区大小</font>\n\n```shell\n# ethtool -g enp26s0f0np0 \nRing parameters for enp26s0f0np0:\nPre-set maximums:\nRX:             1024\nRX Mini:        n/a\nRX Jumbo:       n/a\nTX:             1024\nCurrent hardware settings:\nRX:             1024\nRX Mini:        n/a\nRX Jumbo:       n/a\nTX:             1024\nRX Buf Len:             n/a\nTX Push:        n/a\n```\n\n<font style=\"color:rgb(51, 51, 51);\">发现RX的网卡缓冲区大小为1024，临时修改为8192，看看问题是否还存在</font>\n\n```shell\n# ethtool -G enp26s0f0np0 rx 8192\n# ethtool -g enp26s0f0np0 \nRing parameters for enp26s0f0np0:\nPre-set maximums:\nRX:             8192\nRX Mini:        n/a\nRX Jumbo:       n/a\nTX:             8192\nCurrent hardware settings:\nRX:             8192\nRX Mini:        n/a\nRX Jumbo:       n/a\nTX:             1024\nRX Buf Len:             n/a\nTX Push:        n/a\n#  ethtool -S enp26s0f0np0 | grep -i rx_out_of_buffer\n```\n\n在修改后发现 rx_out_of_buffer 的数量没有再添加了，过了5分钟左右对应的告警也恢复了\n\n![](./alert_recovery.png)\n\n现在就要把对应的配置进行持久化\n\n```shell\n# 修改对应的配置文件，添加对应的配置 ${DEVICE} 为对应的网卡名称\n# vi /etc/sysconfig/network-scripts/ifcfg-enp26s0f0np0\n...\nETHTOOL_OPTS=\"-G ${DEVICE} rx 8192\"\n# 我这里是bond所以另一张网卡也要修改\n# vi /etc/sysconfig/network-scripts/ifcfg-enp26s0f1np1\n...\nETHTOOL_OPTS=\"-G ${DEVICE} rx 8192\"\n```\n\n# 总结\n\n这个网卡的<font style=\"color:rgb(51, 51, 51);\">缓冲区有什么用</font>\n\n<font style=\"color:rgb(51, 51, 51);\">先看一下一个网络数据包到linux上的大概流程</font>\n\n[https://zhuanlan.zhihu.com/p/256428917](https://zhuanlan.zhihu.com/p/256428917) 这里参考大佬的文章\n\n![](./net1.png)\n\n这里可以看到把数据保存在缓冲区（Ring buffer）为第6步\n\n![](./net2.png)\n\n这个流程再细分一下就是以上的步骤，也就是说当网卡把数据拷到内存的时候发现缓冲区队列满了，无法再添加数据导致本次丢包\n\n同时上述的文章里面也说了为什么ethtool可以修改这个缓冲区的配置，因为<font style=\"color:rgb(25, 27, 31);\">网卡驱动实现了ethtool所需要的接口，也在这里完成函数地址的注册。当 ethtool 发起一个系统调用之后，内核会找到对应操作的回调函数（实际上调用的就是网卡的接口）。</font>\n\n![](./net3.png)\n\n","tags":["排障","linux"]},{"title":"如何制作Windows11镜像并在KubeVirt中部署","url":"/2025/07/07/如何制作Windows11镜像并在KubeVirt中部署/","content":"\n> 最近需要在k8s中解锁win11虚拟机，所以进行了相关学习并记录下此篇博客\n\n# win11虚拟机配置\n\n本地先安装win11虚拟机并准备所需要的软件，这里使用linux来创建win11虚拟机\n\n```bash\nvirt-install \\\n --virt-type=kvm \\\n --name win11-render-1.6 \\\n --ram 16384 \\\n --vcpus=8 \\\n --os-variant=win11  \\\n --cdrom=/data/render/Win11_24H2_Chinese_Simplified_x64.iso \\\n --network=default,model=virtio \\\n --graphics vnc,listen=0.0.0.0 --noautoconsole \\\n --disk path=/data/render/virtio-win-0.1.271.iso,device=cdrom \\\n --disk path=/data/render/win11-render-1.6.qcow2,size=256,bus=virtio,format=qcow2 \\\n --boot uefi # win11默认使用uefi启动\n```\n\n这里有一个要注意的点是启动的时候除了挂载win11的iso还要挂载virtio-win的驱动，不然会出现个别硬件无法使用的问题\n\nvirtio-win下载地址：[https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/](https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/)\n\n## 驱动安装\n\n参考文档\n\n[https://kubevirt.io/user-guide/user_workloads/windows_virtio_drivers/](https://kubevirt.io/user-guide/user_workloads/windows_virtio_drivers/)\n\n在安装系统的时候会出现无可选磁盘的问题，这个时候就要安装virtio-win里面的磁盘驱动（vioscsi），同时也可以把网络驱动给安装了(NetKvm)\n\n![](./install-vioscsi.png)\n\n进到系统后打开设备管理器看看那些设备需要安装驱动（从virtio-win里面安装）\n\n![](./driver.png)\n\n## 解锁BitLocker\n\n成功进入系统后先别急着对系统进行定制安装，先把磁盘的BitLocker给关闭掉，以便后面进行sysprep打包\n\n在powershell中输入\n\n```powershell\n# 查看磁盘解锁状态\nmanage-bde -status\nBitLocker 驱动器加密: 配置工具版本 10.0.26100\n版权所有 (C) 2013 Microsoft Corporation。保留所有权利。\n\n可以使用 BitLocker 驱动器加密\n保护的磁盘卷:\n卷 C: []\n[OS 卷]\n\n    大小:              255.19 GB\n    BitLocker 版本:    无\n    转换状态:          完全解密\n    已加密百分比:      0.0%  # 查看解密进度\n    加密方法:          无\n    保护状态:          保护关闭\n    锁定状态:          已解锁\n    标识字段:          无\n    密钥保护器:        找不到\n    \n# 关闭对应磁盘BitLocker\nmanage-bde -off C:\n```\n\n## cloudbase-init 安装\n\n在完成系统定制化软件安装后，下载cloudbase-init进行系统配置 [https://cloudbase.it/cloudbase-init/](https://cloudbase.it/cloudbase-init/)\n\n使用cloudbase-init可以读取虚拟机启动时的配置进行动态配置系统信息（比如hostname、用户密码等）\n\n![](./cloudbase1.png)\n\n如果你这个时候已经配置好系统了，可以勾选下面的选项，它将会自动执行sysprep并关机\n\n![](./cloudbase2.png)\n\n### 配置cloudbase-init\n\n cloudbase-init 的典型用法主要是通过配置文件进行配置，以确定要使用哪些服务（services）和插件（plugins），然后在这些文件中设置参数来定制它们的行为以及整体初始化过程  \n\n[https://cloudbase-init.readthedocs.io/en/latest/](https://cloudbase-init.readthedocs.io/en/latest/)\n\n这里进行默认用户密码修改和hostname的配置\n\n首先先配置对应的配置文件 cloudbase-init.conf ，这里只用配置 cloudbase-init.conf ，不用配置cloudbase-init-unattend.conf \n\n因为我们这使用的plugins是 **cloudbaseinit.plugins.common.sethostname.SetHostNamePlugin** 、**cloudbaseinit.plugins.common.setuserpassword.SetUserPasswordPlugin** 和 **cloudbaseinit.plugins.common.userdata.UserDataPlugin**\n\n而cloudbase-init-unattend.conf中只能使用**MTU** 和 **hostname** plugins\n\n因为要在本地验证配置，所以使用的service是  **cloudbaseinit.metadata.services.nocloudservice.NoCloudConfigDriveService**\n\n[https://cloudbase-init.readthedocs.io/en/latest/tutorial.html#configuration-file](https://cloudbase-init.readthedocs.io/en/latest/tutorial.html#configuration-file)\n\n```plain\n# cloudbase-init.conf  metadata_services和plugins等相关配置\nmetadata_services=cloudbaseinit.metadata.services.nocloudservice.NoCloudConfigDriveService\nplugins=cloudbaseinit.plugins.common.sethostname.SetHostNamePlugin,cloudbaseinit.plugins.common.setuserpassword.SetUserPasswordPlugin,cloudbaseinit.plugins.windows.updates.WindowsAutoUpdatesPlugin,cloudbaseinit.plugins.common.userdata.UserDataPlugin\nallow_reboot=false    # allow the service to reboot the system\nstop_service_on_exit=false\n```\n\n### 准备 <font style=\"color:rgb(0, 0, 0);\">NoCloudConfigDriveService 数据</font>\n\n> <font style=\"color:rgb(64, 64, 64);background-color:rgb(252, 252, 252);\">The metadata is provided on a config-drive (vfat or iso9660) with the label cidata or CIDATA.</font>\n>\n> <font style=\"color:rgb(64, 64, 64);background-color:rgb(252, 252, 252);\">The default folder structure for NoCloud is:</font>\n>\n> + <font style=\"color:rgb(64, 64, 64);background-color:rgb(252, 252, 252);\">/user-data</font>\n> + <font style=\"color:rgb(64, 64, 64);background-color:rgb(252, 252, 252);\">/meta-data</font>\n\n这个 `NoCloudConfigDriveServic ` 数据是一个iso磁盘，并且名称要为 cidata 或 CIDATA\n\n<font style=\"color:rgb(0, 0, 0);\">里面的文件可以有：meta-data(元数据) 和user-data(用户定义的数据)</font>\n\n[https://cloudinit.readthedocs.io/en/latest/reference/datasources/nocloud.html](https://cloudinit.readthedocs.io/en/latest/reference/datasources/nocloud.html)\n\n[https://cloudbase-init.readthedocs.io/en/latest/plugins.html#user-data-main](https://cloudbase-init.readthedocs.io/en/latest/plugins.html#user-data-main)\n\n以下就是这次meta_data和user_data的配置\n\n```shell\n# cat meta_data.yaml \ninstance-id: test-windows\nhostname: test-windows\n# cat user_data.yaml\n#cloud-config\nset_hostname: test-windows\n\nusers:\n  - name: admin\n    passwd: 1qaz@WSX # 对应用户的初始化密码\n    groups:\n      - Administrators\n\nruncmd:\n  - 'powershell -command \"Write-Host \\\"Cloudbase-Init configuration applied successfully!\\\"\"'\n```\n\n进行iso打包\n\n[https://documentation.ubuntu.com/public-images/public-images-how-to/use-local-cloud-init-ds/](https://documentation.ubuntu.com/public-images/public-images-how-to/use-local-cloud-init-ds/)\n\n```shell\n# 下载 cloud-localds\n# dnf --enablerepo=devel install cloud-utils\n# cloud-localds config-drive.iso user_data.yaml meta_data.yaml\n```\n\n把打包出来的config-drive.iso挂载到对应的虚拟机里面\n\n```shell\n# virsh edit --domain win11-test-cloudbase-init \n...\n    <disk type='file' device='cdrom'>\n      <driver name='qemu' type='raw'/>\n      <source file='/data/render/cloudbase-init/config-drive.iso'/>\n      <target dev='sdd' bus='sata'/>\n      <readonly/>\n      <address type='drive' controller='0' bus='0' target='0' unit='3'/>\n    </disk>\n...\n```\n\n![](./disk.png)\n\n```powershell\nPS G:\\> dir\n\n    目录: G:\\\n\nMode                 LastWriteTime         Length Name\n----                 -------------         ------ ----\n--r---          2025/7/6     23:32             49 meta-data\n--r---          2025/7/6     23:32            224 user-data\n```\n\n接下来可以按照项目要求安装软件，比如在windows上面安装openssh方便使用ansible进行管理\n\n[使用ansible管理windwos - kehaha-5](https://kehaha-5.github.io/2025/07/06/使用ansible管理windwos/)\n\n## sysprep系统封装\n\n手动进行sysprep系统封装\n\n[https://learn.microsoft.com/zh-cn/windows-hardware/manufacture/desktop/sysprep--system-preparation--overview?view=windows-11](https://learn.microsoft.com/zh-cn/windows-hardware/manufacture/desktop/sysprep--system-preparation--overview?view=windows-11)\n\n命令文档：[https://learn.microsoft.com/zh-cn/windows-hardware/manufacture/desktop/sysprep-command-line-options?view=windows-11](https://learn.microsoft.com/zh-cn/windows-hardware/manufacture/desktop/sysprep-command-line-options?view=windows-11)\n\n如果你的系统想要第一次进入时进行重新配置(oobe)就执行 \n\noobe: <font style=\"color:rgb(22, 22, 22);\">客户可以通过开箱即用体验 (OOBE) 添加用户特定的信息并接受 Microsoft 软件许可条款</font>\n\n```powershell\n%WINDIR%\\system32\\sysprep\\sysprep.exe /generalize /shutdown /oobe /mode:vm\n```\n\n如果想要系统第一次进入时进入审核模式，方便系统调式就执行\n\n```powershell\n%WINDIR%\\system32\\sysprep\\sysprep.exe /generalize /shutdown /oobe /mode:vm\n```\n\n如果想要进入到无人接触模式就要配置好unattend文件，同时命令中指定对应的文件\n\n```powershell\n%WINDIR%\\system32\\sysprep\\sysprep.exe /generalize /oobe /shutdown /unattend:xxx /mode:vm\n```\n\n这里使用的是无人接触模式，如果没有其它特殊的配置，unattend.xml可以用cloudbase-init安装时会携带的unattend.xml文件\n\n注意必须要添加 `/mode:vm` 参数，否则可能会出现启动蓝屏（code:INACCESSIBLE BOOT DEVICE）或者出现提示系统无法启动，请重新安装windwos等无法启动的问题\n\n这里使用添加 `/shutdown` 参数确保系统打包好后自动关机\n\n## 配置验证\n\n在配置完上述的CloudBase-Init和Sysprep打包关机后，手动启动系统，在系统启动完成后可以在cloudbase-init的log目录下面查看对应的执行日志\n\n```log\n2025-07-07 14:59:04.852 3768 INFO cloudbaseinit.metadata.services.osconfigdrive.windows [-] Config Drive found on G:\\\n2025-07-07 14:59:04.857 3768 DEBUG cloudbaseinit.metadata.services.baseconfigdrive [-] Metadata copied to folder: 'C:\\\\WINDOWS\\\\TEMP\\\\tmpbxgu6_cf' load C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\metadata\\services\\baseconfigdrive.py:81\n2025-07-07 14:59:04.862 3768 INFO cloudbaseinit.init [-] Metadata service loaded: 'NoCloudConfigDriveService'\n2025-07-07 14:59:04.864 3768 DEBUG cloudbaseinit.init [-] Instance id: test-windows configure_host C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\init.py:202\n2025-07-07 14:59:04.869 3768 DEBUG cloudbaseinit.utils.classloader [-] Loading class 'cloudbaseinit.plugins.common.sethostname.SetHostNamePlugin' load_class C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\utils\\classloader.py:35\n2025-07-07 14:59:04.873 3768 DEBUG cloudbaseinit.utils.classloader [-] Loading class 'cloudbaseinit.plugins.common.setuserpassword.SetUserPasswordPlugin' load_class C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\utils\\classloader.py:35\n2025-07-07 14:59:04.876 3768 DEBUG cloudbaseinit.utils.classloader [-] Loading class 'cloudbaseinit.plugins.windows.updates.WindowsAutoUpdatesPlugin' load_class C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\utils\\classloader.py:35\n2025-07-07 14:59:04.882 3768 DEBUG cloudbaseinit.utils.classloader [-] Loading class 'cloudbaseinit.plugins.common.userdata.UserDataPlugin' load_class C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\utils\\classloader.py:35\n2025-07-07 14:59:04.887 3768 INFO cloudbaseinit.init [-] Executing plugins for stage 'MAIN':\n2025-07-07 14:59:04.888 3768 DEBUG cloudbaseinit.init [-] Plugin 'SetHostNamePlugin' execution already done, skipping _exec_plugin C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\init.py:61\n2025-07-07 14:59:04.894 3768 DEBUG cloudbaseinit.init [-] Plugin 'SetUserPasswordPlugin' execution already done, skipping _exec_plugin C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\init.py:61\n2025-07-07 14:59:04.899 3768 DEBUG cloudbaseinit.init [-] Plugin 'WindowsAutoUpdatesPlugin' execution already done, skipping _exec_plugin C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\init.py:61\n2025-07-07 14:59:04.902 3768 DEBUG cloudbaseinit.init [-] Plugin 'UserDataPlugin' execution already done, skipping _exec_plugin C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\init.py:61\n2025-07-07 14:59:04.906 3768 DEBUG cloudbaseinit.metadata.services.baseconfigdrive [-] Deleting metadata folder: 'C:\\\\WINDOWS\\\\TEMP\\\\tmpbxgu6_cf' cleanup C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\Python\\Lib\\site-packages\\cloudbaseinit\\metadata\\services\\baseconfigdrive.py:93\n2025-07-07 14:59:04.911 3768 INFO cloudbaseinit.init [-] Plugins execution done\n```\n\n从上面的日志可以看到，找到了Config Drive （G盘），获取了hostname为`test-windows`，并成功启动了 `UserDataPlugin`、`SetHostNamePlugin` 、`SetUserPasswordPlugin`  这几个插件\n\n> 注意如果想要hostname配置成功，可能需要重新系统\n\n[https://cloudbase-init.readthedocs.io/en/latest/userdata.html#cloud-config](https://cloudbase-init.readthedocs.io/en/latest/userdata.html#cloud-config)\n\n# kubevirt配置\n\n此时就完成了系统的封装，接下来把对应的qcow2镜像导出到webserver，方便后续导入cdi中\n\n启动虚拟机用的是kubevirt组件，里面一个虚拟机的相关资源有 cdi、vm、vmi和pod，其中\n\ncdi是用来导入虚拟机镜像的，可以用http的方式导入上面打包好的win11的镜像\n\nvm可以理解为定义虚拟机的xml\n\nvmi就是对应的虚拟机运行时的实例\n\npod就是对应k8s中的运行资源\n\n## biso配置\n\n其中需要额外配置一下vm，因为使用的镜像为win11要使用uef启动，kubevirt默认使用的是biso启动的\n\n![](./boot.png)\n\n## cpu配置\n\n还有就是要配置一下cpu的相关配置，不然任务管理中的cpu数量会跟虚拟机配置和设备管理器中显示的数量不一致\n\n![](./cpu1.png)\n\n不一致的情况如下\n\n![](./cpu2.png)\n\n## cloudbase配置\n\nkubevirt是支持 `cloudInitNoCloud` 和 `cloudInitConfigDrive`的配置的，并且也支持从k8s的secret中获取配置信息\n\n具体看如下链接\n\n[https://kubevirt.io/user-guide/user_workloads/startup_scripts/#cloud-init-examples](https://kubevirt.io/user-guide/user_workloads/startup_scripts/#cloud-init-examples)\n\n其它参考：\n\n[https://kubevirt.io/2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html](https://kubevirt.io/2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html)\n\n至此，就可以在k8s中解锁win11虚拟机了🥰\n\n","tags":["边做边学","windows11","kubevirt"]},{"title":"使用ansible管理windwos","url":"/2025/07/06/使用ansible管理windwos/","content":"\n> 最近要对20多台windows主机进行管理（启动进程、挂载网络硬盘和修改环境变量），所以使用了ansible做管理工具\n\n# 安装ansible\n\n在使用absible时，只需要在集群中的一台选择任意一台服务器(linux系统)在上面安装ansible即可\n\n但是集群中是windwos系统， 所以要在windwos上面安装wsl，先要在系统功能里面开启wsl功能，然后安装wsl2程序\n\n[安装 WSL | Microsoft Learn](https://learn.microsoft.com/zh-cn/windows/wsl/install)\n\n```powershell\nwsl --list --online # 查询可以按照的linux版本\nwsl --install -d <Distribution Name> # 安装对应的linux系统\n```\n\n安装完成后在wsl中安装ansible，我的系统是使用dnf进行安装\n\n```shell\n# dnf install ansible-core\n```\n\n# 配置windwos\n\nansible要控制windwos的话，windwos需要进行配置，要开启windwos的wrm（Windows Remote Management）或者配置ssh服务\n\n[https://docs.ansible.com/ansible/latest/os_guide/windows_winrm.html](https://docs.ansible.com/ansible/latest/os_guide/windows_winrm.html)\n\n[https://docs.ansible.com/ansible/latest/os_guide/windows_ssh.html](https://docs.ansible.com/ansible/latest/os_guide/windows_ssh.html)\n\n这里配置ssh，因为它功能更好，但是要注意版本\n\n> <font style=\"color:rgb(64, 64, 64);background-color:rgb(252, 252, 252);\">Microsoft provides an OpenSSH implementation with Windows since Windows Server 2019 as a Windows capability. It can also be installed through an upstream package under </font>[<font style=\"color:rgb(41, 128, 185);\">Win32-OpenSSH</font>](https://github.com/PowerShell/Win32-OpenSSH)<font style=\"color:rgb(64, 64, 64);background-color:rgb(252, 252, 252);\">. Ansible officially only supports the OpenSSH implementation shipped with Windows, not the upstream package. The OpenSSH version must be version </font> `7.9.0.0` <font style=\"color:rgb(64, 64, 64);background-color:rgb(252, 252, 252);\"> at a minimum. This effectively means official support starts with Windows Server 2022 because Server 2019 ships with version `7.7.2.1` <font style=\"color:rgb(64, 64, 64);background-color:rgb(252, 252, 252);\">. Using older Windows versions or the upstream package might work but is not supported.</font>\n\n<font style=\"color:rgb(64, 64, 64);background-color:rgb(252, 252, 252);\">配置ssh的话可以根据文档进行安装</font>\n\n```powershell\nGet-WindowsCapability -Name OpenSSH.Server* -Online |\n    Add-WindowsCapability -Online\nSet-Service -Name sshd -StartupType Automatic -Status Running\n\n$firewallParams = @{\n    Name        = 'sshd-Server-In-TCP'\n    DisplayName = 'Inbound rule for OpenSSH Server (sshd) on TCP port 22'\n    Action      = 'Allow'\n    Direction   = 'Inbound'\n    Enabled     = 'True'  # This is not a boolean but an enum\n    Profile     = 'Any'\n    Protocol    = 'TCP'\n    LocalPort   = 22\n}\nNew-NetFirewallRule @firewallParams\n\n$shellParams = @{\n    Path         = 'HKLM:\\SOFTWARE\\OpenSSH'\n    Name         = 'DefaultShell'\n    Value        = 'C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe'\n    PropertyType = 'String'\n    Force        = $true\n}\nNew-ItemProperty @shellParams\n```\n\n指定登录后的shell为powershell\n\n```powershell\n# Set default to powershell.exe\n$shellParams = @{\n    Path         = 'HKLM:\\SOFTWARE\\OpenSSH'\n    Name         = 'DefaultShell'\n    Value        = 'C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe'\n    PropertyType = 'String'\n    Force        = $true\n}\nNew-ItemProperty @shellParams\n\n# Set default back to cmd.exe\nRemove-ItemProperty -Path HKLM:\\SOFTWARE\\OpenSSH -Name DefaultShell\n```\n\n配置好后，可以通过ssh windows用户名@地址 登录到系统\n\n# 配置ansible\n\n首先要对我们将要管理的资产写一个ansible的资产文件里面\n\n```powershell\nwin:\n  hosts:\n   10.88.128.xxx:\n   10.88.128.xxx:\n  vars:\n    ansible_user: \"xxx\" # Windows上的SSH用户名\n    ansible_password: \"xxx\" # Windows SSH用户的密码 (或者配置SSH Key)\n    ansible_port: 22 # SSH默认端口\n    ansible_connection: ssh # 指定使用SSH连接\n    ansible_shell_type: powershell # 指定Windows上的Shell类型，Powershell是常见的选择\n```\n\n配置好后可以进行测试\n\n```shell\n[root@Server-68ca6f1e-6023-4e99-9051-10e3746df9bb ~]# ansible -i /etc/ansible/win_inventory.yml all -m win_ping \n[WARNING]: Collection ansible.windows does not support Ansible version 2.14.18\n[WARNING]: Collection ansible.windows does not support Ansible version 2.14.18\n[WARNING]: Collection ansible.windows does not support Ansible version 2.14.18\n[WARNING]: Collection ansible.windows does not support Ansible version 2.14.18\n[WARNING]: Collection ansible.windows does not support Ansible version 2.14.18\n10.88.128.xx | SUCCESS => {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n[WARNING]: Collection ansible.windows does not support Ansible version 2.14.18\n10.88.128.xxx | SUCCESS => {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n```\n\n如果报win模块没有的话要进行安装\n\n```shell\nansible-galaxy collection install ansible.windows\n```\n\n如果使用win_pong发现结果是乱码的话，可以到windows上面重新安装一下ssh\n\n```shell\nRemove-WindowsCapability -Online -Name OpenSSH.Client # 卸载open-ssh\n# 重新执行上面的脚本安装open-ssh\n```\n\n后面就可以根据实际情况编写playbook了\n\nansible中win相关的命令文档\n\n[https://docs.ansible.com/ansible/latest/collections/ansible/windows/index.html#plugins-in-ansible-windows](https://docs.ansible.com/ansible/latest/collections/ansible/windows/index.html#plugins-in-ansible-windows)\n\n","tags":["边做边学","windows","ansible"]},{"title":"K8s中VPC-NAT-GW公网IP无法访问的排障过程","url":"/2025/05/15/K8s中VPC-NAT-GW公网IP无法访问的排障过程/","content":"\n> 测试团队发现在某个环境中，发现公网ip无法正常使用\n\n# 排查\n\n该k8s中使用kube-ovn作为cni，并且对特定的vpc使用集中式网关，该问题就是出现在某个vpc的网关中\n\n[VPC 入门 - Kube-OVN 文档](https://kubeovn.github.io/docs/v1.13.x/vpc/vpc/#vpc_2)\n\n进入对应的vpc-nat-gw的pod，查看信息是否正常\n\n查看网卡信息\n\n![ip-a](./ip-a.png)\n\n查看iptable配置\n\n![ip-a](./iptable-legacy-save.jpg)\n\n查看路由配置\n\n![ip-r](./ip-r.jpg)\n\n上述都没有问题，开始使用tcpdump进行抓包\n\n![ip-r](./tcpdump.jpg)\n\n可用看到有流量从eth0过来（在对应vpc上对应的subnet的应用上面ping 119.29.29.29）\n\n也有xxx.xxx.255.5的流量过来（本地ping对应的公网ip）\n\n上述说明应用流量和公网流量可以正常达到vpc-nat-gw\n\n但是有个问题，按照iptables的配置来说，10.0.4.10的流量应该回经过snat变成公网ip(xxx.xxx.xxx.71)，然后从net1中流出，同理公网ip(xxx.xxx.xxx.71)也应该也要变成10.0.4.10从eth0中流出\n\n所以问题可能出在iptable的配置中\n\n```bash\n# 查看POSTROUTING和PREROUTING的规则，看看是否有规则冲突，可以发现规则没有冲突\n# iptables-legacy -t nat -L POSTROUTING -n -v \nChain POSTROUTING (policy ACCEPT 835 packets, 38717 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n  0     0   SNAT_FILTER  0    --  *      *       0.0.0.0/0            0.0.0.0/0  \n# iptables-legacy -t nat -L PREROUTING  -n -v \nChain PREROUTING (policy ACCEPT 1189 packets, 60269 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n   0    0   DNAT_FILTER  0    --  *      *       0.0.0.0/0            0.0.0.0/0 \n   \n# 查看dnat和snat的转换记录，发现没有转换数据\n# iptables-legacy -t nat -L EXCLUSIVE_SNAT -n -v \nChain EXCLUSIVE_SNAT (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n   0  0 \t SNAT       0    --  *      *       10.0.4.10            0.0.0.0/0          to:xxx.xxx.xxx.71\n# iptables-legacy -t nat -L EXCLUSIVE_DNAT -n -v \nChain EXCLUSIVE_DNAT (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n  0 \t0    DNAT       0    --  *      *       0.0.0.0/0           xxx.xxx.xxx.71       to:10.0.4.10\n```\n\n查看ipv4的转发情况，发现没有开启对于的配置，使用 sysctl -w net.ipv4.ip_forward=1 临时开启，访问就正常了\n\n![ip-r](./net.png)\n\n# 总结\n\n后面发现是对应被调度到vpc-nat-gw的节点没有配置ipv4转发\n\n可能是之前临时配置了（ sysctl -w net.ipv4.ip_forward=1 ），后面被重置了\n\n后续通过编辑 /etc/sysctl.conf 添加 net.ipv4.ip_forward=1进行永久配置","tags":["排障","k8s","kube-ovn"]},{"title":"MTU设置不当导致请求失败：问题分析与解决方法","url":"/2025/04/26/MTU设置不当导致请求失败：问题分析与解决方法/","content":"\n> 最近，我在系统中超融合的虚拟机中发现一个问题，即访问部分服务时会出现无法访问的问题。\n\n# 现象\n\n```shell\n[root@test-mtu ~] curl http://bilibili.com # 正常访问 \n<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n<html>\n<head><title>301 Moved Permanently</title></head>\n<body>\n<center><h1>301 Moved Permanently</h1></center>\n<hr/>Powered by Tengine<hr><center>tengine</center>\n</body>\n</html>\n[root@test-mtu ~] curl -v https://bilibili.com  # 访问失败\n*   Trying 8.134.50.24:443...\n* Connected to baidu.com (8.134.50.24) port 443 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n*  CAfile: /etc/pki/tls/certs/ca-bundle.crt\n* TLSv1.0 (OUT), TLS header, Certificate Status (22):\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* OpenSSL SSL_connect: Connection reset by peer in connection to bilibili.com:443 \n* Closing connection 0\ncurl: (35) OpenSSL SSL_connect: Connection reset by peer in connection to bilibili.com:443 \n```\n\n简单总结为：\n\nping 或者 curl bilibili.com、baidu.com这些是没有问题的\n\n但是用scp、rysnc、访问https服务就会存在访问不通的问题\n\n# 排查\n\n使用curl命令访问 `http://bilibili.com`和`https://bilibili.com`\n\n并且使用tcpdump进行抓包\n\n`tcpdump -nne -w ./cap -i eth0 host bilibili.com`\n\n后面使用wireshark进行分析\n\n![cap-mtu](.\\cap_mtu_1.png)\n\n可以看到上面的1-10是正常的http，11-21是出现错误的https请求\n\n注意里面的第16个包，信息为 `[TCP Previous segment not captured] ` 表示前一个tcp包的数据没有被捕获，说明可以前一个数据包丢失了或者tcpdump没有捕获到对应的tcp包，但是后者应该不可能，首先在上面的http请求中tcpdump完整地捕获到了所有的tcp包，然后curl `https`请求时确实会出现访问失败的问题，所以可以断定是tcp数据包存在丢失的问题。\n\n同时因为16、17的包出现问题，系统没有收到TLS握手时的 `server hello` ，以为第13个包发送失败，在第18、19个包就是重新发送了`[TCP Dup ACK 13#1]` \n\nTLS握手流程如下，在客户端发送 `Client Hello` 后，服务器应该返回`Server Hello `和对应的证书信息\n\n![tls](.\\tls_handle_share.gif)\n\n# 修复\n\n后续发现时网卡的mtu配置错误了，该虚拟机的默认配置为 8950 ，在默认情况下，应该要配置跟路由器一致的mtu值\n\n使用该命令来确认对应网关的mtu值\n\n```shell\n[root@test-mtu ~] ping -M do -S mtu值 网关ip\n```\n\n后续把mtu修改为1450（跟我的路由器一致时），该问题就解决了，可以正常访问服务\n\n# 总结\n\n我在查看第16个包时，发现它的ack为518，刚好跟第14个包(Client Hello)的确认要求一致`(ack=1+tcp.len=517=518)`，并且在客户端发送的SYN包的MSS为8910，所以感觉就是MTU的配置导致系统以为该包是存在错误的\n\n> MSS 是 [TCP 协议](https://zhida.zhihu.com/search?content_id=251059409&content_type=Article&match_order=1&q=TCP+协议&zhida_source=entity)中的一个概念，它表示 TCP 数据包每次能够传输的最大数据量。MSS 是在建立 TCP 连接时协商确定的，它的值通常要小于 MTU。因为 TCP 数据包在传输过程中需要添加 TCP 头部和 [IP 头部](https://zhida.zhihu.com/search?content_id=251059409&content_type=Article&match_order=1&q=IP+头部&zhida_source=entity)，所以 MSS = MTU - IP 头部大小 - TCP 头部大小。例如，在以太网中 MTU 为 1500 字节，IP 头部通常是 20 字节，TCP 头部通常是 20 字节，那么 MSS = 1500 - 20 - 20 =1460 字节。","tags":["排障","mtu"]},{"title":"casdoor服务启动却无法访问排障","url":"/2025/04/03/casdoor服务启动却无法访问排障/","content":"\n> 最近，我们团队在生产环境中遇到了一个问题，就是casdoor服务重启后，日志也显示出来了，但是就是无法访问，使用curl在本地访问也是不通。\n\n日志：\n\n![log](./casdoor-log.png)\n\n首先先简绍一下casdoor是什么[概述 | Casdoor · An open-source UI-first Identity and Access Management (IAM) / Single-Sign-On (SSO) platform with web UI supporting OAuth 2.0, OIDC, SAML, CAS, LDAP, SCIM, WebAuthn, TOTP, MFA, RADIUS, Google Workspace, Active Directory and Kerberos](https://casdoor.org/zh/docs/overview)\n\n简单点理解就是它是一个实现了`OAuth 2.0`认证的一个服务，我们可以利用它来完成一个账号对多个应用进行无感登录，同时可以把用户数据放到`casdoor`中，其它应用的数据库可以只用来存放业务数据。\n\n# 环境\n\n根据相关同学的描述，生产环境的`casdoor`环境如下。\n\n- 3台服务器使用`docker`或`podman`启动了一个`casdoor`服务，实现高可用\n- 3台服务器同时使用同一个数据库(`mysql`)\n- 使用nfs来同步`casdoor`中的`tmp`文件，该文件为`casdoor`的默认`session`保存位置\n   - [session配置的相关文档](https://casdoor.org/zh/docs/basic/server-installation/#%E9%80%9A%E8%BF%87-ini-%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE:~:text=redisEndpoint%20%E5%90%8E%E5%A1%AB%E5%86%99,provider%2Dconfig%E3%80%82)\n   - 当时该 tmp 文件大小至少有40G，但是我们环境当前的用户数量不至于会有这么多的`session`缓存\n   - `nfs`对应挂载服务的进程占用过高的`cpu`\n- `casdoor`的版本为1.814，其中的`beego`版本为1.12.3\n\n# 排查\n\n后续的排查环境是在测试环境中搭建的，并且复刻了对应的问题。环境如下\n\n- 使用`dcoker-compose`启动了3个`casdoor`和一个`mysql`\n- 宿主机的`8001`,`8002`,`8003`分别映射3个`casdoor`的8000端口\n- 使用了 docker 的挂载把tmp文件同时挂载到了每个`casdoor`的目录（模拟`nfs`）\n- `tmp`的文件大小为`31G`\n\n## 端口转发\n\n因为服务是使用docker启动的，并且使用的网络模式为`Bridge`，所以我们先排查`docker`网络相关的问题。\n\n先 `curl` `docker`的网卡(`docker0`)的`ip`地址，但是还是 `curl` 不通相关服务。\n\n```shell\n[root@rocky-testing casdoor]# ss -anpl | grep 8001 # 查看端口监听情况\ntcp   LISTEN 0      4096  0.0.0.0:8001             0.0.0.0:*    users:((\"docker-proxy\",pid=1456272,fd=7)) \ntcp   LISTEN 0      4096 [::]:8001                [::]:*    users:((\"docker-proxy\",pid=1456280,fd=7))     \n[root@rocky-testing casdoor]# ifconfig docker0 #获取docker的网卡\ndocker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500\n        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255\n        inet6 fe80::1469:daff:febf:1315  prefixlen 64  scopeid 0x20<link>\n        ether 16:69:da:bf:13:15  txqueuelen 0  (Ethernet)\n        RX packets 32519  bytes 2160933 (2.0 MiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 37734  bytes 94754551 (90.3 MiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\n[root@rocky-testing casdoor]# curl 172.17.0.1:8001 # curl 对应的网卡ip 8001为casdoor启动的端口\n```\n\n使用`nsenter ` 进入对应服务的`network ns`，并尝试`curl`访问服务，但还是无法访问\n\n```shell\n[root@rocky-testing casdoor]# docker compose top casdoor-1 # 获取应用的pid\nWARN[0000] /root/test/casdoor/docker-compose.yaml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion \ncasdoor-1\nUID    PID       PPID      C    STIME   TTY   TIME       CMD\n1000   1456231   1456210   4    14:58   ?     00:01:21   /server   \n[root@rocky-testing casdoor]# nsenter -n --target 1456231 # 进入对应的 ns\n[root@rocky-testing casdoor]# ip link # 查看网卡，跟宿主机的网卡不一致，只剩下lo和eth0网卡，说明进入了容器的ns中\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0@if898: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default \n    link/ether 82:2e:b0:c9:cc:00 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n[root@rocky-testing casdoor]# ss -anlp | grep 8000 # 查看端口占用，确实服务已经启动 \ntcp   LISTEN 0      4096               *:8000               *:*    users:((\"server\",pid=1456231,fd=13))\n[root@rocky-testing casdoor]# curl 0.0.0.0:8000 # 无法curl通服务\n[root@rocky-testing casdoor]# exit # 退出对应ns\nlogout\n```\n\n## strace\\pstack查看系统调用\n\n在上述的两步操作后，还是`curl`不通，说明服务虽然日志已经打印了，但是服务还未可用。\n\n使用`strace`和`pstack`命令查看对应应用的系统调用，查看casdoor的启动后的行为\n\n```shell\nstrace -p 1456231 -f -o ./strace-casdoor-1.log \npstack 1456231 > ./pstack-casdoor-1.log\n```\n\n查看其日志，以下日志内容为生产环境中获取的`strace`和`pstack`日志\n\n```log\n# 分析strace的日志，发现程序是有规律的执行以下系统调用\n... 都是nanosleep({tv_sec=0, tv_nsec=20000}, NULL) = 0 的调用\n1146720 nanosleep({tv_sec=0, tv_nsec=20000}, NULL) = 0\n1146720 futex(0x45252c0, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=103439635} <unfinished ...>\n1146729 <... newfstatat resumed>{st_mode=S_IFREG|0644, st_size=20, ...}, AT_SYMLINK_NOFOLLOW) = 0\n1146729 --- SIGURG {si_signo=SIGURG, si_code=SI_TKILL, si_pid=1, si_uid=1000} ---\n1146729 rt_sigreturn({mask=[]})         = 0\n1146729 futex(0x45252c0, FUTEX_WAKE_PRIVATE, 1) = 1\n1146720 <... futex resumed>)            = 0\n1146729 newfstatat(AT_FDCWD, \"tmp/0/0/001575f66990e62d6ed701f24599e1ca\",  <unfinished ...>\n1146720 nanosleep({tv_sec=0, tv_nsec=20000}, NULL) = 0\n1146720 futex(0x45252c0, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=43396353} <unfinished ...>\n1146704 <... epoll_pwait resumed>[], 128, 998, NULL, 0) = 0\n1146704 epoll_pwait(4, [], 128, 0, NULL, 0) = 0\n1146704 epoll_pwait(4,  <unfinished ...>\n1146720 <... futex resumed>)            = -1 ETIMEDOUT (Connection timed out)\n1146720 nanosleep({tv_sec=0, tv_nsec=20000}, NULL) = 0\n... 都是nanosleep({tv_sec=0, tv_nsec=20000}, NULL) = 0 的调用\n# 分析pstack日志，程序也是不停地执行以下方法\n 37028.625 (47.259 ms): server/1146720  ... [continued]: futex())                                            = 0\n 37075.871 (         ): server/1146729 newfstatat(dfd: CWD, filename: 0xb77bf0, statbuf: 0xc000adced8, flag: SYMLINK_NOFOLLOW) ...\n 37075.900 ( 0.058 ms): server/1146720 nanosleep(rqtp: 0xc00008ff10)                                         = 0\n 37075.871 (62.859 ms): server/1146729  ... [continued]: newfstatat())                                       = 0\n 37075.965 (         ): server/1146720 futex(uaddr: 0x45252c0, op: WAIT|PRIVATE_FLAG, utime: 0xc00008feb0) ...\n 37138.754 ( 0.012 ms): server/1146729 futex(uaddr: 0x45252c0, op: WAKE|PRIVATE_FLAG, val: 1)                = 1\n 37075.965 (62.898 ms): server/1146720  ... [continued]: futex())                                            = 0\n 37138.781 (         ): server/1146729 newfstatat(dfd: CWD, filename: 0xb77c50, statbuf: 0xc000adcfa8, flag: SYMLINK_NOFOLLOW) ...\n 37138.881 ( 0.067 ms): server/1146720 nanosleep(rqtp: 0xc00008ff10)                                         = 0\n 37138.954 (25.650 ms): server/1146720 futex(uaddr: 0x45252c0, op: WAIT|PRIVATE_FLAG, utime: 0xc00008feb0)   = -1 ETIMEDOUT (Connection timed out)\n 37164.628 ( 0.088 ms): server/1146720 nanosleep(rqtp: 0xc00008ff10)                                         = 0\n 37164.724 ( 0.110 ms): server/1146720 nanosleep(rqtp: 0xc00008ff10)                                         = 0\n 37164.845 ( 0.085 ms): server/1146720 nanosleep(rqtp: 0xc00008ff10)                                         = 0\n```\n\n通过上述的分析，可以发现`casdoor`程序在启动后，一直读取`tmp`目录下的文件信息(并且其中还有一个锁)，结合前面的信息可以得出`casdoor`在启动的时候会一直读取`tmp`里面的`session`信息，导致`nfs`的进程的`cpu`使用率过高。\n\n所以后面在生产环境中把`nfs`的挂载给取消了(`tmp`中的`session`文件就会被清空，变成一个空文件夹)，后面再启动，就可以正常访问服务了，后续可能会考虑把`session`放到`redis`中进行处理。\n\n# 问题🤔\n\n虽然服务可以启动了但是留下了3个问题：\n\n- **为什么casdoor启动的时候要去访问session文件(tmp目录下的文件)**\n- **为什么casdoor的日志都显示服务的端口已经监听了，但是curl该服务还是不能马上返回请求**\n- **同学反映该tmp文件之前已经清理过一次了，但是一天左右磁盘占用率又上来了这是为什么**\n\n针对上面的3个问题，我选择 `源码` 启动👀\n\n## 为什么casdoor启动时要去访问session文件\n\n为什么casdoor启动要去访问session文件\n\n首先找到对应的[github.dev/casdoor/casdoor/tree/v1.814.0](https://github.dev/casdoor/casdoor/tree/v1.814.0)版本，查看其`main.go`\n\n```go\nhttps://github.dev/casdoor/casdoor/blob/a5a627f92efd3cc3f3613ca119b340b66258ce64/main.go#L65-L95\n\t....\n\tbeego.BConfig.WebConfig.Session.SessionOn = true\n\tbeego.BConfig.WebConfig.Session.SessionName = \"casdoor_session_id\"\n\tif conf.GetConfigString(\"redisEndpoint\") == \"\" {\n\t\tbeego.BConfig.WebConfig.Session.SessionProvider = \"file\"\n\t\tbeego.BConfig.WebConfig.Session.SessionProviderConfig = \"./tmp\"\n\t} else {\n\t\tbeego.BConfig.WebConfig.Session.SessionProvider = \"redis\"\n\t\tbeego.BConfig.WebConfig.Session.SessionProviderConfig = conf.GetConfigString(\"redisEndpoint\")\n\t}\n\tbeego.BConfig.WebConfig.Session.SessionCookieLifeTime = 3600 * 24 * 30 //30天的缓存时间\n\tbeego.BConfig.WebConfig.Session.SessionGCMaxLifetime = 3600 * 24 * 30 //30天的缓存时间\n\t// beego.BConfig.WebConfig.Session.SessionCookieSameSite = http.SameSiteNoneMode\n\n\terr := logs.SetLogger(logs.AdapterFile, conf.GetConfigString(\"logConfig\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tport := beego.AppConfig.DefaultInt(\"httpport\", 8000)\n\t// logs.SetLevel(logs.LevelInformational)\n\tlogs.SetLogFuncCall(false)\n\n\terr = util.StopOldInstance(port)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tgo ldap.StartLdapServer()\n\tgo radius.StartRadiusServer()\n\tgo object.ClearThroughputPerSecond()\n\n\tbeego.Run(fmt.Sprintf(\":%v\", port))\n```\n\n在上面的代码里面，我们可以看到默认情况下`casdoor`使用了`SessionProvider`为`file`，并且目录配置到了`./tmp`下面，后面启动的就是`beego`的服务了，所以后面查看的是对应版本[beego](https://github.dev/beego/beego/tree/v1.12.11)的源码。\n\n首先从`beego.go`开始启动服务\n\n```go\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/beego.go#L51-L73\n// Run beego application.\n// beego.Run() default run on HttpPort\n// beego.Run(\"localhost\")\n// beego.Run(\":8089\")\n// beego.Run(\"127.0.0.1:8089\")\nfunc Run(params ...string) {\n\tinitBeforeHTTPRun() //这里再程序启动前，调用了initBeforeHTTPRun这个方法\n\tif len(params) > 0 && params[0] != \"\" {\n\t\tstrs := strings.Split(params[0], \":\")\n\t\tif len(strs) > 0 && strs[0] != \"\" {\n\t\t\tBConfig.Listen.HTTPAddr = strs[0]\n\t\t}\n\t\tif len(strs) > 1 && strs[1] != \"\" {\n\t\t\tBConfig.Listen.HTTPPort, _ = strconv.Atoi(strs[1])\n\t\t}\n\n\t\tBConfig.Listen.Domains = params\n\t}\n\n\tBeeApp.Run()\n}\n....\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/beego.go#L91-L107\nfunc initBeforeHTTPRun() {\n\t//init hooks 在这个方法中注册了Session服务\n\tAddAPPStartHook(\n\t\tregisterMime,\n\t\tregisterDefaultErrorHandler,\n\t\tregisterSession,\n\t\tregisterTemplate,\n\t\tregisterAdmin,\n\t\tregisterGzip,\n\t)\n\n\tfor _, hk := range hooks {\n\t\tif err := hk(); err != nil { //在这里执行了注册的session服务\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n```\n\n查看对应的`registerSession`的代码实现，在`hook.go`中\n\n```go\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/hooks.go#L47-L76\nfunc registerSession() error {\n\tif BConfig.WebConfig.Session.SessionOn {\n\t\tvar err error\n\t\tsessionConfig := AppConfig.String(\"sessionConfig\")\n\t\tconf := new(session.ManagerConfig)\n\t\tif sessionConfig == \"\" {\n\t\t\t....\n            conf.Gclifetime = BConfig.WebConfig.Session.SessionGCMaxLifetime //定义了session的存活时间\n\t\t\tconf.ProviderConfig = filepath.ToSlash(BConfig.WebConfig.Session.SessionProviderConfig) //在这里tmp的配置变成了文件路径配置\n\t\t\t....\n\t\t} else {\n\t\t\tif err = json.Unmarshal([]byte(sessionConfig), conf); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif GlobalSessions, err = session.NewManager(BConfig.WebConfig.Session.SessionProvider, conf); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgo GlobalSessions.GC() //这里使用了GO协程启动了一个CG服务，该操作不会阻塞主协程\n\t}\n\treturn nil\n}\n```\n\n进入`GlobalSessions.GC() `函数查看对应实现，在`session.go`中\n\n```go\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/session/session.go#L290-L295\n// GC Start session gc process.\n// it can do gc in times after gc lifetime.\nfunc (manager *Manager) GC() {\n\tmanager.provider.SessionGC() //启动时先执行一次SessionCG 然后再使用time.AfterFunc来定时执行\n\ttime.AfterFunc(time.Duration(manager.config.Gclifetime)*time.Second, func() { manager.GC() })\n}\n```\n\n进入`manager.provider.SessionGC() `函数查看其实现，因为我们使用的`sessionProvider`为`file`，所以要在`session/sess_file.go`中查看其实现\n\n```go\n// filepder的定义\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/session/sess_file.go#L30-L33\nvar (\n\tfilepder      = &FileProvider{}\n\tgcmaxlifetime int64\n)\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/session/sess_file.go#L113-L118\n// FileProvider File session provider\ntype FileProvider struct {\n\tlock        sync.RWMutex\n\tmaxlifetime int64\n\tsavePath    string\n}\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/session/sess_file.go#L200-L207\n// SessionGC Recycle files in save path\nfunc (fp *FileProvider) SessionGC() {\n\tfilepder.lock.Lock() //这里先锁住了filepder 这个锁很关键\n\tdefer filepder.lock.Unlock()\n\n\tgcmaxlifetime = fp.maxlifetime //获取gc超时时间\n\tfilepath.Walk(fp.savePath, gcpath) //这里使用walk来浏览session文件目录下的文件，并且执行gcpath的回调\n}\n....\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/session/sess_file.go#L284-L296\n// remove file in save path if expired\nfunc gcpath(path string, info os.FileInfo, err error) error {\n\tif err != nil {\n\t\treturn err\n\t}\n\tif info.IsDir() {\n\t\treturn nil\n\t}\n\tif (info.ModTime().Unix() + gcmaxlifetime) < time.Now().Unix() {\n\t\tos.Remove(path) //如果该文件的修改时间加上超时时间大于当前时间则进行删除\n\t}\n\treturn nil\n}\n```\n\n到此第一个问题已经解决了，就是因为`beego.go`启动的时候会执行session的GC，而程序配置为利用`file`保存`session`，所以就解释了为什么查看`strace`和`pstack`的日志时会有锁`futex` 和文件读取的`newfstatat`的系统调用\n\n## 为什么服务日志都显示端口已经监听了，但是curl该服务还是不能马上返回请求\n\n为什么日志已经显示了端口绑定了，但是`curl`对应服务还是不通\n\n在上面我们得知我们的服务是在`beego.go`中启动的，所以先查看其源码\n\n```go\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/beego.go#L51-L73\n// Run beego application.\n// beego.Run() default run on HttpPort\n// beego.Run(\"localhost\")\n// beego.Run(\":8089\")\n// beego.Run(\"127.0.0.1:8089\")\nfunc Run(params ...string) {\n\tinitBeforeHTTPRun() //这里再程序启动前，调用了initBeforeHTTPRun这个方法\n\tif len(params) > 0 && params[0] != \"\" {\n\t\tstrs := strings.Split(params[0], \":\")\n\t\tif len(strs) > 0 && strs[0] != \"\" {\n\t\t\tBConfig.Listen.HTTPAddr = strs[0]\n\t\t}\n\t\tif len(strs) > 1 && strs[1] != \"\" {\n\t\t\tBConfig.Listen.HTTPPort, _ = strconv.Atoi(strs[1])\n\t\t}\n\n\t\tBConfig.Listen.Domains = params\n\t}\n\n\tBeeApp.Run() //正式启动服务\n}\n```\n\n我们这里直接查看`BeeApp.Run()`的实现\n\n```go\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/app.go#L62-L239\n// Run beego application.\nfunc (app *App) Run(mws ...MiddleWare) {\n\taddr := BConfig.Listen.HTTPAddr\n\n\tif BConfig.Listen.HTTPPort != 0 {\n\t\taddr = fmt.Sprintf(\"%s:%d\", BConfig.Listen.HTTPAddr, BConfig.Listen.HTTPPort)\n\t}\n\tapp.Server.Handler = app.Handlers //这里的app.Handlers就是定义好的路由和服务，把它注册到http服务的回调里面\n\t// run cgi server\n\tif BConfig.Listen.EnableFcgi {\n\t\t....\n\t}\n\t....\n\t// run graceful mode\n\tif BConfig.Listen.Graceful {\n        ...\n\t}\n\t// run normal mode\n\tif BConfig.Listen.EnableHTTPS || BConfig.Listen.EnableMutualHTTPS {\n\t\t...\n\t}\n\tif BConfig.Listen.EnableHTTP {\n\t\tgo func() {\n\t\t\tapp.Server.Addr = addr\n\t\t\tlogs.Info(\"http server Running on http://%s\", app.Server.Addr) //对应的casdoor日志打印的地方\n\t\t\tif BConfig.Listen.ListenTCP4 {\n                ...\n\t\t\t} else {\n\t\t\t\tif err := app.Server.ListenAndServe(); err != nil { //服务启动了\n\t\t\t\t\tlogs.Critical(\"ListenAndServe: \", err)\n\t\t\t\t\ttime.Sleep(100 * time.Microsecond)\n\t\t\t\t\tendRunning <- true\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\t<-endRunning\n}\n```\n\n那么当有请求过来的时候，就会调用`app.Handlers`中的`ServeHTTP`来处理(必须实现[HandlerFunc](https://cs.opensource.google/go/go/+/go1.24.2:src/net/http/server.go;l=2290) )，`app.Handlers`就在`route.go`里面\n\n```go\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/router.go#L693-L975\n// Implement http.Handler interface.\nfunc (p *ControllerRegister) ServeHTTP(rw http.ResponseWriter, r *http.Request) {\n\tstartTime := time.Now()\n\t....\n    https://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/router.go#L756-L770\n\t// session init 在这里将会处理请求中的session\n\tif BConfig.WebConfig.Session.SessionOn {\n\t\tvar err error\n\t\tcontext.Input.CruSession, err = GlobalSessions.SessionStart(rw, r) //通过rw和r获取对应的session信息\n\t\tif err != nil {\n\t\t\tlogs.Error(err)\n\t\t\texception(\"503\", context)\n\t\t\tgoto Admin\n\t\t}\n\t\tdefer func() {\n\t\t\tif context.Input.CruSession != nil {\n\t\t\t\tcontext.Input.CruSession.SessionRelease(rw)\n\t\t\t}\n\t\t}()\n\t}\n\t...\n}\n```\n\n`GlobalSessions.SessionStart`其实现的方法在`session/session.go`中\n\n```go\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/session/session.go#L207-L253\n// SessionStart generate or read the session id from http request.\n// if session id exists, return SessionStore with this id.\nfunc (manager *Manager) SessionStart(w http.ResponseWriter, r *http.Request) (session Store, err error) {\n    sid, errs := manager.getSid(r) // 在请求中获取sid (sessionId)\n\tif errs != nil {\n\t\treturn nil, errs\n\t}\n\n\tif sid != \"\" && manager.provider.SessionExist(sid) { // 如果获取到就用id来查看对应的sessionStore，可以通过sessionStore来管理sessoin\n\t\treturn manager.provider.SessionRead(sid)\n\t}\n\n\t// Generate a new session 如果获取不到session就重新生成session\n\tsid, errs = manager.sessionID()\n\tif errs != nil {\n\t\treturn nil, errs\n\t}\n\n\tsession, err = manager.provider.SessionRead(sid)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n    // 设置一个cookie，把sid放到cookie中，下次即可在请求中获取对应的sid，从而获取session\n\tcookie := &http.Cookie{\n\t\tName:     manager.config.CookieName,\n\t\tValue:    url.QueryEscape(sid),\n\t\tPath:     \"/\",\n\t\tHttpOnly: !manager.config.DisableHTTPOnly,\n\t\tSecure:   manager.isSecure(r),\n\t\tDomain:   manager.config.Domain,\n\t\tSameSite: manager.config.CookieSameSite,\n\t}\n\tif manager.config.CookieLifeTime > 0 {\n\t\tcookie.MaxAge = manager.config.CookieLifeTime\n\t\tcookie.Expires = time.Now().Add(time.Duration(manager.config.CookieLifeTime) * time.Second)\n\t}\n\tif manager.config.EnableSetCookie {\n\t\thttp.SetCookie(w, cookie)\n\t}\n\tr.AddCookie(cookie)\n\n\tif manager.config.EnableSidInHTTPHeader {\n\t\tr.Header.Set(manager.config.SessionNameInHTTPHeader, sid)\n\t\tw.Header().Set(manager.config.SessionNameInHTTPHeader, sid)\n\t}\n\n\treturn\n}\n```\n\n在上述的源码中，我们应该要关注里面的`SessionExist`、`SessionRead` 的实现，所以我们查看对应的源码，在`session/sess_file.go`中\n\n```go\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/session/sess_file.go#L176-L190\n// SessionExist Check file session exist.\n// it checks the file named from sid exist or not.\nfunc (fp *FileProvider) SessionExist(sid string) bool {\n\tfilepder.lock.Lock() // 这里跟上面的sessionCG一样，上锁了\n\tdefer filepder.lock.Unlock()\n\n\tif len(sid) < 2 {\n\t\tSLogger.Println(\"min length of session id is 2\", sid)\n\t\treturn false\n\t}\n\n\t_, err := os.Stat(path.Join(fp.savePath, string(sid[0]), string(sid[1]), sid))\n\treturn err == nil\n}\n\nhttps://github.com/beego/beego/blob/c280209bf5b94cf31094a200b166c97eb3898851/session/sess_file.go#L128-L176\n// SessionRead Read file session by sid.\n// if file is not exist, create it.\n// the file path is generated from sid string.\nfunc (fp *FileProvider) SessionRead(sid string) (Store, error) {\n\tinvalidChars := \"./\"\n\t...\n\tfilepder.lock.Lock() // 这里跟上面的sessionCG一样，上锁了\n\tdefer filepder.lock.Unlock()\n\n\terr := os.MkdirAll(path.Join(fp.savePath, string(sid[0]), string(sid[1])), 0755) //创建对应的文件夹\n\tif err != nil {\n\t\tSLogger.Println(err.Error())\n\t}\n    // 获取文件并写入session信息或者创建对应的文件并写入session信息\n\t_, err = os.Stat(path.Join(fp.savePath, string(sid[0]), string(sid[1]), sid)) \n\tvar f *os.File\n\tif err == nil {\n\t\tf, err = os.OpenFile(path.Join(fp.savePath, string(sid[0]), string(sid[1]), sid), os.O_RDWR, 0777)\n\t} else if os.IsNotExist(err) {\n\t\tf, err = os.Create(path.Join(fp.savePath, string(sid[0]), string(sid[1]), sid))\n\t} else {\n\t\treturn nil, err\n\t}\n\n\tdefer f.Close()\n\t// 修改文件Modify时间，在CG中也是通过该属性判断文件是否要被CG\n\tos.Chtimes(path.Join(fp.savePath, string(sid[0]), string(sid[1]), sid), time.Now(), time.Now())\n\tvar kv map[interface{}]interface{}\n    // 读取session信息\n\tb, err := ioutil.ReadAll(f)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(b) == 0 {\n\t\tkv = make(map[interface{}]interface{})\n\t} else {\n\t\tkv, err = DecodeGob(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tss := &FileSessionStore{sid: sid, values: kv}\n\treturn ss, nil\n}\n```\n\n所以我们这里就可以解释，为什么日志显示服务启动了，但是请求就是不行\n\n因为在一开始启动的时候，`beego`就会自己对`session`文件进行CG同时`lock`了一个`RWMutex`\n\n后面请求过来的时候，`beego`又会读取为了读`session`而上锁，但是如果前面的`CG`还未执行完成，那么这个就会被阻塞，直到可以上锁为止\n\n所以当`tmp`的`session`文件过多，那么启动的时候就要花大量时间进行`CG`导致请求被卡住，也说明为什么在生产环境中`nfs`的挂载进程会`cpu`过高，然后把`nfs`取消挂载后(`tmp`中的`session`被清空后)，启动后马上就可以正常访问了\n\n以下是在测试环境的模拟\n\n```shell\n[root@rocky-testing casdoor]# du -h --max-depth=1 ./shared-tmp/ \n....\n31G     ./shared-tmp/ # 这里弄了一个31G的session请求\n```\n\n重启一个`casdoor`服务后，`curl`该服务\n\n![log](./curl-8001.png)\n\n在第一个curl通了以后，后面的curl服务响应时间才会正常\n\n![log](./curl-8001-normal.png)\n\n## 为什么会产生这么多tmp文件\n\n为什么一天多的时间，`tmp`文件会产生很多的文件，占用比较大的存储？\n\n这就不得不提这个`casdoor`了，从上面我们知道`casdoor`可以让是用来解决用户在多个应用中的登录问题的，那么应用在用户登录的时候就要访问`casdoor`获取用户的信息，而刚好，我们在生产的环境中，有一个服务大概会每秒请求`casdoor`服务3次左右(后面改成利用缓存机制了)，同时`casdoor`内部的缓存超时时间配置设置成了30天🙁，而`beego`自己默认是1小时。\n\n这里又有一个问题，`beego`里面的`session/session.go`中不是有`manager.provider.SessionExist`来查看`sid`是否存在，不存在的话才创建新的。对的，这个对一般网页上使用的用户来说是没有问题的，因为浏览器会帮你把`cookie`放到每次的请求中，所以一个用户只用创建一个`sid`即可，但是这个是应用的请求，取决于编写应用的人有没有把`cookie`或者`sid`给你传递过来，如果没有传递就会出现每次请求都有重新创建一个`sid`和对应的`session`信息\n\n# 总结\n\n根据上述的分析，我画出来了整个请求流程图\n\n![log](./流程图.png)\n\n同时得出这次的问题**应该是**`beego`框架的问题🤥，但是`casdoor`也是存在这一些问题的~~（感觉像是最佳实践）~~\n\n首先`casdoor`的缓存设置成立30天，如果用的是`file`作为`sessionProvider`，当用户数量多的时候还是会有问题，或者使用`redis`来做`session`缓存，至少`io`性能比磁盘要快，`sessionCG`时间也快\n\n除了上面的问题，我们在其它地方也发现了一些问题\n\n比如在访问`/api/get-groups`时，对应的接口在调用的时候没有传分页参数，所以会全查数据表，刚好里面有个获取多条数据，然后循环多条数据并协程查询数据库，那个时候刚好我们弄了很多测试数据，导致该环境下调用该接口就会把数据库连接打满了。后面处理方案应该是更换了其它版本的`casdoor`，在目前环境的那个版本找不到对应的接口信息了。\n\n\n\n","tags":["排障","casdoor"]},{"title":"k8s中为Pod配置自签证书以访问HTTPS服务","url":"/2025/03/22/k8s中为Pod配置自签证书以访问HTTPS服务/","content":"\n> 最近在部署一套系统的其它环境时，遇到了自签https证书和应用自签证书到k8s中的问题，故此记录一下。\n>\n> 在部署的应用中，有一个需要使用Standard OIDC Client所有必须要https，而且要认证的服务不是部署在本集群中\n\n# 概念\n\n## Https相关概念\n\n什么是https，https证书的种类，如何自签https这些内容这里就不做展开了仅作一个自己的总结。因为网上的文章和视频太多了，这里推荐一下几位大佬的文章。\n\n* [有关 TLS/SSL 证书的一切 | 卡瓦邦噶！](https://www.kawabangga.com/posts/5330)\n* [HTTPS 隐私安全的一些实践](https://blog.laisky.com/p/https-in-action/#gsc.tab=0)\n* [写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 - This Cute World](https://thiscute.world/posts/about-tls-cert/)\n\n## 总结\n\n**以下总结部分摘抄与上述文章中**\n\n- **HTTPS是HTTP+TLS**，其中的TLS就是为了向客户端证明服务端自己的身份，避免别人伪造自己的身份对客户端造成影响。\n- **CA、客户端、网站**之间的关系\n  - 客户端信任 CA 机构；\n  - CA 机构给网站签发证书；\n  - 客户端在访问网站的时候，网站出示自己的证书，由于客户端信任 CA 机构，也就信任 CA 机构签发的证书；\n- PKI 架构使用「**数字证书链**（也叫做**信任链**）」的机制来解决这个问题: \n  - CA 机构首先生成自己的根证书与私钥，并使用私钥给根证书签名\n    - 因为私钥跟证书本身就是一对，因此根证书也被称作「自签名证书」\n  - CA 根证书被直接交付给各大软硬件厂商，内置在主流的操作系统与浏览器中\n  - 然后 CA 机构再使用私钥签发一些所谓的「中间证书」，之后就把私钥雪藏了，非必要不会再拿出来使用。\n    - 根证书的私钥通常**离线存储**在安全地点\n    - 中间证书的有效期通常会比根证书短一些\n    - 部分中间证书会被作为备份使用，平常不会启用\n  - CA 机构使用这些中间证书的私钥，为用户提交的所有 CSR 请求签名\n  - CA 机构也可能会在经过严格审核后，为其他机构签发中间证书，这样就能赋予其他机构签发证书的权利，而且根证书的安全性不受影响。\n  - 客户端要相信这个证书，最终的目的一定是验证到一个自己信任的 **CA Root** 上去。一旦验证了 **CA Root**，客户端就信任了 **Root** 签发的证书，也就信任了他们的证书。\n- 交叉签名\n  - 实际上在 PKI 体系中，一些证书链上的中间证书会被使用多个根证书进行签名——我们称这为交叉签名。交叉签名的主要目的是提升证书的兼容性——客户端只要安装有其中任何一个根证书，就能正常验证这个中间证书。\n  - 从而使中间证书在较老的设备也能顺利通过证书验证。\n- 证书状态认证 **OCSP协议**\n  - 为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。\n  - Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。\n  - 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。\n  - 为了解决这两个问题，[rfc6066](https://www.rfc-editor.org/rfc/rfc6066) 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。\n\n# 实践\n\n## 证书的生成和安装\n\n### 生成自签证书\n\n流程 \n\n- 生成私钥 key\n- 生成**证书生成请求csr**，可以在**csr**中配置要生成的证书的信息，如**countryName、stateOrProvinceName、organizationName、CA、SAN**\n  - **SAN (Subject Alternative Name):** 可选字段，用于指定证书可以使用的其他域名或主机名。\n    - 可以通过配置这个来让证书保护多个域名，例如 `example.com`、`www.example.com`、`*.example.com`。\n    - 可以使用这个配置来对对应的IP进行保护，即把SAN配置为对应的IP地址\n  - **CA**：配置为 `TRUE` 或 `FALSE`，它指示该证书是否可以作为证书颁发机构 (CA) 证书。\n    - **`CA:TRUE`**：表明该证书可以用于签署其他证书。 也就是说，这个证书可以作为一个 CA 证书，颁发下级证书，用于创建根 CA 证书或中间 CA 证书。 只有 CA 证书才能签署其他证书，建立信任链。\n    - **`CA=FALSE`**：表明该证书不能用于签署其他证书。 也就是说，这个证书是一个终端实体证书，只能用于身份验证、加密等目的，不能颁发下级证书，用于创建服务器证书、客户端证书、代码签名证书等。 这些证书用于保护特定的服务或应用程序，而不是用于颁发其他证书。\n- 通过`key+csr`生成对应的`crt`证书\n\n本文通过`openssl`来生成自签证书，以下是生成证书的脚本\n\n```shell\n#!/bin/bash\n\n# Check if domain name is provided\nif [ -z \"$1\" ]; then\n    echo \"Usage: $0 <domain>\"\n    exit 1\nfi\n\nDOMAIN=$1\n# 证书过期日期\nDAYS=3650\nCOUNTRY=\"CN\"\nSTATE=\"GZ\"\nCERT_DIR=\"./certs\"\nCONFIG_FILE=\"$CERT_DIR/openssl.cnf\"\n\n# Create directory for certificates if it doesn't exist\nmkdir -p $CERT_DIR\n\n# 生成 private key\nopenssl genrsa -out $CERT_DIR/${DOMAIN}.key 2048\n\n# Create OpenSSL configuration file 通过该配置文件生成对应的csr\ncat > $CONFIG_FILE <<EOL\n[ req ]\ndistinguished_name = req_distinguished_name\nprompt = no\ndefault_bits = 2048\nreq_extensions = cert_ext\n\n[ req_distinguished_name ]\ncountryName         = $COUNTRY\nstateOrProvinceName = $STATE\nlocalityName        = $STATE\norganizationName    = $STATE\ncommonName          = $DOMAIN\n\n[ cert_ext ]\nbasicConstraints = CA:TRUE # CA配置 \nkeyUsage = digitalSignature, keyEncipherment\nextendedKeyUsage = serverAuth, clientAuth\nsubjectAltName = @alt_names\n# SAN配置\n[ alt_names ]\nDNS.1 = $DOMAIN\nDNS.2 = *.$DOMAIN\n\nEOL\n\n# Generate CSR with extensions 生成csr\nopenssl req -new -key $CERT_DIR/${DOMAIN}.key -out $CERT_DIR/${DOMAIN}.csr -config $CONFIG_FILE\n\n# Generate self-signed certificate with both extensions 生成crt\nopenssl x509 -req -days $DAYS \\\n    -in $CERT_DIR/${DOMAIN}.csr \\\n    -signkey $CERT_DIR/${DOMAIN}.key \\\n    -out $CERT_DIR/${DOMAIN}.crt \\\n    -extensions cert_ext \\\n    -extfile $CONFIG_FILE\n\necho \"Certificate and key have been generated in the $CERT_DIR directory.\"\n```\n\n对应生成出来的证书，可以使用以下命令来进行查看\n\n```shell\n# 查看证书(crt)信息\nopenssl x509 -noout -text -in server.crt\n\n# 查看证书请求(csr)信息\nopenssl req -noout -text -in server.csr\n\n# 查看 RSA 私钥(key)信息\nopenssl rsa -noout -text -in server.key\n\n# 验证证书是否可信\n## 1. 使用系统的证书链进行验证\nopenssl verify server.crt\n## 2. 使用指定的 CA 证书进行验证\nopenssl verify -CAfile ca.crt server.crt\n```\n\n### 为系统安装自签证书\n\n这里只介绍 RockyLinux9.3 和 Debain12 。因为我的系统是`RockyLinux9.3`，pod的镜像是 `Debain12`其它的大差不差。\n\nRockyLinux9.3\n\n- 为RockyLinux安装自签证书要有一个注意点，既要把证书中的 **CA** 设置为 **TRUE**\n- 复制证书到 `/etc/pki/ca-trust/source/anchors/` 执行 `update-ca-trust `\n- 就会生成包含私有证书的`ca-bundle.crt`到`/etc/pki/tls/certs/`中\n\nDebain12\n\n- 复制证书到 `/usr/local/share/ca-certificates/` 执行 `update-ca-certificates --fresh`\n- 就会生包含私有证书的`ca-certificates.crt` 到 `/etc/ssl/certs/`中\n\n安装完成以后，就可以使用`curl`命令对证书进行验证\n\n```shell\n# 在把证书配置到web服务后，可以使用curl命令来查看证书是否匹配\ncurl -v https://xxx.test.com --cacert /etc/ssl/certs/xxx.test.com.crt \n```\n\n## 为Pod配置证书\n\n- 本次将会使用两种方式来进行配置，一种是通过`init-container`的方式来安装证书，另一种是使用`certs-manager`中的`trust-manager`来自动管理pod的证书\n\n- 应用部署环境：项目使用的是`Kustomize`来部署不同的环境。项目中的代码：[k8s_demo/certs/app at main · kehaha-5/k8s_demo](https://github.com/kehaha-5/k8s_demo/tree/main/certs/app)\n\n\n```shell\n[root@rocky-testing app]# tree -L 3\n.\n├── debianupcaDockerfile\n├── deploy\n│   ├── base\n│   │   ├── deployment.yaml\n│   │   └── kustomization.yaml\n│   └── overlays\n│       ├── prod # 生成环境用的是CA的证书\n│       ├── uat # 使用init-container安装证书\n│       └── uat-trust # 使用trust-manager\n├── Dockerfile\n├── go.mod\n├── go.sum\n├── main.go\n└── makefile\n\n6 directories, 8 files\n```\n\n### 在镜像系统中安装证书\n\n根据上述的为系统安装证书的步骤，我们可以使用 `init-container` 把证书安装到应用的`pod`中。注意一些镜像是没有默认安装`ca-certificates ` 需要额外安装或者把物理机的证书`COPY`到镜像内\n\n1. 首先先把证书crt apply到k8s的secret中\n\n```shell\nkubectl create secret generic ssl-test-web-crt --from-file=./ssl.test.com.crt --dry-run=client  -o yaml  | kubectl apply -f -\n```\n\n2. 使用`init-container` 把证书生成到`ca-certificates.crt`中，同时把新的证书挂载到应用的`Pod`中\n\n```yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n....\n\npatchesJson6902:\n  - target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: https-client\n    patch: |-\n    .....\n      - op: add\n      # 把包含证书的secret挂载到volumes中\n        path: /spec/template/spec/volumes/-\n        value:\n          name: ssl-test-web-crt\n          secret:\n            secretName: ssl-test-web-crt\n      - op: add\n      # 把生成好的ca-certificates.crt挂载到应用上\n        path: /spec/template/spec/containers/0/volumeMounts/-\n        value:\n          name: ssl-certs\n          mountPath: /etc/ssl/certs/\n      - op: add\n      # 使用emptyDir来临时保存生成的ca-certificates.crt\n        path: /spec/template/spec/volumes/-\n        value:\n          name: ssl-certs\n          emptyDir: {}\n      - op: add\n        path: /spec/template/spec/initContainers\n        value:\n          - name: init-container\n            image: debian\n            command: [\"/bin/sh\", \"-c\", \"update-ca-certificates --fresh && cp -r /etc/ssl/certs/* /mnt/ssl-certs/\" ]\n            volumeMounts:\n              - name: ssl-test-web-crt\n                mountPath: /usr/local/share/ca-certificates/\n              - name: ssl-certs\n                mountPath: /mnt/ssl-certs\n```\n\n3. 使用 `kubectl apply -k ./deploy/overlays/uat/`部署该环境\n\n查看日志对应的日志，发现`init-container`生成新证书，并且应用https访问成功\n\n![log](.\\uat-log.png)\n\n### 使用trust-manager\n\n在`certs-manager`的文档中有一个叫`trust-manager`的组件[trust-manager - cert-manager Documentation](https://cert-manager.io/docs/trust/trust-manager/)\n\n>trust-manager is the easiest way to manage trust bundles in Kubernetes and OpenShift clusters.\n>\n>It orchestrates bundles of trusted X.509 certificates which are primarily used for validating certificates during a TLS handshake but can be used in other situations, too.\n\n- 你可以单独安装`trust-manager`，使用里面的`bundles`来配置证书https://cert-manager.io/docs/trust/trust-manager/installation/#installing-trust-manager-without-cert-manager\n\n- 也可以安装`certs-manager`和`trust-manager`，使用`certs-manager`生成自签证书，然后配置到`trust-manager`中 https://cert-manager.io/docs/trust/trust-manager/#quick-start-example\n\n我这里直接只安装`trust-manager`使用`bundles`来配置证书\n\n- 先通过`helm`安装`trust-manager`\n  - 注意安装的时候有一个`app.trust.namespace`的配置(默认值为cert-manager)，这个配置定义了那些`namespace`下的`Secret`可以被读取，这样保证了应用的安全 https://cert-manager.io/docs/trust/trust-manager/installation/#trust-namespace\n\n> By default, the trust namespace is the only namespace where`Secret`s will be read. This restriction is in place for security reasons - we don't want to give trust-manager the permission to read all `Secret`s in all namespaces. With additional configuration, secrets may be read from or written to other namespaces.\n\n```yaml\nhelm repo add jetstack https://charts.jetstack.io --force-update\nhelm upgrade trust-manager jetstack/trust-manager \\\n  --install \\\n  --namespace cert-manager \\\n  --wait\n# 可以利用 helm template 命令生成对应的helm value \nhelm template \\\n  trust-manager jetstack/cert-manager \\\n  --namespace cert-manager \n  > cert-manager.custom.yaml\n```\n\n- 先把证书导入到`trust-namespace`的 \n\n```yaml\nkubectl create secret generic ssl-test-web-crt --from-file=./ssl.test.com.crt --dry-run=client -n cert-manager  -o yaml  | kubectl apply -f -   \n```\n\n- 编写对应的`Bundle`\n  - `Bundle`会自己在对应的`namespcae`下面生成一个 `public-bundle` 的 `configMap`\n\n```yaml\napiVersion: trust.cert-manager.io/v1alpha1\nkind: Bundle\nmetadata:\n  name: public-bundle\nspec:\n  sources:\n# 为true会帮你更新对应容器中的CA证书\n# https://cert-manager.io/docs/trust/trust-manager/#securely-maintaining-a-trust-manager-installation\n  - useDefaultCAs: true\n  - secret:\n      name: \"ssl-test-web-crt\"\n      key: \"ssl.test.com.crt\"\n  target:\n    configMap:\n      key: \"ca-certificates.crt\"\n# 这里要给对应要生成 public-bundle configMap 的namespace 打上label\n# kubectl label ns default trust=enabled\n    namespaceSelector:\n      matchLabels:\n        trust: enabled\n```\n\n- 我们只需要在对应的容器中挂载`public-bundle`到`/etc/ssl/certs`\n\n```yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n- ../../base\n\n.....\n\npatchesJson6902:\n  - target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: https-client\n    patch: |-\n.....\n      - op: add\n        path: /spec/template/spec/volumes/-\n        value:\n        #声明Volumes\n          name: public-bundle\n          configMap:\n            name: public-bundle\n            defaultMode: 0644\n            optional: false\n      - op: add\n        path: /spec/template/spec/containers/0/volumeMounts/-\n        value:\n        # VolumeMounts 挂载到/etc/ssl/certs/\n          name: public-bundle\n          mountPath: /etc/ssl/certs/\n          readOnly: true\n\nimages:\n  - name: https-client\n    newTag: v1.0\n    newName: https-client\n```\n\n- 如果成功了，在容器中就会如下显示\n\n```shell\nroot@https-client-6b74cfbddd-2nw2r:/app# ls -ltr /etc/ssl/certs/ca-certificates.crt              \nlrwxrwxrwx. 1 root root 26 Mar 23 06:26 /etc/ssl/certs/ca-certificates.crt -> ..data/ca-certificates.crt\n```\n\n# 最后\n\n对比上述的两种方法\n\n第一种方法我觉得比较简单，无须额外安装其它组件，但是对配置的入侵性比较高，需要额外配置一个有`ca-certificates`的`init-container`\n\n第二种方法需要额外安装多一个`trust-manager`，但是落实到具体的`Pod`配置只需要额外挂载一个 `ConfigMap` 到对应的证书路径即可，同时对应的`bundle/public-bundle`也是支持自动更新的，即你的`secret`更新了，它也会自动同步。\n\n```shell\n[root@rocky-testing ~]# kubectl get events\nLAST SEEN   TYPE     REASON              OBJECT                               MESSAGE\n52m         Normal   Synced              bundle/public-bundle                 Successfully synced Bundle to namespaces that match this label selector: trust=enabled\n```\n\n当然除了上述两种方法，还有没有**更简单的方法，甚至不需要额外的配置**🤔\n\n有的！如果你公司有一份支持多域名的证书，就可以直接使用该 crt(一般叫做xxx_chain.crt或者.pem) 和 key\n\n如何查看？使用 `openssl x509 -noout -text -in server.crt` 查看证书信息 类似如下\n\n```text\n            ·······\n # 如果 SAN (Subject Alternative Name) 中包含多个域名 或者 通配符 *.test.com \n            X509v3 Subject Alternative Name: \n                DNS:*.test.com, DNS:test.com, DNS:abc.test.com\n    Signature Algorithm: sha256WithRSAEncryption\n    Signature Value:\n    ······\n```\n\n这样你就可以用 `abc.test.com` 等类似的`xxx.test.com` 的二级域名 (注意三级不行 如 `xxx.adb.test.com`)\n\n再通过修改hosts或者k8s中的coredns，就可以使用不需要额外配置使用`Https`了 😊","tags":["k8s","certs","边做边学"]},{"title":"Mysql执行过长排查","url":"/2025/01/05/Mysql执行过长排查/","content":"\n\n> 开发反应说应用的某个接口返回过慢，查看日志后发现确实有一个接口的sql执行了20s\n\n![log](.\\log.jpg)\n\n# 排查\n\n## sql优化\n\n输入sql执行发现执行时间很短而且整个数据库的数据量不是很大，所以可以排查sql优化的问题\n\n## 网络排查\n\n环境：应用在143上面，数据库在142上面，数据库端口13360\n\n利用tcpdump+wireshark 进行抓包\n\n```shell\ntcpdump -i bond4.128 port 13360 -w ./mysql_cap\n```\n\n使用wireshark打开选中可用的数据流，找到对应的sql数据tcp流\n\n![cap](.\\cap.jpg)\n\n分析发现从tcp握手到数据库密码认证，经历了20秒(52-105)，后面执行数据库用的时间并不多(105-116)\n\n![cap1](.\\cap1.jpg)\n\n可以断定是数据库初始化连接的问题，查询相关信息发现有一个相关配置\n\n```shell\n[mysqld]\nskip-name-resolve\n```\n\n修改该配置后，数据访问正常了\n\n# 扩展\n\n## 作用\n\n那个这个 `skip-name-resolve`的配置有什么作用 为什么会导致链接超时\n\n这个主要作用是把ip变成host 或者 host映射成ip的功能，然后放到一个host cache里面 [相关文档](https://dev.mysql.com/doc/refman/8.4/en/host-cache.html)\n\n那Mysql为什么要这样做捏？\n\n因为Mysql会用这个host cache来追踪这个host的错误情况，对应的相关配置 `max_connect_errors` \n\n![doc](.\\doc.png)\n\n## 流程\n\n详细的数据库连接和对应的host cache建立 也在文档里面有说明\n\n大概流程\n\n+ 第一次tcp clinet连接到达的时候，Mysql就会初始化对应的cache实例\n+ server就会尝试去把 IP to host 或者 name to host 使用 DNS 解析\n+ 如果该连接发生错误 就会把数据记录下来 \n\n![doc](.\\doc1.png)\n\n## 最后\n\n这就是为什么在tcp三次握手后会等待大概20秒左右，就是mysql尝试把连接ip解析host的行为\n\n当然了，这个ip是解析不到host的所以就会出现错误，在docker log 可以看到对应错误\n\n![mysql-log](.\\mysql-log.png)\n为什么说是`172.17.0.1`这个ip解析错误捏，不是上述的143这个ip？\n\n那是因为mysql是跑在docker上面的 而流量会经过docker的网卡再到容器里面，而docker的网卡就是`172.17.0.1`\n\n![docker-bridge](.\\docker-bridge.png)\n\n","tags":["mysql","排障"]},{"title":"k8s-jenkins-构建持续交付流水线","url":"/2024/08/23/k8s-jenkins-构建持续交付流水线/","content":"\n> 本次配置和代码都在[github](https://github.com/kehaha-5/k8s_demo)上 \n\n使用方法:\n\n> `git clone https://github.com/kehaha-5/k8s_demo`\n> `cd k8s_demo`\n> `git fetch --all`\n> `git checkout -b tag 2.0`\n\n# 概要\n\n## 环境\n\n本次的环境跟上一章[从零到一--搭建web项目集群 - kehaha-5](https://kehaha-5.github.io/2024/08/16/从零到一--搭建web项目集群/)的环境一摸一样，都是使用`k3s`来进行集群的搭建\n\n## 流程\n\n这次用到了k8s集群+jenkins+harbor构建持续交付流水线\n\n1. 手动在jenkins中触发构建\n2. 拉去`git`项目代码\n3. 分别构建`vue3`前端 和 `go` 后端项目 [k8s_demo/web at main · kehaha-5/k8s_demo (github.com)](https://github.com/kehaha-5/k8s_demo/tree/main/web)\n4. 修改`k8s`部署文件，添加版本号和 `kubernetes.io/change-cause`\n5. 更新`k8s`部署\n\n这次没有私有化部署`gitlab`是因为没有用到`webhook`，是直接通过jenkins直接触发构建然后拉去的代码\n\n# Harbor\n\n首先要先部署`harbor`作为私有镜像仓库，方便上传后续构建的私有镜像和所需要的镜像。\n\n这次部署会去配置harbor的`tls ` [Harbor docs | Configure HTTPS Access to Harbor (goharbor.io)](https://goharbor.io/docs/2.0.0/install-config/configure-https/) 按着该文档就可以生成对应的域名证书，并且要配置`k3s`的私有仓库配置[Private Registry Configuration | K3s](https://docs.k3s.io/installation/private-registry)，同时在我的`ubuntu`系统上面还要进行`update-ca-certificates` [Harbor docs | Troubleshooting Harbor Installation (goharbor.io)](https://goharbor.io/docs/2.0.0/install-config/troubleshoot-installation/)\n\n在生成好证书以后还要把证书配置到`secret`\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-harbor-tls\n  namespace: devops-tools  \ntype: kubernetes.io/tls\ndata:\n  tls.crt: |\n    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0t....\n  tls.key: |\n    LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tL....\n    \n```\n\n这次安装是通过`helm`来安装，所以需要修改`value.yaml`来进行详细配置 [Harbor docs | Deploying Harbor with High Availability via Helm (goharbor.io)](https://goharbor.io/docs/2.0.0/install-config/harbor-ha-helm/)\n\n这里说一下我的`expose`配置，我是要通过`ingress`来进行对外暴露的，因为我自己生成了tls证书，所以`certSource`要改成`secret`，并且把`secretName: `改成我上述配置的secret中\n\n```yaml\n  ....\n  type: ingress\n  tls:\n    enabled: true\n    # The source of the tls certificate. Set as \"auto\", \"secret\"\n    # or \"none\" and fill the information in the corresponding section\n    # 1) auto: generate the tls certificate automatically\n    # 2) secret: read the tls certificate from the specified secret.\n    # The tls certificate can be generated manually or by cert manager\n    # 3) none: configure no tls certificate for the ingress. If the default\n    # tls certificate is configured in the ingress controller, choose this option\n    certSource: secret\n    auto:\n      # The common name used to generate the certificate, it's necessary\n      # when the type isn't \"ingress\"\n      commonName: \"\"\n    secret:\n      # The name of secret which contains keys named:\n      # \"tls.crt\" - the certificate\n      # \"tls.key\" - the private key\n      secretName: \"my-harbor-tls\"\n\tingress:\n      hosts:\n        core: harbor.k8s.demo\n    ....\n```\n\n其他的按需配置即可\n\n![harbor](.\\1.png)\n\n在成功部署以后，先把项目需要到的镜像进行构建和上传\n\n```Dockerfile\nFROM alpine:3.20.2\nRUN set -eux && sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories\nRUN apk update && apk add tzdata\n```\n\n先构建后端基础镜像，到时候就流水线中直接使用，不需要而外下载所需要的包，节约流水线时间\n\n同时在`docker login`以后把配置文件保存到`Secret`里面，方便后续在流水线中进行镜像上传\n\n```yaml\n#harborLoginSecret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: harbor-login\n  namespace: devops-tools  # Specify your namespace\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJoYXJib3IuazhzLmRlbW8iOiB7CgkJCSJhdXRoIjogIllXUnRhVzQ2U0dGeVltOXlNVEl6TkRVPSIKCQl9Cgl9Cn0=\n```\n\n# Jenkins\n\n这里按着官方文档[Kubernetes (jenkins.io)](https://www.jenkins.io/doc/book/installing/kubernetes/)在k8s中部署jenkins\n\n其中因为jenkins在流水线中会使用到`kubernetes`的`agents` [Using Jenkins agents](https://www.jenkins.io/doc/book/using/using-agents/)，所以这里再service中要同时代理`50000`端口的流量，让`agents`可以和`jenkins`可以进行连接 [Jenkins Remoting](https://www.jenkins.io/projects/remoting/)\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: jenkins-service\n  namespace: devops-tools\n  annotations:\n      prometheus.io/scrape: 'true'\n      prometheus.io/path:   /\n      prometheus.io/port:   '8080'\nspec:\n  selector:\n    app: jenkins-server\n  type: NodePort\n  ports:\n    - port: 8080\n      name: httpport\n      targetPort: 8080\n      nodePort: 32000\n    - port: 50000\n      name: jnlpport\n      targetPort: 50000\n      nodePort: 32500\n```\n\n![jenkins](.\\2.png)\n\n## git配置\n\n这里要去系统配置里面的凭证里面配置一个`SSH Username with private key`，这样才可以通过`git`来拉去私有仓库\n\n同时还要再`安全配置`里面配置`Git Host Key Verification Configuration` 把它配置为**Accept first connection**\n\n> Controls how Git plugin verifies the keys presented by the host during SSH connecting.\n>\n> - Known hosts file (default)\n>\n>   Verifies all host keys using the `known_hosts` file.\n>\n> - Accept first connection\n>\n>   Automatically adds host keys to the `known_hosts` file if the host has not been seen before, and does not allow connections to previously-seen hosts with modified keys.\n>\n>   - Note that when using ephemeral agents (ex. cloud agents), this strategy is essentially equivalent to **No verification** because it uses the `known_hosts` file on the agent. To avoid this, you can pre-configure `known_hosts` with all relevant hosts when creating the images or templates used to define your agents, or use the **Manually provided keys** or **Known hosts file** strategies.\n>\n>   - OpenSSH version 7.6 or higher is required to use this option with command line Git.\n>\n> - Manually provided keys\n>\n>   Verifies all host keys using a set of keys manually configured here.\n>\n> - No verification (not recommended)\n>\n>   Does not verify host keys at all.\n\n## Kubernetes \n\n对应k8s的配置，先要去下载Kubernetes的插件[Kubernetes | Jenkins plugin](https://plugins.jenkins.io/kubernetes/)\n\n并且在系统的`clouds`中添加一个k8s集群，其中有一个`Kubernetes 地址`可以不用进行配置，因为我的jenkins是部署在k8s集群里面的，在每个pods都可以访问其自身的k8s集群 [从 Pod 中访问 Kubernetes API | Kubernetes](https://kubernetes.io/zh-cn/docs/tasks/run-application/access-api-from-pod/)\n\n> **直接访问 REST API**\n>\n> 在运行在 Pod 中时，你的容器可以通过获取 `KUBERNETES_SERVICE_HOST` 和 `KUBERNETES_SERVICE_PORT_HTTPS` 环境变量为 Kubernetes API 服务器生成一个 HTTPS URL。 API 服务器的集群内地址也发布到 `default` 命名空间中名为 `kubernetes` 的 Service 中， 从而 Pod 可以引用 `kubernetes.default.svc` 作为本地 API 服务器的 DNS 名称。\n\n### 在K8S中动态创建代理\n\nJenkins构建项目时，并行构建，如果多个项目同时构建就会有等待。所以这里采用`master/slave`架构\n\n这里官方是提供了基础的`slave`镜像 [jenkins/inbound-agent - Docker Image | Docker Hub](https://hub.docker.com/r/jenkins/inbound-agent/)\n\n但是我是需要`node`和`go`环境来进行项目构建的，所以这里需要在官方镜像的基础上面添加所需要的构建工具\n\n```dockerfile\nFROM jenkins/inbound-agent:alpine3.20-jdk17\n\n# 切换到 root 用户\nUSER root\n\nRUN set -eux && sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update  &&\\\napk add nodejs npm && npm config set registry https://registry.npmmirror.com/ && \\\nnpm install @vue/cli@5.0.8\n\nCOPY go1.22.6.linux-amd64.tar.gz /app/go1.22.6.linux-amd64.tar.gz\nCOPY kubectl /usr/local/bin/\n\nRUN rm -rf /usr/local/go && \\\n    tar -C /usr/local -xzf /app/go1.22.6.linux-amd64.tar.gz && \\\n    ln -s /usr/local/go/bin/go /usr/local/bin/go && rm -rf /app/go1.22.6.linux-amd64.tar.gz\n\nUSER jenkins\n\n```\n\n把它打包成镜像并上传到私有的`harbor`中\n\n### 在K8s中的权限配置\n\n因为在流水线中，会使用`kubectl`对不同`namespace`的deployment进行操作所以需要有对应的权限\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-prod\n  namespace: devops-tools\n\n--- \n\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: prod\n  name: prod-pod-management\nrules:\n- apiGroups: [\"*\"] \n  resources: [\"deployments\"]\n  verbs: [\"*\"]\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: jenkins-prod-rolebinding\n  namespace: prod\nsubjects:\n- kind: ServiceAccount\n  name: jenkins-prod\n  namespace: devops-tools\nroleRef:\n  kind: Role\n  name: prod-pod-management\n  apiGroup: rbac.authorization.k8s.io\n\n\n```\n\n这里创建了一个`jenkins-prod`的用户，并且可以操作 `namespace`为 ` prod`下的`deployments`所有行为\n\n- 我这里`jenkins-prod`只需要操作`prod` 下的权限所以使用`Role`\n- 如果想操作任意`namespace`下的资源 使用`ClusterRole `\n- [使用 RBAC 鉴权 | Kubernetes](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/#clusterrole-example)\n\n### 打包镜像\n\n我的jenkinis是部署在k8s的环境中，无法使用`docker`来进行打包，所以这里使用`kaniko`[GoogleContainerTools/kaniko: Build Container Images In Kubernetes (github.com)](https://github.com/GoogleContainerTools/kaniko)来进行镜像打包和上传到私有镜像仓库，所以需要提前准备一份`docker login`的`Secret`\n\n## 流水线\n\nok，在一切都准备好了就可以创建一个流水线项目，并编写对应流水线代码\n\n这里需要注意几点：\n\n- 我要定义了`parameters`就是在执行流水线前，要输入本次构建的版本号和镜像地址配置，这样打包出来的镜像都会存在版本信息，方便追踪和管理\n\n```groovy\n    parameters {\n        string(name: 'FRONTEND_VERSION', defaultValue: '1.0.0', description: '前端版本号')\n        string(name: 'BACKEND_VERSION', defaultValue: '1.0.0', description: '后端版本号')\n        string(name: 'HARBOR_IP', defaultValue: '192.168.1.120', description: 'harbor仓库ip')\n        string(name: 'HARBOR_REGISTRY', defaultValue: 'harbor.k8s.demo', description: 'harbor仓库地址')\n    }\n```\n\n- 在使用`agents`时要进行一定的配置\n\n  - 缓存配置，通过nfs挂载`go`和`npm`的目录来进行资源缓存\n\n  - 配置`serviceAccountName`为我创建的`jenkins-prod` ，`serviceAccountName`可以为pod指定一个账号，如果不指定那么账号将会时该`namespace`下的`default `它是没有权限操作其他`namesapce`下的资源的 [为 Pod 配置服务账号 | Kubernetes](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-service-account/)\n\n  - 因为在这个镜像里面，执行的用户是`jenkins`，但是挂载缓存的权限是`root`,所以是无法访问到的，但是通过修改`securityContext.fsGroup`容器中所有进程会是其附组ID的一部分，所以这样就可以把缓存的权限分配到`jenkins`中，使其有足够的权限操作缓存\n\n    - 那为什么是1000捏？因为在镜像定义的时候，就指定了容器内用户和组ID 都为1000\n\n      ![alpine3.20-jdk17](.\\3.png)\n\n```groovy\n    agent {\n        kubernetes {\n            yaml '''\n            apiVersion: v1\n            kind: Pod\n            metadata:\n              name: jenkins-slave\n            spec:\n              serviceAccountName: jenkins-prod\n              securityContext:\n                fsGroup: 1000\n              containers:\n              - name: jnlp\n                image: harbor.k8s.demo/library/jenkins/inbound-agent-k8s-demo:alpine3.20-jdk17\n                resources:\n                  limits:\n                    memory: \"1Gi\"\n                    cpu: \"1000m\"\n                  requests:\n                    memory: \"256Mi\"\n                    cpu: \"250m\"\n                volumeMounts:  \n                  - name: go-cache\n                    mountPath: /home/jenkins/go\n                  - name: npm-cache\n                    mountPath: /home/jenkins/.npm\n              volumes:  \n              - name: go-cache\n                nfs:\n                  server: 192.168.1.125  \n                  path: /mnt/nfs_share/go-cache  \n              - name: npm-cache\n                nfs:\n                  server: 192.168.1.125  \n                  path: /mnt/nfs_share/npm-cache  \n            '''\n        }\n    }\n```\n\n- 在构建镜像时，使用了`kaniko`作为`agents`\n  - 这里使用的版本为`debug`因为流程是启动`agents`，等待jnlp对其进行连接，连接成功了才可以使用，所以不能让pod退出，而`debug`的版本是有` busybox shell ` 可以使用`sleep 99d`是pod不会退出\n  - 这里就是挂载了上述配置的`docker login`后的`config.json`文件，保证了推送到私有仓库的权限\n\n```groovy\n\tagent {\n                kubernetes {\n                    yaml '''\n                    apiVersion: v1\n                    kind: Pod\n                    metadata:\n                      labels:\n                        app: kaniko\n                    spec:\n                      containers:\n                      - name: kaniko\n                        image: harbor.k8s.demo/library/kaniko-project/executor:v1.23.2-debug\n                        command: [\"sleep\",\"99d\"]\n                        volumeMounts:\n                        - name: kaniko-secret\n                          mountPath: /kaniko/.docker\n                      volumes:\n                        - name: kaniko-secret\n                          secret:\n                            secretName: harbor-login\n                            items:\n                              - key: .dockerconfigjson\n                                path: config.json\n                    '''\n                }\n            }\n```\n\n- 构建镜像命令\n  - 在构建的时候需要把所需要的文件从上一个`steps`中获取，这里使用了`stash`和`unstash`  [Pipeline: Basic Steps (jenkins.io)](https://www.jenkins.io/doc/pipeline/steps/workflow-basic-steps/#stash-stash-some-files-to-be-used-later-in-the-build)\n    - 在小文件的时候可以这样使用，但是如果文件过多或太大，会影响流水线执行的效率，这个时候可以考虑使用外部挂载，给构建的agents添加一个新的挂载，把打包的所需文件复制到其中，在打包pod中挂载该`volumne`也可以获取所需的文件\n  - 这里把版本配置到了镜像中，这样可以通过配置文件快速了解其对应的后端和前端的版本\n\n```groovy\n            steps {\n                parallel (\n                    '构建前端镜像': {\n                        unstash 'frontend-artifact'\n                        container('kaniko') {\n                            sh \"echo ${params.HARBOR_IP} ${params.HARBOR_REGISTRY} >> /etc/hosts\"\n                            sh \"\"\"\n                                /kaniko/executor  --context `pwd`/web/k8s/front/docker -f `pwd`/web/k8s/front/docker/Dockerfile \\\n                                --skip-tls-verify \\\n                                --force \\\n                                --destination ${params.HARBOR_REGISTRY}/k8s_demo/front:${params.FRONTEND_VERSION}  \n                            \"\"\"\n                        }\n                    },\n                    '构建后端镜像': {\n                        unstash 'backend-artifact'\n                        container('kaniko') {\n                            sh \"echo ${params.HARBOR_IP} ${params.HARBOR_REGISTRY} >> /etc/hosts\"\n                            sh \"\"\"\n                                /kaniko/executor --context `pwd`/web/k8s/back/docker --dockerfile `pwd`/web/k8s/back/docker/Dockerfile \\\n                                --skip-tls-verify \\\n                                --force \\\n                                --destination ${params.HARBOR_REGISTRY}/k8s_demo/back:${params.BACKEND_VERSION} \n                            \"\"\"\n                        }\n                    }\n                )\n            }\n        }\n```\n\n- k8s更新部署\n\n  - 这里利用了`annotate`给每个版本都`rollout histroy`都配置了`change-cause` 方便后期进行版本回退和查看\n\n  - 同时利用了`rollout restart`来进行滚动更新\n\n    - 滚动更新是 Kubernetes 提供的一种部署更新策略，它逐步替换 Pod 的旧版本，确保在更新过程中始终保持应用程序的可用性。\n\n      - **零停机时间 (Zero Downtime):** 这是滚动更新最大的优势。在更新过程中，旧 Pod 会逐步被新 Pod 替换，而不会一次性停止所有旧 Pod。这保证了应用在更新过程中始终至少有一部分实例在运行，避免了服务中断。\n      - **控制更新速度:** 你可以控制滚动更新的速度，例如一次更新多少个 Pod，以及每次更新之间等待的时间。这让你可以根据应用的实际情况和风险承受能力来调整更新节奏。\n      - **回滚能力:** 如果新版本出现问题，你可以轻松地回滚到之前的版本。Kubernetes 会保留旧 Pod 的历史版本，方便你快速回滚。\n\n    - **滚动更新的配置**\n\n      在 Kubernetes 中，你可以通过 Deployment 对象来管理应用的部署和更新。以下是配置滚动更新的一些关键参数：\n\n      - **`strategy.type: RollingUpdate`**: 指定使用滚动更新策略。\n      - **`strategy.rollingUpdate.maxSurge`**: 指定在更新过程中，可以额外创建的最大 Pod 数量。例如，设置为 `20%` 表示可以额外创建当前 Pod 数量 20% 的 Pod。\n      - **`strategy.rollingUpdate.maxUnavailable`**: 指定在更新过程中，不可用 Pod 的最大数量或百分比。例如，设置为 `25%` 表示在更新过程中，最多允许 25% 的 Pod 不可 用。\n      - **`minReadySeconds`**: 指定新 Pod 进入 \"Ready\" 状态后，需要等待的最短时间（秒），才会被认为是可用的，并继续更新其他 Pod。\n\n    - 所以最好每个应用的副本数至少为2，这样在滚动更新时可以做到继续服务\n\n    - [执行滚动更新 | Kubernetes](https://kubernetes.io/zh-cn/docs/tutorials/kubernetes-basics/update/update-intro/)\n\n```groovy\nstage('更新K8s部署') {\n            steps {\n                script {\n                    sh \"\"\"\n                    sed -i 's|image: .*k8s_demo/.*|image: ${HARBOR_REGISTRY}/k8s_demo/back:${params.BACKEND_VERSION}|g' ./web/k8s/back/k8s/k8sDemoBack.yaml\n                    sed -i 's|image: .*k8s_demo/.*|image: ${HARBOR_REGISTRY}/k8s_demo/front:${params.FRONTEND_VERSION}|g' ./web/k8s/front/k8s/k8sDemoFront.yaml\n                    kubectl apply -f ./web/k8s/back/k8s/k8sDemoBack.yaml\n                    kubectl apply -f ./web/k8s/front/k8s/k8sDemoFront.yaml\n                    kubectl annotate deployment k8s-demo-front-end -n prod  kubernetes.io/change-cause=\"ci/cd update version to ${params.BACKEND_VERSION}\"\n                    kubectl annotate deployment k8s-demo-back-end -n prod  kubernetes.io/change-cause=\"ci/cd update version to ${params.BACKEND_VERSION}\"\n                    kubectl rollout restart deployment -n prod -l app=k8s-demo-front-end\n                    kubectl rollout restart deployment -n prod -l app=k8s-demo-back-end\n                      \"\"\"\n                }\n            }\n        }\n```\n\n最后完整的配置文件在 [k8s_demo/web/jenkins/jenkins.pipeline at main · kehaha-5/k8s_demo (github.com)](https://github.com/kehaha-5/k8s_demo/blob/main/web/jenkins/jenkins.pipeline)\n\n## 执行\n\n在执行的时候可以配置对应的版本和镜像信息\n\n![jenkins k8s-demo](.\\4.png)\n\n在执行成功后可以查看对应的记录\n\n```shell\n#查看滚动更新过程\nroot@node-m:~# kubectl rollout status  deployment  -n prod -l app=k8s-demo-back-end\ndeployment \"k8s-demo-back-end\" successfully rolled out\nroot@node-m:~# kubectl rollout status  deployment  -n prod -l app=k8s-demo-back-end\nWaiting for deployment \"k8s-demo-back-end\" rollout to finish: 1 out of 2 new replicas have been updated...\nWaiting for deployment \"k8s-demo-back-end\" rollout to finish: 1 out of 2 new replicas have been updated...\nWaiting for deployment \"k8s-demo-back-end\" rollout to finish: 1 old replicas are pending termination...\nWaiting for deployment \"k8s-demo-back-end\" rollout to finish: 1 old replicas are pending termination...\ndeployment \"k8s-demo-back-end\" successfully rolled out\n#查看滚动更新历史记录\nroot@node-m:~# kubectl rollout history deployment  -n prod -l app=k8s-demo-back-end\ndeployment.apps/k8s-demo-back-end\nREVISION  CHANGE-CAUSE\n1         <none>\n2         <none>\n3         <none>\n4         <none>\n5         ci/cd-update-version-to-1.0.0\n6         ci/cd update version to 1.1.2\n7         ci/cd update version to 1.1.2\n8         ci/cd update version to 1.1.5\n9         ci/cd update version to 1.1.5\n#查看目前pod信息\nroot@node-m:~# kubectl get pods -n prod -l app=k8s-demo-back-end\nNAME                                 READY   STATUS    RESTARTS   AGE\nk8s-demo-back-end-5c74755bcd-cgt78   1/1     Running   0          2m13s\nk8s-demo-back-end-5c74755bcd-md6j2   1/1     Running   0          2m14s\n```\n\nok，以上就是实现了在k8s中部署jenkins和harbor实现自动交付流水线","tags":["k8s_demo","k3s","jenkins"]},{"title":"从零到一--搭建web项目集群","url":"/2024/08/16/从零到一--搭建web项目集群/","content":"\n> 本次配置和代码都在[github](https://github.com/kehaha-5/k8s_demo)上 \n\n使用方法:\n> `git clone https://github.com/kehaha-5/k8s_demo`\n  `cd k8s_demo`\n  `git fetch --all`\n  `git checkout -b tag 1.0`\n\n# 概述\n\n## 目标\n1. 部署mysql集群\n2. 部署redis集群\n3. 部署一个前后端分离的项目\n    1. 后端gin纯api\n    2. 前端vue3\n\n## 环境\n服务器一共有3台：\n1. 主服务器(4c,6g) 是集群里面的server节点 \n2. 节点服务器1(2c,4g) 是集群里面的agent节点\n3. 节点服务器2(2c,2g) 该服务器不仅是集群里面的agent节点，也是项目中的前端服务器\n    > 那为什么要专门弄一台低配置的服务器专门运行前端?\n    >\n    > 1. 前端的资源流量会在web占据比较大的部分，即使上了`cdn`在某些时刻也会存在流量回流到服务器的，导致服务器带宽被占满，若后端也是部署在该服务器上面，会因为服务器带宽问题导致api的响应变慢，带来不好的用户体验。\n    >\n    > 2. 同时进行合理的资源分配也可以节约成本\n5. 一台nfs存储服务器\n4. 所有服务器都在同一个内网下，可以相互ping通\n\n# 前期准备\n## 集群搭建\n这次利用[k3s](https://github.com/k3s-io/k3s)来进行集群的搭建，k3s是一个轻量级的kubernetes集群，相比较于k8s，k3s的资源占用更少，并且部署更加简单。\n\n在利用k3s部署集群的时候要注意一下几点：\n1. 默认的k3s集群的储存是使用sqlite3，这个并不适合在高可用的生产环境下使用，所以在正式的生产环境中要使用[k3s-高可用外部数据库](https://docs.k3s.io/zh/datastore/ha)\n2. 在存储定义方面`pvc`中，k3s是没有`Kubernetes`这么多卷插件的，需要自己去配置 [k3s-卷和存储](https://docs.k3s.io/zh/storage)\n3. 集群访问的方式，k3s默认使用`k3s kubectl`命令行进行集群的访问，如果是另外安装的`kubectl`，需要配置`kubectl`的访问方式，另外`helm`也是如此 [k3s-集群访问](https://docs.k3s.io/zh/kubectl)\n4. k3s默认是使用了`traefik`作为集群的`ingress controller`，相关配置文件的位置和信息 [k3s-Networking Services](https://docs.k3s.io/zh/networking/networking-services)\n5. 在k3s中如果需要配置镜像源可以参考 [k3s-Private Registry Configuration](https://docs.k3s.io/zh/installation/private-registry)，同时最好启用`Embedded Registry Mirror`功能，这个可以使节点通过集群里面的其他节点获取已有的镜像 [k3s-Embedded Registry Mirror](https://docs.k3s.io/zh/installation/registry-mirror)\n\n## 额外配置\n因为在上述中提到需要一台服务器专门做web资源服务器，所以在部署完集群后，需要把对应服务node打上污点，防止其他pod被调度到该节点上面\n\n```shell\nroot@node-m:~/k8s# kubectl taint nodes node-res type=res:NoSchedule \n# 其中NoSchedule的含有如下和其他污点类型：\n# NoSchedule ：表示k8s将不会将Pod调度到具有该污点的Node上\n# PreferNoSchedule ：表示k8s将尽量避免将Pod调度到具有该污点的Node上\n# NoExecute ：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去\nroot@node-m:~/k8s# kubectl describe nodes node-res\nName:               node-res\n...\nTaints:             type=res:NoSchedule\nUnschedulable:      false\n...\n```\n或者使用k3s中的`agent`配置文件给节点进行污点配置 [k3s-agent](https://docs.k3s.io/zh/cli/agent)\n\n同时创建一个`prod`的namespace，我的所有应用都将会部署到该`namespace`上面\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: prod\n  labels:\n    name: prod\n```\n\n\n\n# 开始部署\n## kubernetes dashboard\n根据[文档](https://github.com/kubernetes/dashboard)命令部署 `dashboard`\n```shell\n# Add kubernetes-dashboard repository\nhelm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\n# Deploy a Helm Release named \"kubernetes-dashboard\" using the kubernetes-dashboard chart\nhelm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard\n```\n部署完后我们还要创建一个用户用于管理k8s [Creating sample user](https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md)\n\n完成上述部署以后，我们就要访问对应的`dashboard`，然后我是要用ip+端口的访问方式，所以还要创建一个`service`把流量代理到对应的pod上\n\n```shell\ncat node-web.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: dashoard-web\n  namespace: kubernetes-dashboard\n  labels:\n    k8s-app: kubernetes-dashboard\nspec:\n  type: NodePort #这里使用NodePort 因为我想要从集群外部访问\n  selector:\n    app.kubernetes.io/name: kong\n  ports:\n    - protocol: TCP\n      port: 8000\n      targetPort: 8443\n```\n注意这里要代理的是`kubernetes-dashboard-kong`不是`kubernetes-dashboard-web`\n```shell\nroot@node-m:~/k8s/dashoboard# kubectl get endpoints -n kubernetes-dashboard\nNAME                               ENDPOINTS              AGE\ndashoard-web                       10.42.1.242:8443       12d\n...\nkubernetes-dashboard-kong-proxy    10.42.1.242:8443       34h\n...\n#这里可以看到我的dashoard-web已经绑定到kubernetes-dashboard-kong-proxy上了\n kubectl get svc -n kubernetes-dashboard\nNAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\ndashoard-web  NodePort    10.43.234.91    <none>        8000:30756/TCP   12d\n...\n#查看service信息可以得知 dashoard-web 的外部访问端口为 30756\n```\n然后ip+30756 即可访问 `dashboard` 同时还要创建对应用户的token\n```shell\nroot@node-m:~/k8s/dashoboard# kubectl create token -n kubernetes-dashboard xxxx\n```\n![dashoard-web](.\\1.png)\n\n## NFS存储挂载\n\n在所有应用部署前，先部署外部存储介质\n\n首先先要安装 [csi-driver-nfs](https://github.com/kubernetes-csi/csi-driver-nfs) 这个是nfs的卷插件，不安装的话是无法使用nfs相关卷功能\n\n```shell\nroot@node-m:~/k8s# cat ./nfs/storageClass.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs\nprovisioner: nfs.csi.k8s.io\nparameters:\n  server: 192.168.1.125\n  share: /mnt/nfs_share\n  # csi.storage.k8s.io/provisioner-secret is only needed for providing mountOptions in DeleteVolume\n  # csi.storage.k8s.io/provisioner-secret-name: \"mount-options\"\n  # csi.storage.k8s.io/provisioner-secret-namespace: \"default\"\nreclaimPolicy: Retain #因为要保存redis 和 mysql的数据 所以使用Retain 当pvc或pv被删除以后仍然可以保留数据\nvolumeBindingMode: Immediate\nmountOptions:\n  - nfsvers=4\n```\n\n这里使用的是` Dynamic Volume Provisioning` 这样就不需要每个`pvc`都声名一个`pv`了\n\n## MySQL集群\n\n这里要部署的是一个1个主节点2个从节点的MySQL集群\n\n```yaml\n#configmap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mysql\n  namespace: prod\ndata:\n  master.cnf: |\n    # 主节点MySQL的配置文件\n    [mysqld]\n    log-bin\n  slave.cnf: |\n    # 从节点MySQL的配置文件\n    [mysqld]\n    log-bin\n    read_only=1\n    replicate_ignore_db=information_schema\n    replicate_ignore_db=performance_schema\n    replicate_ignore_db=mysql\n    replicate_ignore_db=sys\n  init.sh: |\n    #!/bin/bash\n    until mysql  -p\"$MYSQL_ROOT_PASSWORD\" -e \"select 1;\"; do sleep 1; done\n    [[ $HOSTNAME =~ -([0-9]+) ]] || exit 1\n    ordinal=${BASH_REMATCH[1]}\n    if [[ $ordinal -eq 0 ]]; then\n      mysql -p\"$MYSQL_ROOT_PASSWORD\" -e \"grant replication slave on *.* to '$REPLIC_USER'@'%' identified by '$REPLIC_PASSWORD'\"\n      mysql -p\"$MYSQL_ROOT_PASSWORD\" -e \"FLUSH PRIVILEGES\"\n    else\n      mysql -p\"$MYSQL_ROOT_PASSWORD\" -e \"change master to master_host='mysql-0.mysql-svc',master_user='$REPLIC_USER',master_password='$REPLIC_PASSWORD';\"\n      mysql -p\"$MYSQL_ROOT_PASSWORD\" -e \"start slave\"\n    fi\n    \n#sercet.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysql\n  namespace: prod\ntype: Opaque\ndata:\n  pass: NlFBMFpGSTRUYzZ6UmU5VWc4amI= #6QA0ZFI4Tc6zRe9Ug8jb\n  rep_pass: bWtzdFE2M3ZuYmFQeUJ5eGR6Z3Y= #mkstQ63vnbaPyByxdzgv\n  rep_user: cmVw #rep\n  \n#mysql.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n      app.kubernetes.io/name: mysql\n  replicas: 3\n  serviceName: mysql-svc\n  template:\n    metadata:\n      labels:\n        app: mysql\n        app.kubernetes.io/name: mysql\n    spec:\n      initContainers:\n        - name: init-mysql\n          image: mysql:5.7.34\n          command:\n          - bash\n          - \"-c\"\n          - |\n            set -ex\n            # Generate mysql server-id from pod ordinal index.\n            [[ $HOSTNAME =~ -([0-9]+)$ ]] || exit 1\n            ordinal=${BASH_REMATCH[1]}\n            echo [mysqld] > /mnt/conf.d/server-id.cnf\n            # Add an offset to avoid reserved server-id=0 value.\n            echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\n            # cp config file to /mnt/conf.d\n            if [[ $ordinal -eq 0 ]]; then\n              cp /mnt/config-map/master.cnf /mnt/conf.d/\n            else\n              cp /mnt/config-map/slave.cnf /mnt/conf.d\n            fi\n            cp /mnt/config-map/init.sh /mnt/conf.d\n          volumeMounts:\n          - name: conf\n            mountPath: /mnt/conf.d/\n          - name: config-map\n            mountPath: /mnt/config-map/\n      containers:\n      - name: mysql\n        image: mysql:5.7.34\n        volumeMounts:\n          - name: conf\n            mountPath: /etc/mysql/conf.d\n          - name: data\n            mountPath: /var/lib/mysql/\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: pass\n        - name: REPLIC_USER\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: rep_user\n        - name: REPLIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: rep_pass\n        ports:\n        - name: mysql\n          containerPort: 3306\n        resources:\n          requests:\n            cpu: \"500m\"\n            memory: \"500Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"500Mi\"\n        lifecycle:\n          postStart:\n            exec:\n              command: [\"bash\",\"/etc/mysql/conf.d/init.sh\"]\n      volumes:\n      - name: config-map\n        configMap:\n          name: mysql\n      - name: conf\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: \"nfs\" #这里使用的就是上面声名的Dynamic Volume Provisioning\n      resources:\n        requests:\n          storage: 2Gi\n          \n#service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-svc\n  namespace: prod\n  labels:\n    app: mysql\n    app.kubernetes.io/name: mysql\nspec:\n  ports:\n  - name: mysql\n    port: 3306\n  clusterIP: None #使用clusterIP: None 这样每个pod都会有一个独立的dns域名可以单独访问到，同时访问主节点就变成mysql-0.mysql-svc\n  selector:\n    app: mysql\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-read\n  namespace: prod\n  labels:\n    app: mysql\n    app.kubernetes.io/name: mysql\n    readonly: \"true\"\nspec:\n  ports:\n  - name: mysql\n    port: 3306\n  selector:\n    app: mysql\n```\n\n这里有几点需要注意一下：\n\n1. 这里创建集群使用的是`StatefulSet` 因为mysql集群之间存在拓步结构（必须先启动master节点再启动slaver节点），为了处理这种状态的关系所以使用了`StatefulSet`\n\n   > ```shell\n   > [[ $HOSTNAME =~ -([0-9]+)$ ]] || exit 1\n   > ordinal=${BASH_REMATCH[1]}\n   > ```\n   > 这段shell的意思是获取hostname中的数字，然后赋值个ordinal，这样通过判断ordinal就可以知道当前环境是主节点还是从节点。\n   >\n   > `StatefulSet`创建的pod名称是有顺序的，pod-index，index会从0开始，这里规定pod-0为主节点\n\n2. mysql.yaml中的`serviceName`必须跟Service里面的`metadata.name`一致，不然dns只能解析`StatefulSetName`-`ServiceName`到任意一个pod上，不能通过`podsName`-`serviceName`解析到指定的pod上面  [StatefulSet | Kubernetes](https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#:~:text=serviceName)\n\n   > **serviceName** (string), 必需\n   >\n   > serviceName 是管理此 StatefulSet 服务的名称。 该服务必须在 StatefulSet 之前即已存在，并负责该集合的网络标识。 Pod 会获得符合以下模式的 DNS/主机名： pod-specific-string.serviceName.default.svc.cluster.local。 其中 “pod-specific-string” 由 StatefulSet 控制器管理。\n\n3. 因为我这里是用了一个自定义的`namespace`，如果在本`namespace`上面通过`podsName`-`serviceName`是可以直接访问的，但是如果是其他`namespace`访问就要使用 `podsName`-`serviceName`.`my-namcespace` 才可以访问 [详细](https://kubernetes.io/zh-cn/docs/concepts/services-networking/dns-pod-service/#dns-records:~:text=Service%20%E7%9A%84%E5%90%8D%E5%AD%97,%E9%A1%B5%E9%9D%A2%E3%80%82)\n\n4. 我这里的`resources.requests`和`resources.limits`的配置是一样的，这个属于是`Qos`中的`Guaranteed`\n\n   > Qos中的类型：\n   >\n   > **Guaranteed（保障型）：**\n   >\n   > - **条件：** Pod 中所有容器的 `resources.requests` 和 `resources.limits` 都相等，并且都设置了非零值。\n   > - 特点：\n   >   - 优先级最高，资源得到最强的保障。\n   >   - 当节点资源不足时，Guaranteed Pod 不会被驱逐，除非是系统保留资源不足。\n   > - **适用场景：** 对资源要求高且稳定的应用，例如数据库、核心服务等。\n   >\n   > **Burstable（可突增型）：**\n   >\n   > - **条件：** Pod 中至少有一个容器的 `resources.requests` 设置了值，并且 `resources.requests` 小于 `resources.limits`。\n   > - 特点：\n   >   - 优先级中等，可以在 `resources.requests` 的基础上使用更多资源，但不能超过 `resources.limits`。\n   >   - 当节点资源不足时，Burstable Pod 可能被驱逐，优先级低于 Guaranteed Pod。\n   > - **适用场景：** 对资源需求有波动，但峰值使用不会太高的应用，例如 Web 服务器、API 服务等。\n   >\n   > **BestEffort（尽力而为型）：**\n   >\n   > - **条件：** Pod 中所有容器都没有设置 `resources.requests` 和 `resources.limits`。\n   > - 特点：\n   >   - 优先级最低，只能使用节点上剩余的资源。\n   >   - 当节点资源不足时，BestEffort Pod 会被优先驱逐。\n   > - **适用场景：** 对资源要求不高，可以容忍被驱逐的应用，例如批处理任务、测试任务等。\n\n## redis集群\n\nredis高可用服务一般有三种类型：\n\n1. Redis replication redis的主从复制 [Redis replication | Docs](https://redis.io/docs/latest/operate/oss_and_stack/management/replication/)\n\n2. Redis Sentinel 在redis的主从复制基础上添加了哨兵，确保在redis-master下线了以后，重新从redis-slaver中选举一个新的master出来，保证服务可用，当旧的redis-master上线后就变成redis-slaver。[High availability with Redis Sentinel | Docs](https://redis.io/docs/latest/operate/oss_and_stack/management/sentinel/)\n\n3.  Redis Cluster 这个是由多个redis主从一起组成的集群，数据会被分散到每个redis主从上面 [Scale with Redis Cluster | Docs](https://redis.io/docs/latest/operate/oss_and_stack/management/scaling/)\n\n   > 会有16384 hash slots 在 Redis Cluster，它们会被分散到不同的节点上面，读写对应的数据将会在对应的节点中进行\n   >\n   >  with Redis Cluster, you get the ability to:\n   >\n   > - Automatically split your dataset among multiple nodes.\n   > - Continue operations when a subset of the nodes are experiencing failures or are unable to communicate with the rest of the cluster.\n\n而这次部署的就是3个node的`Redis Cluster`其中每个node由一个主一个从组成，所以一共有6个redis服务\n\n```yaml\n#configmap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis\n  namespace: prod\ndata:\n  redis.conf: |\n    port 6379\n    cluster-enabled yes #开启集群模式\n    cluster-node-timeout 5000\n    cluster-config-file /data/nodes.conf\n    appendonly yes\n\n#sercet\napiVersion: v1\nkind: Secret\nmetadata:\n  name: redis\n  namespace: prod\ntype: Opaque\ndata:\n  pass: ZHFYbkdJNEUxM28zY0d6V3E1QXI= #dqXnGI4E13o3cGzWq5Ar\n  \n#redis\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      app.kubernetes.io/name: redis\n  serviceName: redis-svc\n  replicas: 6\n  template:\n    metadata:\n      labels:\n        app: redis\n        app.kubernetes.io/name: redis\n    spec:\n      initContainers:\n        - name: init-redis  \n          image: redis:7.4\n          env:\n          - name: REDISCLI_AUTH\n            valueFrom:\n              secretKeyRef:\n                name: redis\n                key: pass\n          command:\n          - bash\n          - \"-c\"\n          - |\n            set -ex\n            cp /mnt/config-map/redis.conf /mnt/conf.d\n            echo \"requirepass $REDISCLI_AUTH\" >> /mnt/conf.d/redis.conf\n            echo \"masterauth $REDISCLI_AUTH\" >> /mnt/conf.d/redis.conf\n          volumeMounts:\n          - name: conf\n            mountPath: /mnt/conf.d/\n          - name: config-map\n            mountPath: /mnt/config-map/\n      containers:\n      - name: redis\n        image: redis:7.4\n        command: [\"redis-server\"]\n        args:\n        - /usr/local/etc/redis/redis.conf\n        - --cluster-announce-ip\n        - \"$(MY_POD_IP)\"\n        volumeMounts:\n          - name: conf\n            mountPath: /usr/local/etc/redis/\n          - name: data\n            mountPath: /data/\n        env:\n        - name: REDISCLI_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: redis\n              key: pass\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n             fieldPath: status.podIP\n        ports:\n        - name: redis\n          containerPort: 6379\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"200Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"200Mi\"\n      volumes:\n      - name: config-map\n        configMap: \n          name: redis\n      - name: conf\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: \"nfs\"\n      resources:\n        requests:\n          storage: 2Gi\n\n#service\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-svc\n  namespace: prod\n  labels:\n    app: redis\n    app.kubernetes.io/name: redis\nspec:\n  ports:\n  - name: redis\n    port: 6379\n  clusterIP: None #这里跟上面的mysql一样，通过redis-index.redis-svc来访问不同的redis节点\n  selector:\n    app: redis\n\n#job\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: redis-cluster-create\n  namespace: prod\nspec:\n  template:\n    spec:\n      containers:\n      - name: redis-cli\n        image: redis:7.4\n        env:\n        - name: REDISCLI_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: redis\n              key: pass\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - |\n            redis-cli --cluster create \\\n              redis-0.redis-svc:6379 \\\n              redis-1.redis-svc:6379 \\\n              redis-2.redis-svc:6379 \\\n              redis-3.redis-svc:6379 \\\n              redis-4.redis-svc:6379 \\\n              redis-5.redis-svc:6379 \\\n              --cluster-replicas 1 -a $REDISCLI_AUTH --cluster-yes #cluster-replicas 1 表示为每一个主节点分配一个从节点，这样每个node就有两个redis组成一个主一个从，这样3由个node组成的redis cluster就要6个redis\n      restartPolicy: Never\n  backoffLimit: 1\n```\n\n这里要注意几点\n\n1. redis部署完了还要执行创建集群的命令，这里利用了job来完成这个\n\n2. 密码配置说明\n\n   1. requirepass表示redis的密码\n   2. masterauth 表示redis主从时候的密码 两者都要同时设置，不然会存在因密码错误无法同步数据\n   3. 如果想更加细致化的控制redis的权限可以新建用户并且使用`ACL`的功能 [ACL | Docs (redis.io)](https://redis.io/docs/latest/operate/oss_and_stack/management/security/acl/)\n\n\n3. pod的ip问题，在第一次启动redis集群的时候，会把集群信息写入`/data/nodes.conf`类似\n\n   > 127.0.0.1:6379> CLUSTER NODES\n   > 171465406d506a29e3bfcd2cf62a578dfe048613 10.42.0.109:6379@16379 master - 0 1723877446841 3 connected 10923-16383\n   > 6ddad196c4998856d7587a2a31012bd3570342e8 10.42.0.111:6379@16379 myself,slave 50fc764dae51be95e3f24527b2458ceb44533a84 0 0 2 connected\n   > e43744bd3bd58f04158ea90bc044b1b200e0b153 10.42.1.40:6379@16379 slave 171465406d506a29e3bfcd2cf62a578dfe048613 0 1723877446842 3 connected\n   > 2ca5a824b3edca3baaf8caacf6d036061c9cb647 10.42.0.110:6379@16379 slave 5947054d518ac8ed6b1aeb4b378ad50eedd3bcbb 0 1723877445837 1 connected\n   > 5947054d518ac8ed6b1aeb4b378ad50eedd3bcbb 10.42.0.108:6379@16379 master - 0 1723877445000 1 connected 0-5460\n   > 50fc764dae51be95e3f24527b2458ceb44533a84 10.42.1.39:6379@16379 master - 0 1723877446000 2 connected 5461-10922\n\n   但是在重启redis集群后pod的ip会更改，导致集群状态为`failed `。解决可以[参考](https://github.com/redis/redis/issues/4289)\n\n## Gin后端\n\n> 代码 [k8s_demo/web/back at main · kehaha-5/k8s_demo (github.com)](https://github.com/kehaha-5/k8s_demo/tree/main/web/back)\n\n这个gin后端的话，除了提供api服务以外它还而外监听一个端口用来给pod提供健康检测 [配置存活、就绪和启动探针]([Kubernetes](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/))\n\n```dockerfile\nFROM alpine:3.20.2\nCOPY ./k8s_demo /app/k8s_demo\nCOPY ./config-prod.yaml /app/config-prod.yaml\nRUN set -eux && sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories\nRUN apk update && apk add tzdata\n```\n\n我们要把后端代码和所需要的环境打包成镜像，因为我的k3s使用的运行时是`containerd`，所以这里利用`docker`进行打包，然后导出镜像再导入到`containerd`里面\n\n```shell\ndocker build -t k8s_demo/back:1.0 . #注意-t的格式，只有这样的格式导入到ctr中才会保留\ndocker save  k8s_demo/back -o k8s_demo_back.tar\nctr -n k8s.io images import k8s_demo_back.tar #导入时，要选择k8s.io的namesapce 不然可能出现pod找不到image\n```\n\n```yaml\n#k8s-demo-back-end\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k8s-demo-back-end\n  namespace: prod\n  labels:\n    app: k8s-demo-back-end\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: k8s-demo-back-end\n  template:\n    metadata:\n      labels:\n        app: k8s-demo-back-end\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - k8s-demo-back-end\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: k8s-demo-back-end\n        image: docker.io/k8s_demo/back:1.0@sha256:bf9041895c649d1a586851e2f2e072fb9680fe134c359ea59a577b010029ba5d\n        command: [\"/app/k8s_demo\"]\n        args: [\"-c\",\"/app/config-prod.yaml\"]\n        ports:\n        - name: api-port\n          containerPort: 40814\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 3\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis\n              key: pass\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: pass\n        - name: GIN_MODE\n          value: \"release\"\n        resources:\n          requests:\n            cpu: \"200m\"\n            memory: \"200Mi\"\n          limits:\n            cpu: \"1000m\"\n            memory: \"500Mi\"\n     \n#service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: k8s-demo-back-end-service\n  namespace: prod\n  labels:\n    name: k8s-demo-back-end-service\nspec:\n  selector:\n    app: k8s-demo-back-end\n  ports:\n    - protocol: TCP\n      name: api-port\n      port: 40814     \n      targetPort: 40814  \n  clusterIP: None\n```\n\n这里要注意一下，因为我一个共有3台服务器，除去一台专门的前端服务器，那我还有两台服务器，所以这里的`replicas`设置为2，然后我想让每个pod尽量不要调度到同一个节点上面，所以使用了`affinity.podAntiAffinity`尽量不要调度到已经有`app=k8s-demo-back-end`的节点上面\n\n```shell\nroot@node-m:~/k8s# kubectl get pods -n prod -l app=k8s-demo-back-end -o wide\nNAME                   READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES\nk8s-demo-back-end...   1/1     Running   0          28s   10.42.1.42    node-s   <none>           <none>\nk8s-demo-back-end...   1/1     Running   0          27s   10.42.0.116   node-m   <none>           <none>\n```\n\n## 前端部署\n\n> 代码 [k8s_demo/web/front/k8s_demo at main · kehaha-5/k8s_demo (github.com)](https://github.com/kehaha-5/k8s_demo/tree/main/web/front/k8s_demo)\n\n这里也是要使用docker进行打包\n\n```dockerfile\nFROM  alpine:3.20.2\nCOPY ./dist/ /app/\n```\n\n导入到`containerd`\n\n```shell\ndocker build -t k8s_demo/front:1.0 .\ndocker save  k8s_demo/front -o k8s_demo_front.tar\nctr -n k8s.io images import k8s_demo_front.tar\n```\n\n```yaml\n#configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: web-nginx\n  namespace: prod\ndata:\n  app.conf: |\n    server {\n        listen       80;\n        listen  [::]:80;\n        server_name  localhost;\n\n        #access_log  /var/log/nginx/host.access.log  main;\n\n        location / {\n            root   /usr/share/nginx/html;\n            index  index.html index.htm;\n            try_files $uri $uri/ /index.html; #vue3的history模式下的路由配置\n        }\n    }\n\n#k8s-demo-back-front\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k8s-demo-back-front\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      app: k8s-demo-back-front\n      app.kubernetes.io/name: k8s-demo-back-front\n  template:\n    metadata:\n      labels:\n        app: k8s-demo-back-front\n        app.kubernetes.io/name: k8s-demo-back-front\n    spec:\n      tolerations:\n        - key: \"type\"\n          operator: \"Equal\"\n          value: \"res\"\n          effect: \"NoSchedule\"\n      initContainers:\n        - name: init-nginx\n          image: docker.io/k8s_demo/front:1.0@sha256:fecfc2dc67c49d8ff27a77cff8aee431f9baaee3ffc0fd22da8e8da20caaeddf\n          command:\n          - sh\n          - \"-c\"\n          - |\n            set -ex\n            cp -r /app/* /mnt/app/\n          volumeMounts:\n          - name: app\n            mountPath: /mnt/app/\n      containers:\n      - name: app-nginx\n        image: nginx:1.27.1-alpine3.20\n        volumeMounts:\n          - name: app\n            mountPath: /usr/share/nginx/html/\n          - name: config-map\n            mountPath: /etc/nginx/conf.d/\n        ports:\n        - name: app-nginx\n          containerPort: 80\n      volumes:\n      - name: app\n        emptyDir: {}\n      - name: config-map\n        configMap: \n          name: web-nginx\n      \n#service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: k8s-demo-back-front-service\n  namespace: prod\n  labels:\n    name: k8s-demo-back-front-service\nspec:\n  selector:\n    app: k8s-demo-back-front\n  ports:\n    - protocol: TCP\n      name: web\n      port: 80     \n      targetPort: 80  \n  clusterIP: None\n\n```\n\n这里需要注意一下我使用了`tolerations`容忍，容忍了节点可以存在`type=res`的键值对，刚好对应了`node-res`这个专门用来运行前端的节点\n\n```shell\nroot@node-m:~/k8s# kubectl get pods -n prod -l app=k8s-demo-back-front -o wide\nNAME                     READY   STATUS    RESTARTS        AGE   IP           NODE       NOMINATED NODE   READINESS GATES\nk8s-demo-back-front...   1/1     Running   3 (3h10m ago)   23h   10.42.2.41   node-res   <none>           <none>\n```\n\n## ingress\n\n现在所有的服务都部署了，但是需要把它们统一暴露出去给外部访问，这里就用到了`ingress`，`ingress`可以说是管理`service`的`service`所以就是它来暴露服务到外部\n\n为了让`ingress`可以正常工作，我们必须要有一个可用的`ingress`控制器 [Ingress 控制器 | Kubernetes](https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress-controllers/)\n\n这里使用的是k3s默认使用的是`Traefik ingress controller`的，所以第一步先要部署`traefik`的`dashboard`\n\n### Traefik dashboard\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: traefik-dashboard\n  labels:\n    name: traefik-dashboard\n\n---\n\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard\n  namespace: traefik-dashboard\nspec:\n  routes:\n  - match: Host(`traefik.localhost.com`) && (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\n    kind: Rule\n    services:\n    - name: api@internal\n      kind: TraefikService\n    middlewares:\n      - name: auth\n\n---\n\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: auth\n  namespace: traefik-dashboard\nspec:\n  basicAuth:\n    secret: authsecret  # Kubernetes secret named \"secretName\"\n\n\n--- \n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: authsecret\n  namespace: traefik-dashboard\ndata:\n  users:\n    cm9vdDokYXByMSRibUhSclNOaSQ0L2d4L0xNMmpzVGdBRTUzMTcwSEkuCgo= #htpasswd -nb root wwM9yCz52mWpWysVONM0 | base64\n```\n\n这个就是部署`Traefik dashboard`的配置，它就是使用`ingress route`把服务暴露到了`traefik.localhost.com`的`/dashboard`和`/api`。同时我还使用了中间件来做用户认证，在访问的时候必须输入账号和密码才能正常访问，否则就是401。\n\n除此之外，我还要修改本地的host文件把`traefik.localhost.com`这个域名指向我的节点ip。之后访问`http://traefik.localhost.com/dashboard/#/`即可\n\n![Traefik-dashboard](.\\2.png)\n\n参考:\n\n[Traefik Dashboard Documentation - Traefik](https://doc.traefik.io/traefik/operations/dashboard/)   \n\n[Traefik BasicAuth Documentation - Traefik](https://doc.traefik.io/traefik/middlewares/http/basicauth/)\n\n### 服务配置\n\n```yaml\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: header\n  namespace: prod\nspec:\n  headers:\n    accessControlAllowMethods:\n      - \"GET\"\n      - \"OPTIONS\"\n      - \"POST\"\n      - \"PUT\"\n    accessControlAllowHeaders:\n      - \"*\"\n    addVaryHeader: true\n    accessControlAllowOriginList: \n      - \"*\" #这里最好设置为前端的域名 web.k8s.demo\n\n---\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web-ingress\n  namespace: prod\n  annotations:\n    traefik.ingress.kubernetes.io/router.middlewares: prod-header@kubernetescrd\nspec:\n  rules:\n  - host: api.k8s.demo\n    http:\n      paths:\n      - path: /api\n        pathType: Prefix\n        backend:\n          service:\n            name: k8s-demo-back-end-service\n            port: \n              number: 40814\n  - host: web.k8s.demo\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: k8s-demo-back-front-service\n            port: \n              number: 80\n```\n\n这个就是我的服务配置，这里使用了`api.k8s.demo`访问后端，`web.k8s.demo`访问前端，其中`web.k8s.demo`要把ip绑定到前端节点上面，同时这里还写了一个中间件来解决跨域问题。\n\n![Traefik-ingress1](.\\3.png)\n\n![Traefik-ingress1](.\\4.png)\n\n这里需要注意两点：\n\n1. 在`Ingress`里面使用中间件需要遵循一定的命名规则\n\n   > ```yaml\n   > apiVersion: traefik.io/v1alpha1\n   > kind: Middleware\n   > metadata:\n   >   name: stripprefix\n   >   namespace: appspace\n   > spec:\n   >   stripPrefix:\n   >     prefixes:\n   >       - /stripit\n   > \n   > ---\n   > apiVersion: networking.k8s.io/v1\n   > kind: Ingress\n   > metadata:\n   >   name: ingress\n   >   namespace: appspace\n   >   annotations:\n   >     # referencing a middleware from Kubernetes CRD provider: \n   >     # <middleware-namespace>-<middleware-name>@kubernetescrd\n   >     \"traefik.ingress.kubernetes.io/router.middlewares\": appspace-stripprefix@kubernetescrd\n   > spec:\n   >   # ... regular ingress definition\n   > ```\n   >\n   > [Traefik Configuration Discovery Overview - Traefik](https://doc.traefik.io/traefik/providers/overview/#provider-namespace)\n\n2. 在后面实现的时候发现因为使用了ingress 导致无论是访问前端还是后端流量都会走到节点上面，所以如果真的要把流量分开，最好是不用`ingress`，使用service中的`NodePort`公开服务\n\nok 以上就是`从零到一--搭建web项目集群`所有内容了","tags":["k8s_demo","k3s"]},{"title":"docker-compose 部署php-web项目","url":"/2024/04/18/docker-compose-部署php-web项目/","content":"\n> 最近有一个朋友他想让我帮他部署一个几年前的php项目，既然是几年前的就项目了，为什么还要重新部署捏？那肯定是还有剩余价值啊\n\n# 框架升级\n在部署项目之前，先看一下这个项目有什么安全问题，发现在github的Dependabot alerts给我报告了10个左右的安全问题，看了一下，除了老生常谈的php序列化漏洞以外，剩下的都可以通过升级依赖来进行解决，所以第一件事就是先把项目的依赖进行升级\n\n![alerts](.\\1.png)\n\n# 项目结构 && 依赖安装\n\nok，依赖升级好了，就先clone下来看一下目录结构先。利用tree命令查看项目结构\n```sh\nkehaha@DESKTOP-4GO6ORF:~/docker/xxxxx$ tree -L 2 \n.\n└── xxxxx\n    ├── README.md\n    ├── back_end\n    └── font_end\n\n3 directories, 1 file\n```\n不难发现，这个项目是一个前后端分离的项目.\n\n## back_end\n先看一下后端的文件，点开来看bank_end进行查看，先看一下php用来管理依赖的`composer.json`\n```json\n\"require\": {\n    \"php\": \">=7.1.0\",\n    \"topthink/framework\": \"^6.0.0\",\n    \"topthink/think-orm\": \"^2.0\",\n    \"topthink/think-multi-app\": \"^1.0\",\n    \"topthink/think-captcha\": \"^3.0\",\n    \"thans/tp-jwt-auth\": \"^1.1\",\n    \"phpoffice/phpspreadsheet\": \"^1.17\"\n},\n```\n好的这个项目用到了thinkphp6，要求的php版本要大于 7.1。既然这样先利用[composer](https://www.phpcomposer.com/)来安装后端的依赖。\n\n因为我的电脑里面没有安装php和composer 所以这次就利用docker来安装依赖，这里选择的是composer:1.8.2的镜像\n```sh\nkehaha@DESKTOP-4GO6ORF:~/docker/xxxx/xxxx/back_end$ docker run --rm --interactive --tty   --volume $PWD:/app composer:1.8.2 composer i\nLoading composer repositories with package information\nInstalling dependencies (including require-dev) from lock file\nYour requirements could not be resolved to an installable set of packages.\n\n  Problem 1\n    - Installation request for phpoffice/phpspreadsheet 1.17.1 -> satisfiable by phpoffice/phpspreadsheet[1.17.1].\n    - phpoffice/phpspreadsheet 1.17.1 requires ext-gd * -> the requested PHP extension gd is missing from your system.\n\n  To enable extensions, verify that they are enabled in your .ini files:\n    -\n    - /usr/local/etc/php/conf.d/date_timezone.ini\n    - /usr/local/etc/php/conf.d/docker-php-ext-sodium.ini\n    - /usr/local/etc/php/conf.d/docker-php-ext-zip.ini\n    - /usr/local/etc/php/conf.d/memory-limit.ini\n  You can also run `php --ini` inside terminal to see which files are used by PHP in CLI mode.\n```\n依赖安装失败了因为需要安装gd扩展，看来不如直接安装php然后安装完所需要的扩展最后再安装composer来下载依赖。所以这次直接利用docker的php来安装的框架依赖和扩展依赖，这里我选择的php:7.3.33-zts-alpine3.14。\n\n那么php如何快速安装扩展，可以参考[hub.dockerphp的overview](https://hub.docker.com/_/php#:~:text=How%20to%20install%20more%20PHP%20extensions)。这里我选择的使用[docker-php-extension-installer](https://github.com/mlocati/docker-php-extension-installer)来安装扩展。\n\n首先先启动php:7.3.33的容器并把项目挂载到容器内，然后进入容器，安装脚本，安装依赖\n```sh\ninstall-php-extensions gd zip @composer #后面发现还要安装zip\n```\n安装完成后，利用`composer i`安装框架依赖\n```sh\nGenerating autoload files\n> @php think service:discover\nSucceed!\n> @php think vendor:publish\nFile /app/config/jwt.php exist!\nFile /app/config/captcha.php exist!\nFile /app/config/trace.php exist!\nSucceed!\n9 packages you are using are looking for funding.\nUse the `composer fund` command to find out more!\n```\n那么后端框架依赖就安装完成了。\n\n## front_end\n进到前端的文件，看了一下是一个vue2的项目，我的电脑是安装了nvm的，因此可以不用docker来安装依赖和打包项目。先用 `nvm use 14` 切换到 node14 的版本，然后用 `npm i` 和 `npm run build:prod` 来进行依赖和项目的安装\n\n# 编写部署文件\n\n因为整个项目要用到`mysql` `nginx` `php-fpm`因此利用docker-compose来进行项目部署。首先编写docker-compose.yaml，把要利用到的镜像和端口都进行配置，这里不用挂载是因为这个项目只是跑来测试，不是正式上线，所以就没有使用volume进行文件挂载了。\n\n## docker-compose.yaml\n```yaml\nversion: \"3.8\"\n\nnetworks:\n  web_network:\n\nservices:\n  back:\n    container_name: php_back\n    build: ./php/\n    networks:\n      - web_network\n    depends_on:\n      - _front\n      - _mysql\n    restart: always\n    tty: true\n\n  front:\n    build: ./nginx/\n    container_name: _front\n    networks:\n      - web_network\n    ports:\n      - \"4416:80\"\n    restart: always\n\n  mysql:\n    build: ./mysql/\n    container_name: _mysql\n    networks:\n      - web_network\n    ports:\n      - \"44166:3306\"\n    restart: always\n```\n这里可能就有问题了，所有容器虽然再同一个compose中，但是容器自己是如何访问对方的捏。其实这里我定义了一个`web_network`那么它们都在这个网络里面的话，可以直接通过容器名来访问对方。\n[官方文档](https://docs.docker.com/compose/networking/#:~:text=For%20example%2C%20suppose,is%20running%20locally.)\n\n## nginx \n这里单独编写nginx的Dockerfile是因为要进行一些配置\n```yaml\nFROM nginx:stable\n\nCOPY ./dist/ /usr/share/nginx/html/ #复制前端打包好的文件进入\n\nCOPY ./default.conf /etc/nginx/conf.d/\n\nEXPOSE 80\n\n```\n```conf\nserver {\n    listen       80;\n    listen  [::]:80;\n    server_name  localhost;\n\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n        root   /usr/share/nginx/html;\n    }\n\n    location ~ \\.php$ {\n        root           html;\n        fastcgi_pass   php_back:9000; #php的容器名称\n        fastcgi_index  index.php;\n        fastcgi_param  SCRIPT_FILENAME  /var/www/html/$fastcgi_script_name; #这个/var/www/html/是php容器中项目存放的位置，就是通过获取要执行的php文件位置来调用cgi来执行对应的php文件\n        include        fastcgi_params;\n    }\n}\n```\n\n## php\n\n这里的php我使用的是 `php:7.3-fpm-alpine3.14` 为什么捏，在 `dockerhub` 的 `php overview` 中有如下解释。[官方文档](https://hub.docker.com/_/php#:~:text=php%3A%3Cversion%3E%2Dfpm,this%20image%20variant.)\n```text\nphp:<version>-fpm\nThis variant contains PHP-FPM, which is a FastCGI implementation for PHP. See the PHP-FPM website for more information about PHP-FPM.\n\nIn order to use this image variant, some kind of reverse proxy (such as NGINX, Apache, or other tool which speaks the FastCGI protocol) will be required.\n\nSome potentially helpful resources:\n\nPHP-FPM.org\nsimplified example by @md5\nvery detailed article by Pascal Landau\nStack Overflow discussion\nApache httpd Wiki example\nWARNING: the FastCGI protocol is inherently trusting, and thus extremely insecure to expose outside of a private container network -- unless you know exactly what you are doing (and are willing to accept the extreme risk), do not use Docker's --publish (-p) flag with this image variant.\n```\n我们是利用nginx来访问cgi来调用php的，因此我们要使用fpm的版本。\n\n如下是php的Dokerfile\n```yaml\nFROM php:7.3-fpm-alpine3.14\n\nRUN curl -sSLf \\\n        -o /usr/local/bin/install-php-extensions \\\n        https://github.com/mlocati/docker-php-extension-installer/releases/latest/download/install-php-extensions && \\\n    chmod +x /usr/local/bin/install-php-extensions && \\\n    install-php-extensions gd zip #安装依赖 gd zip\n    \nCOPY ./back_end /var/www/html/\n\nRUN mv \"$PHP_INI_DIR/php.ini-production\" \"$PHP_INI_DIR/php.ini\" \n\nEXPOSE 9000\n```\n\n## mysql\n\nmysql的Dockerfile文件：\n这里我的mysql用的版本是5.7.36\n\n```yaml\nFROM mysql:5.7.36\n\nCOPY ./my.cnf /etc/mysql/conf.d/ #mysql的配置文件\nCOPY ./xxxx.sql /docker-entrypoint-initdb.d/xxxx.sql #复制提前准备好的sql文件进入容器，容器在初始化的时候会执行对应的sql\n\n# 定义环境变量\nENV MYSQL_USER=\"xxxx\" #创建用户，一般我项目部署的话不会使用root用户，会单独创建一个用户，该用户只有该库的所有权限\nENV MYSQL_PASSWORD=\"xxxxx\" #创建用户的密码\nENV MYSQL_ROOT_PASSWORD=\"xxxx\" # root用户的密码\n\n# 官方文档\n# https://hubgw.docker.com/_/mysql#:~:text=Initializing%20a%20fresh,the%20MYSQL_DATABASE%20variable.\n# https://hubgw.docker.com/_/mysql#:~:text=MYSQL_DATABASE,to%20this%20database.\n# ENV MYSQL_DATABASE=\"xxxx\" 可以创建指定名称的数据库，并且在执行时sql时会默认使用该数据库\n\nEXPOSE 3306\n```\nmy.cnf文件\n```conf\n[mysqld]\ndefault-time_zone = '+8:00'\ncharacter-set-server=utf8mb4\ncollation-server=utf8mb4_general_ci\n\n[client]\ndefault-character-set=utf8mb4\n```\nsql文件\n```sql\nCREATE DATABASE IF NOT EXISTS xxxx  DEFAULT CHARACTER SET utf8mb4  COLLATE utf8mb4_unicode_ci; //创建数据库\nFLUSH PRIVILEGES; \nuse xxxx;\nSET FOREIGN_KEY_CHECKS=0;\n-- ----------------------------\n-- Table structure for xxx\n-- ----------------------------\nDROP TABLE IF EXISTS `xxx`;\nCREATE TABLE `xxx` (\n  `xxxx` int(11) NOT NULL AUTO_INCREMENT,\n  `xx` int(11) DEFAULT NULL COMMENT '中文',\n  `xxx` varchar(11) DEFAULT NULL COMMENT '中文',\n  `xxx` decimal(10,0) DEFAULT NULL COMMENT '中文',\n  PRIMARY KEY (`xxxx`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n\n.........\n```\nok，所有配置文件编写好了，可以进行部署了。\n\n## 部署\n\n使用 `docker composer up -d --build` 来构建和部署项目，部署完成访问`127.0.0.1:4416`，ok访问成功，查看容器的数据库也看到有对应的表了\n\n# 问题\n\n虽然部署成功，但是还存在着一些问题 \n\n## api \n\n![php-api](.\\2.png)\n\n我的api接口为 `/public/index.php/xxx/xxx` 太长了，我想使用 `/api/xxx` 来代替。那么如何进行替代了，肯定是使用`nginx`进行配置\n1. 首先，先在前端的 `.env.production` 文件中修改 `API` 参数 并且进行打包\n\n```js\n# just a flag\nENV = 'production'\n\n# base api\nVUE_APP_BASE_API = '/public/index.php' -> '/api'\nVUE_APP_UPLOAD_API = '/public/index.php/admin/user/Upload' -> '/api/admin/user/Upload'\nVUE_APP_DOWNLOAD_API = '/public/index.php/admin/user/download' -> '/api/admin/user/download'\n\n```\n2. 修改`nginx`配置文件中处理php请求的部分\n```conf \n    location ~ ^/api/(.*)$ { # (.*)$ 把 /api/abc/xxx 中的/abc/xxx变成 $1\n        root           root;\n        fastcgi_pass   php_back:9000; \n        fastcgi_index  index.php;\n        fastcgi_param  SCRIPT_FILENAME  /var/www/html/public/index.php;\n        fastcgi_param  PATH_INFO /$1; #设置 PATH_INFO 定义thinkphp6的路由\n        include        fastcgi_params;\n    }\n```\n3. 就可以成功访问了\n\n![api](.\\3.png)\n\n\n这里有一个问题为什么 把路由设置到 `PATH_INFO` 就可以成功访问，因为thinkphp支持通过`PATH_INFO`来访问 [文档](https://static.kancloud.cn/manual/thinkphp/1697#:~:text=%E8%BF%99%E7%A7%8DURL%E6%A8%A1%E5%BC%8F,3)\n\n## mysql扩展\n当访问数据库的api时发现如下问题\n\n![api-mysql](.\\4.png)\n\n通过查询pdo发现还需要安装驱动 `PDO_MYSQL` \n\n![PDO_MYSQL](.\\5.png)\n\n在php的Dockefile修改为\n```yaml\n......\n\nRUN curl -sSLf \\\n        -o /usr/local/bin/install-php-extensions \\\n        https://github.com/mlocati/docker-php-extension-installer/releases/latest/download/install-php-extensions && \\\n    chmod +x /usr/local/bin/install-php-extensions && \\\n    install-php-extensions gd zip PDO_MYSQL #\n    \n......\n```\n## session\n当所有都安装好了，但是当要用到验证码进行登陆的时候，却一直显示验证码错误？由于那个验证码使用`topthink/think-captcha`来生成的，所有直接点开对应的源码看看是如何生成验证码的\n\n首先是验证码的生成，我发现里面调用了`session`方法把验证码的值放到了`session`里面\n\n![session1](.\\6.png)\n\n在验证验证码的时候，就是从`session`里面获取key来进行验证的\n\n![session2](.\\7.png)\n\n而`session`在这个项目中是以文件的形式保存在 `runtime` 目录里面的，所以我进入了php的容器，查看`runtime`目录发现什么文件都没有生成，甚至log文件都没有生成\n```sh\n/var/www/html # ls ./runtime/\n/var/www/html #\n```\n所以很有可能就是权限有问题，所以在php的Dockerfile中，把`runtime`目录的权限改成`777` \n```yaml\n......\nRUN mv \"$PHP_INI_DIR/php.ini-production\" \"$PHP_INI_DIR/php.ini\" && chmod 777 /var/www/html/runtime    \n......\n```\n改了以后问题就顺利解决了，对应的访问log也有了\n```sh\n/var/www/html # ls ./runtime/*\n./runtime/log:\n202404\n\n./runtime/session:\nsess_3d7d70488b437c484f150d574558688f\n```\n\n至此，顺利通过 `docker-compose` 部署了一个php的web项目 ","tags":["web","docker-compose"]},{"title":"valgrind进行性能分析","url":"/2024/03/05/使用valgrind进行性能分析/","content":"\n> 最近写了一个[c++项目](https://github.com/kehaha-5/udp-transfiler)来学习，在完成后，我想利用 `valgrind` 对服务端和客户端进行性能分析\n\n# 前言\n\n客户端和服务端都在本地运行\n\n在未开始优化时，客户端下载速度\n\n![speed1](./speed1.png)\n速度只有13MB\n\n\n# 工具\n\n本次性能分析利用的是valgrind的[callgrind](https://valgrind.org/docs/manual/cl-manual.html#cl-manual.options)工具\n```bash\n# 分析命令\n valgrind --tool=callgrind --callgrind-out-file=./valgrindCheck/callgrind/client --separate-threads=yes ./build/TRANFILER_CLIENT #客户端\n valgrind --tool=callgrind --callgrind-out-file=./valgrindCheck/callgrind/server --separate-threads=yes ./build/TRANFILER_SERVER #服务端\n#--separate-threads=<no|yes> [default: no]\n# This option specifies whether profile data should be generated separately for every thread. If yes, the file names get \"-threadID\" appended.\n```\n结果分析用的是 [kcachegrind](https://kcachegrind.github.io)\n\n# 测试过程\n客户端和服务端分别执行以上命令，然后客户端下载一个文件(大约200M)查看结果（客户端和服务端打包的都是release版本）\n\n# 分析\n\n## 客户端\n因为是要对下载性能进行检测，因此我挑了下载代码来进行分析\n\n![客户端-下载](./speed1-client-analysis-sendMsg.png)\n上图是客户端的发送消息的代码，从图中可以分析出来 `google::protobuf` 使用了一半的cpu，另一半是被 `setAck()`使用了\n\n![客户端-下载](./speed1-client-analysis-recv.png)\n\n![客户端-下载](./speed1-client-analysis-recv2.png)\n\n上面两张图都是客户端的接收数据的代码，从图中可以分析出来 `google::protobuf` 使用了快80%的cpu \n\n## 服务端\n![服务端-下载](./speed1-server-analysis-1.png)\n从图中可以看出，服务端中获取文件内容 `file::server` 使用了1.63%的cpu，而处理`google::protobuf`使用了 17.26%\n\n## 结论\n因此得出导致性能问题的原因大概率是`google::protobuf`，打开proto文件\n```proto\nsyntax = \"proto2\";\n\npackage msg.proto;\n\noption optimize_for = CODE_SIZE;\n\nmessage FileDownMsg{\n    required string hash = 1;\n    required uint64 startPos =2;\n    required uint64 dataIndex = 3;\n    optional int64 size = 4;\n    optional bytes data = 5;\n}\n```\n发现了问题：optimize_for用的是CODE_SIZE ~~为什么当初会用这个，我也不知道🤡~~，只要把optimize_for删除了默认使用的就是SPEED [文档](https://protobuf.dev/programming-guides/proto2/#:~:text=optimize_for%20(file%20option,full%20Message%20interface.)\n\n# 再次测试\n![speed2](./speed2.png)\n速度达到23MB，提高了75% 🤞\n\n## 客户端\n![speed2-客户端-下载](./speed2-client-recv.png)\n\n![speed2-客户端-下载](./speed2-client-sendMsg.png)\n\n由上面两幅图可以知道，经过优化后，接收数据中的 `google::protobuf` 的性能使用跟 `recvfrom` 的使用几乎一样了，发送端更是看不到了`google::protobuf`性能统计\n\n## 服务端\n![speed2-服务端-下载](./speed2-server-analysis-1.png)\n由上图也知服务端的`google::protobuf` 的性能使用也明显下降了\n\n# 还能再优化吗\n\n## 第二次分析\n但是，在服务端中的处理请求中为什么只使用了26%左右的cpu，那其他cpu性能去哪里捏 🤔\n\n![speed2-服务端-下载](./speed2-server-analysis-2.png)\n\n通过查看整个调用栈我发现了 `log` 函数使用了40%的cpu，尤其记录log时获取的时间函数\n\n## 获取时间\n获取时间的代码如下\n```cpp\nstd::string Log::getCurrTime() {\n    std::time_t t = std::time(NULL);\n    std::string mbstr(50, 0);\n    std::strftime(&mbstr[0], mbstr.size(), \"%F-%T\", std::localtime(&t));\n    return mbstr;\n}\n```\n那么这段代码里面有什么性能问题捏，可以在kcachegrind中点击`log`函数进行查看\n\n![speed2-服务端-下载](./speed2-server-analysis-3.png)\n\n发现了就是 `strftime` 和 `localtime`，那为什么它们执行怎么慢捏，因为它们都进行了`syscall`（详情请看[strftime](https://stackoverflow.com/questions/8174147/strftime-performance-vs-snprintf#:~:text=POSIX%20requires%20strftime%20to%20call%20tzset()%20(or%20act%20as%20if%20it%20did)%2C%20which%20on%20a%20linux%20system%20will%20likely%20stat%20/etc/timezone%20and%20other%20files%2C%20which%20is%20slow%20(compared%20to%20snprintf).%20Setting%20the%20TZ%20environment%20variable%20will%20generally%20give%20it%20a%20great%20boost.),[localtime](https://en.cppreference.com/w/cpp/chrono/c/localtime#:~:text=Notes-,This%20function%20may%20not%20be%20thread%2Dsafe.,.,-Example)）\n那如何进行修改\n第一种方法是：修改代码，代码参考[这里](https://gist.github.com/felipou/ad107bbb3a91814679beb22c0686fbeb)\n第二种方法是：~~只生产 timeval，在真正进行写入的时候在转换成日期（log线程）但是既然都进行了优化了，肯定是选择第二个方案，并且采用参考代码里面的func3~~（我都不显示正常请求的log了，详细请看后文）\n\n## 再次测试 🤡\n\n![speed3](./speed3.png)\n\n??? 为什么速度基本上跟上次一样，速速打开kcachegrind进行分析 🤺🤺🤺\n\n## 第三次分分析\n\n![speed2-服务端-下载](./speed3-server-analysis-1.png)\n\n可以看到`log`的cpu占用确实变少了，但还是占用了27%的cpu，所以不如点进去仔细分析一下那行代码占用大量cpu\n\n![speed2-服务端-下载](./speed3-server-analysis-2.png)\n进入`log`后，可以看到在`Callee Map`中，最大的两块分别是 `sprintf` `vsprintf`\n\n![speed2-服务端-下载](./speed3-server-analysis-3.png)\n查看到对应的代码就是输出时的格式化（要在debug模式下才可以看到源码），那这个真的没有办法优化了吗，其实还是有的，就是关闭正常请求时的log显示。因为一个请求就会输出两个log信息\n```bash\n2024-03-xx 09:03:44.975857 INFO 63 | udp recvfrom data to ip 127.0.0.1 prot 35512 ack is 1315955221\n2024-03-xx 09:03:44.975940 INFO 63 | udp recvfrom data to ip 127.0.0.1 prot 35512 ack is 508775843\n2024-03-xx 09:03:44.975978 INFO 90 | udp send data to ip 127.0.0.1 prot 35512 ack is 3372925139\n2024-03-xx 09:03:44.976044 INFO 90 | udp send data to ip 127.0.0.1 prot 35512 ack is 508775843\n```\n而在下载文件的时候一秒钟会传输大量的数据包\n![speed2-服务端-下载](./speed3-server-analysis-4.png)\n这就会大量调用log函数，从而影响性能。但是为了调试方便，可以把这个是否显示正常请求的log写入到配置文件中，根据实际进行配置\n\n# 最终结果\n经过了上述`log`模块的修改，以下就是最终结果，从刚开始的13M提升到35M 🤞🤞\n![speed4](./speed4.png)\n","tags":["valgrind","性能"]},{"title":"valgrind检测内存泄漏","url":"/2024/03/04/使用valgrind检测内存泄漏/","content":"\n> 最近写了一个[c++项目](https://github.com/kehaha-5/udp-transfiler)来学习，发现客户端在多次下载文件后内存只增不减，因此想利用 `valgrind` 来对我写的客户端进行内存泄漏检测\n\n# 检测命令 && 工具\n\n## 检测命令\n\n我这次主要是使用 valgrind 中的 [massif](https://valgrind.org/docs/manual/ms-manual.html) 来进行内存泄漏检测\n\n```bash\nvalgrind --tool=massif --time-unit=ms --detailed-freq=1 --massif-out-file=./valgrindCheck/massif.out.client ./build/TRANFILER_CLIENT \n# --time-unit=<i|ms|B> [default: i] 可以说是记录的颗粒度，如果程序运行很快就用B\n# The time unit used for the profiling. There are three possibilities: instructions executed (i), which is good for most cases; real (wallclock) time (ms, i.e. milliseconds), which is sometimes useful; and bytes allocated/deallocated on the heap and/or stack (B), which is useful for very short-run programs, and for testing purposes, because it is the most reproducible across different machines.\n#--detailed-freq=<n> [default: 10] 每多少次间隔记录一次详细信息\n# Frequency of detailed snapshots. With --detailed-freq=1, every snapshot is detailed.\n```\n\n## 查看工具\n\n生成出来的`massif.out.client`文件，使用 [massif-visualizer](https://apps.kde.org/zh-cn/massif-visualizer/) 来查看\n\n# 测试过程\n客户端下载一个文件，然后再下载另一个，以此反复几次\n\n# 检测分析\n\n首先按照正常结构来说，客户端内存使用会在下载文件的时候上升(保存下载记录)，在下载完成后内存使用会下降\n\n### 分析结果\n![结果](./res.png)\n\n通过上图可以知道正常的内存使用都是在使用完后会被销毁的，即图中的有上下坡线的内存，而存在内存泄漏的内存的内存就只有不断的上升\n\n![分析-1](./analysis-1.png)\n\n打开一个进行详情的分析\n1. 程序中用的是`protobuf`进行消息序列化的，因此带有`google::protobuf`的内存信息，可以通过在下载结束的析构函数里面添加 [google::protobuf::ShutdownProtobufLibrary()](https://protobuf.dev/getting-started/cpptutorial/#:~:text=Also%20notice%20the,clean%20up%20everything) 去解决 在本项目中只要主线程有下载或者发送数据就会用到protobuf所以就不做处理了\n2. 只有 `ack::AckSet::setCbByAck`是在程序中与 protobuf 不太相关的\n3. 虽然 `downfile::interruption` 跟 protobuf 相关但是这个是本次下载数据的临时保存，因此在下载完也应该被释放\n\n\n### 问题修复\n\n#### `ack::AckSet::setCbByAck`\n\n查看对应的代码\n```cpp\n//调用段代码\n_ackSetPtr->setCbByAck(ack, std::bind(&DownloaderEvents::timerExce, this, ack, resMsg));\n//对应方法\nvoid AckSet::setCbByAck(u_long& ack, Cb cb) {\n    std::lock_guard<std::mutex> lock_guard(_ackMsgMapLock);\n    auto it = _ackMsgMap.find(ack);\n    if (it == _ackMsgMap.end()) {\n        return;\n    }\n    it->second = _timerPtr->runEvery(config::ClientConfig::getInstance().getPacketsTimerOut(), cb);\n}\n//通过查看方法发现调用了添加定时器的代码\nuint Timer::runEvery(u_long timerout, TimerCb cb) {\n    TimerEvenSharedPtr even = std::make_shared<timerEven>();\n    even->timeout = timerout;\n    even->interval = true;\n    even->cd = cb;\n    runAfter(even);\n    uint index = _currTimerIndex++;\n    _allTimerEven[index] = even;\n    return index;\n}\n```\n其中 `std::unordered_map<unsigned int, std::shared_ptr<timer::timerEven>` 就是 `_allTimerEven` 一个用于保存当前定时器中所有定时任务的成员对象，但是我在一定条件下会取消该定时任务，我查找到了对应的代码\n```cpp\nvoid cancelTimerEven(uint index) { _allTimerEven[index].reset(); };\n//通过释放shared_ptr 可以导致在获取超时even(weak_ptr)时 无法通过 lock() 获取其对象 从而得出该定时事件已经被取消\n```\n\n是的，在这个方法里面只有释放了该定时器对应的 `TimerEvenSharedPtr even` 的指针，没有释放对应 `index` 的下标的空间，修改后的代码\n```cpp\nvoid cancelTimerEven(uint index) {\n    _allTimerEven[index].reset();\n    _allTimerEven.erase(index);\n};\n```\n\n#### `downfile::interruption`\n\n这个是用于保存 `protobuf` 的数据，其中 `SingleBlockInfo` 是一个 `repeated` 类型的数据，因此可以用只能指针的方式，在下载结束时条用其`clear()`和`reset()`方法来释放内存\n```cpp\n//下载结束要判断是否下载完成，未完成则保存中断数据到文件，完成的就刷新出正常的文件名出来\nvoid Downloader::flushAllInterruptionData() {\n    for (auto &it : _downfileInterruptionInfos) {\n        if (!it.second->isfinish()) {\n            flushInterruptionData(it.first);\n        } else {\n            delFlushInterruptionFile(it.first);\n        }\n        it.second->clear_info();\n        it.second.reset();\n    }\n    _downfileInterruptionInfos.clear();\n}\n```\n\n## 重新测试\n\n经过上述的代码修改后，重新运行了一下程序，得到了以下结果\n\n![结果-2](./res2.png)\n\n当观察内存详细时发现 `ack::AckSet::setCbByAck` 已经不见了，也就是说内存被正常释放了\n\n![分析-2-1](./analysis-2-1.png)\n\n其中的`downfile::interruption`在一次下载完成后也出现了明显的内存使用下降\n\n![分析-2-2](./analysis-2-2.png)\n\n![分析-2-3](./analysis-2-3.png)\n\n内存泄漏分析至此就完成了 ✌✌✌","tags":["valgrind","内存泄漏"]}]