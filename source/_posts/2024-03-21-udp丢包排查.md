---
title: udp丢包排查
date: 2024-03-21 21:50:17
tags: [c++,linux,udp]
---



> 最近在写项目的时候，发现了udp丢包情况，特此记录一下



# 问题描述

- 在使用**本地网卡**发送多个udp包时，会出现丢包问题
  - 客户端发送1000个udp包
  - 服务端只收到900多个包

# 问题复现

我写了 server.cc 和 client.cc 来复现这个问题

[/udp/server.cc](https://github.com/kehaha-5/kehaha-5.github.io/blob/main/source/_posts/2024-03-21-udp%E4%B8%A2%E5%8C%85%E6%8E%92%E6%9F%A5/udp_drop_packets_test/udp/server.cc) 


[/udp/client.cc](https://github.com/kehaha-5/kehaha-5.github.io/blob/main/source/_posts/2024-03-21-udp%E4%B8%A2%E5%8C%85%E6%8E%92%E6%9F%A5/udp_drop_packets_test/udp/client.cc)

通过上述代码可知道，客户端将会每秒发送一定数量的udp包给服务端，服务端就是每秒显示它所接收到的包数量

# 测试结果

![SER](.\1-1.png)

![CLI](.\1-2.png)

# 问题分析

我们从上图中可以看到当客户端每秒发送1000个udp包时，服务端只接受到了不到1000，那剩下没有接收到的包去哪里了🤔，这时我们可以利用`sar`来进行网络分析

![CLI](.\1-3.png)

我们可以看到 ` odgm/s` 跟我们client每秒发送的包的数量一致，`idgm/s`表示的时每秒送达的包数量，但是不跟`odgm/s`相同，而没有收到的包数量刚好等于` idgmerr/s` 那么这个` idgmerr/s`所表示的是什么捏，通过查询发现解释如下 [查看]( https://www.man7.org/linux/man-pages/man1/sar.1.html#:~:text=idgmerr/s%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20The%20number%20of%20received%20UDP%20datagrams%20per%20second%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20that%20could%20not%20be%20delivered%20for%20reasons%20other%20than%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20the%20lack%20of%20an%20application%20at%20the%20destination%20port%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5BudpInErrors%5D.)

> idgmerr/s
> The number of received UDP datagrams per second that could not be delivered for reasons other than the lack of an application at the destination port [udpInErrors].

但是udp丢包的原因有很多

- 网卡驱动丢包
- 内核缓冲区丢包
- 性能不足导致丢包
  - cpu满载
  - free mem太少
  - ....

为了更近一步地分析是什么原因导致的丢包，利用 `netstat ` 进行分析

## 网卡驱动丢包

利用`netstat -i`看查看网卡信息

```text
Kernel Interface table
Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0      1432      517      0      0 0           378      0      0      0 BMRU
lo       65536    48420      0      0 0         48420      0      0      0 LRU
```

我们是在本地进行测试的，也就是说用的是本地网卡(lo)，但是通过查看发现它们的`RX-ERR` ` RX-DRP` `RX-OVR` 都为0，即网卡驱动没有丢包

## 内核缓冲区丢包

利用`netstat -su`来查看信息

- 在开始测试前记录一次

  ```text
  Udp:
      18 packets received
      0 packets to unknown port received
      0 packet receive errors
      18 packets sent
      0 receive buffer errors
      0 send buffer errors
  UdpLite:
  IpExt:
      InOctets: 25796930
      OutOctets: 25722753
      InNoECTPkts: 1414
  ```

- 在执行测试后再查看信息

  ```text
  IcmpMsg:
      InType3: 1
      OutType3: 1
  Udp:
      11257 packets received
      1 packets to unknown port received
      373 packet receive errors
      11631 packets sent
      373 receive buffer errors
      0 send buffer errors
      IgnoredMulti: 1
  UdpLite:
  IpExt:
      InBcastPkts: 1
      InOctets: 616541094
      OutOctets: 616446301
      InBcastOctets: 229
      InNoECTPkts: 17160
  ```

- 再结合`sar` 的信息进行查看

  ```text
  Linux 5.15.133.1-microsoft-standard-WSL2 (DESKTOP-4GO6ORF)      03/22/24        _x86_64_        (8 CPU)
  
  10:56:08       idgm/s    odgm/s  noport/s idgmerr/s
  10:56:09         0.00      0.00      0.00      0.00
  10:56:10         0.00      0.00      0.00      0.00
  10:56:11       993.00   1000.00      0.00      7.00
  10:56:12       926.00   1000.00      0.00     74.00
  10:56:13       931.00   1000.00      0.00     69.00
  10:56:14       943.00   1000.00      0.00     57.00
  10:56:15       980.00   1000.00      0.00     20.00
  10:56:16       983.00   1002.00      0.00     19.00
  10:56:17       981.00   1000.00      0.00     19.00
  10:56:18       957.00   1000.00      0.00     43.00
  10:56:19       935.00   1000.00      0.00     65.00
  10:56:20         0.00      0.00      0.00      0.00
  10:56:21         0.00      0.00      0.00      0.00
  ```

从上述中，发现 `packet receive errors`  统计到的丢包数量跟   `idgmerr/s` 的数量差不多，因此可以确定丢包的[原因](https://linux-tips.com/t/udp-packet-drops-and-packet-receive-error-difference/237#:~:text=There%20are%2038491,or%20kernel%20side)

- udp packet header corruption or checksum problems
- packet receive buffer problems in application or kernel side

在看到`receive buffer errors`的数量和 `packet receive errors`一致可以确定丢包的原因就是 packet receive buffer problems in application or kernel side

## 性能不足导致丢包

在测试时执行`sar`命令查看cpu 和 内存使用情况

```text
sar -u 1
Linux 5.15.133.1-microsoft-standard-WSL2 (DESKTOP-4GO6ORF)      03/22/24        _x86_64_        (8 CPU)

14:38:19        CPU     %user     %nice   %system   %iowait    %steal     %idle
14:38:20        all      0.37      0.00      0.87      0.00      0.00     98.75
14:38:21        all      0.25      0.00      0.25      0.00      0.00     99.50
14:38:22        all      1.38      0.00      1.63      0.00      0.00     96.99
14:38:23        all      0.25      0.00      0.87      0.12      0.00     98.75
14:38:24        all      0.38      0.00      0.63      0.00      0.00     99.00
14:38:25        all      0.25      0.00      0.50      0.00      0.00     99.25
14:38:26        all      0.25      0.00      1.12      0.00      0.00     98.63
14:38:27        all      0.25      0.00      0.25      0.00      0.00     99.50
14:38:28        all      0.37      0.00      1.12      0.00      0.00     98.50
14:38:29        all      0.25      0.00      0.25      0.00      0.00     99.50
14:38:30        all      0.63      0.00      0.38      0.00      0.00     99.00
14:38:31        all      0.13      0.00      0.13      0.00      0.00     99.75
14:38:32        all      0.25      0.00      0.75      0.00      0.00     99.00
14:38:33        all      0.12      0.00      0.25      0.00      0.00     99.62
14:38:34        all      0.63      0.00      0.38      0.25      0.00     98.75
14:38:35        all      0.13      0.00      0.25      0.00      0.00     99.62
14:38:36        all      0.25      0.00      1.12      0.00      0.00     98.62
14:38:37        all      0.25      0.00      0.00      0.00      0.00     99.75
14:38:38        all      0.37      0.00      0.62      0.00      0.00     99.00
14:38:39        all      0.00      0.00      0.13      0.88      0.00     99.00
^C
Average:        all      0.34      0.00      0.58      0.06      0.00     99.02
```

```text
sar -r 1 -h
Linux 5.15.133.1-microsoft-standard-WSL2 (DESKTOP-4GO6ORF)      03/22/24        _x86_64_        (8 CPU)

14:38:20    kbmemfree   kbavail kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty
14:38:21         3.9G      4.4G      1.1G     19.4%     52.5M    688.2M      1.4G     21.0%    286.4M      1.3G      0.0k
14:38:22         3.9G      4.4G      1.1G     19.4%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G      0.0k
14:38:23         3.9G      4.4G      1.1G     19.4%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G      4.0k
14:38:24         3.9G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     16.0k
14:38:25         3.9G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G    108.0k
14:38:26         3.9G      4.4G      1.1G     19.4%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G      0.0k
14:38:27         3.9G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G      0.0k
14:38:28         3.8G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G      0.0k
14:38:29         3.9G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     16.0k
14:38:30         3.9G      4.4G      1.1G     19.4%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     16.0k
14:38:31         3.9G      4.4G      1.1G     19.4%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     16.0k
14:38:32         3.9G      4.4G      1.1G     19.4%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     16.0k
14:38:33         3.9G      4.4G      1.1G     19.4%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     16.0k
14:38:34         3.8G      4.4G      1.1G     19.6%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     16.0k
14:38:35         3.9G      4.4G      1.1G     19.4%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     16.0k
14:38:36         3.9G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     20.0k
14:38:37         3.8G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     20.8%    286.5M      1.3G     20.0k
14:38:38         3.9G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     20.7%    286.5M      1.3G     96.0k
14:38:39         3.8G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     20.7%    286.5M      1.3G     96.0k
14:38:40         3.8G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     20.7%    286.5M      1.3G     96.0k
^C

Average:         3.9G      4.4G      1.1G     19.5%     52.5M    688.2M      1.4G     21.0%    286.5M      1.3G     28.4k
```

通过上面查看并没有什么问题，因此性能问题被排除

# 解决方法

## 增加缓冲区

在查询`socket`时，发现它有一个option [SO_RCVBUF](https://www.man7.org/linux/man-pages/man7/socket.7.html#:~:text=SO_RCVBUF%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20Sets%20or,option%20is%20256.)

> ```
>  SO_RCVBUF
>               Sets or gets the maximum socket receive buffer in bytes.
>               The kernel doubles this value (to allow space for
>               bookkeeping overhead) when it is set using setsockopt(2),
>               and this doubled value is returned by getsockopt(2).  The
>               default value is set by the
>               /proc/sys/net/core/rmem_default file, and the maximum
>               allowed value is set by the /proc/sys/net/core/rmem_max
>               file.  The minimum (doubled) value for this option is 256.
> ```

也就是说我们可以通过修改这个值去增大接收端的buff，但是最大值不能超过 `/proc/sys/net/core/rmem_max`，当然我们也是可以通过命令来修改`/proc/sys/net/core/rmem_max`的值

```bash
sysctl -w net.core.rmem_max=26214400 # 设置为 25M
```

若觉得要用命令修改太麻烦我们也可以使用 [SO_RCVBUFFORCE](https://www.man7.org/linux/man-pages/man7/socket.7.html#:~:text=SO_RCVBUFFORCE%20(since%20Linux%202.6.14)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20Using%20this%20socket%20option%2C%20a%20privileged%20(CAP_NET_ADMIN)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20process%20can%20perform%20the%20same%20task%20as%20SO_RCVBUF%2C%20but%20the%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20rmem_max%20limit%20can%20be%20overridden.) 需要root权限执行

> ```text
> SO_RCVBUFFORCE (since Linux 2.6.14)
>        Using this socket option, a privileged (CAP_NET_ADMIN)
>        process can perform the same task as SO_RCVBUF, but the
>        rmem_max limit can be overridden.
> ```

我这里通过在代码中使用`setsockopt()`和`SO_RCVBUFFORCE`增加程序中socket的接收缓冲区大小 

[/udp/server_increase_recv_buff.cc](https://github.com/kehaha-5/kehaha-5.github.io/blob/main/source/_posts/2024-03-21-udp%E4%B8%A2%E5%8C%85%E6%8E%92%E6%9F%A5/udp_drop_packets_test/udp/server_increase_recv_buff.cc)

```c++
  int socket_opt_recv_buff = 0;
  socklen_t socket_opt_recv_buff_len = sizeof(socket_opt_recv_buff);

  if (getsockopt(socketfd, SOL_SOCKET, SO_RCVBUF, &socket_opt_recv_buff,
                 &socket_opt_recv_buff_len) == -1) {
    perror("getsockopt");
    exit(EXIT_FAILURE);
  }
```



# 再次测试

```text
kehaha@DESKTOP-4GO6ORF:~/projects/study/blogsDemos/epoll_read_even_and_sendto$ sudo ./build/SER_INC 134217728 #128M
setsocket opt recvbuff :134217728
socket recvbuff :268435456
udp bind is listenning ... port 8888
hasRevc:0
hasRevc:0
hasRevc:10000
hasRevc:20000
hasRevc:30000
hasRevc:40000
hasRevc:50000
hasRevc:60000
hasRevc:70000
hasRevc:80000
hasRevc:90000
hasRevc:100000
hasRevc:110000
hasRevc:120000
hasRevc:130000
hasRevc:140000
```

```text
kehaha@DESKTOP-4GO6ORF:~/projects/study/blogsDemos/epoll_read_even_and_sendto$ ./build/CLI 10000 # 1000*64kb ~ 62.5M
hasSendPacketsNum:10000
hasSendPacketsNum:20000
hasSendPacketsNum:30000
hasSendPacketsNum:40000
hasSendPacketsNum:50000
hasSendPacketsNum:60000
hasSendPacketsNum:70000
hasSendPacketsNum:80000
hasSendPacketsNum:90000
hasSendPacketsNum:100000
hasSendPacketsNum:110000
hasSendPacketsNum:120000
hasSendPacketsNum:130000
```

```text
kehaha@DESKTOP-4GO6ORF:~$ sudo sar -n UDP 1
Linux 5.15.133.1-microsoft-standard-WSL2 (DESKTOP-4GO6ORF)      03/22/24        _x86_64_        (8 CPU)

14:54:45       idgm/s    odgm/s  noport/s idgmerr/s
14:54:46     10000.00  10000.00      0.00      0.00
14:54:47     10000.00  10000.00      0.00      0.00
14:54:48     10000.00  10000.00      0.00      0.00
14:54:49     10000.00  10000.00      0.00      0.00
14:54:50     10000.00  10000.00      0.00      0.00
14:54:51     10000.00  10000.00      0.00      0.00
14:54:52     10000.00  10000.00      0.00      0.00
14:54:53     10000.00  10000.00      0.00      0.00
14:54:54     10000.00  10000.00      0.00      0.00
14:54:55     10000.00  10000.00      0.00      0.00
14:54:56     10000.00  10000.00      0.00      0.00
14:54:57     10000.00  10000.00      0.00      0.00
14:54:58         0.00      0.00      0.00      0.00
14:54:59         0.00      0.00      0.00      0.00
^C

Average:      8571.43   8571.43      0.00      0.00
```

通过上述观察，这个丢包问题就被解决了✌

# 总结

## 扩展

- 我这次主要问题是接收端的buff不足导致丢包，如果是发送端可以使用以下方法解决

  - 修改 `/proc/sys/net/core/wmem_max` 和 `/proc/sys/net/core/wmem_default`

    ```bash
    sysctl -w net.core.wmem_max=26214400 # 设置为 25M
    ```

  - 使用`setsockopt()`

    - [SO_SNDBUF ](https://www.man7.org/linux/man-pages/man7/socket.7.html#:~:text=SO_SNDBUF%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20Sets%20or,option%20is%202048.)

       > ```text
        > SO_SNDBUF
        >               Sets or gets the maximum socket send buffer in bytes.  The
        >               kernel doubles this value (to allow space for bookkeeping
        >               overhead) when it is set using setsockopt(2), and this
        >               doubled value is returned by getsockopt(2).  The default
        >               value is set by the /proc/sys/net/core/wmem_default file
        >               and the maximum allowed value is set by the
        >               /proc/sys/net/core/wmem_max file.  The minimum (doubled)
        >               value for this option is 2048.
        > ```

    - [SO_SNDBUFFORCE ](https://www.man7.org/linux/man-pages/man7/socket.7.html#:~:text=SO_SNDBUFFORCE%20(since%20Linux%202.6.14)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20Using%20this%20socket%20option%2C%20a%20privileged%20(CAP_NET_ADMIN)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20process%20can%20perform%20the%20same%20task%20as%20SO_SNDBUF%2C%20but%20the%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20wmem_max%20limit%20can%20be%20overridden.)

       > ```text
        >        SO_SNDBUFFORCE (since Linux 2.6.14)
        >               Using this socket option, a privileged (CAP_NET_ADMIN)
        >               process can perform the same task as SO_SNDBUF, but the
        >               wmem_max limit can be overridden.
        > ```
    
  - 使用`epoll`函数去监听[POLLOUT](https://www.man7.org/linux/man-pages/man7/socket.7.html#:~:text=The%20user%20can%20then%20wait%20for%20various%20events%20via%0A%20%20%20%20%20%20%20poll(2)%20or%20select(2).)事件

    ```text
    The user can then wait for various events via poll(2) or select(2).
           ┌───────────────────────────────────────────────────────────────┐
           │                          I/O events                           │
           ├────────────┬───────────┬──────────────────────────────────────┤
    		.....
           ├────────────┼───────────┼──────────────────────────────────────┤
           │ Write      │ POLLOUT   │ Socket has enough send buffer space  │
           │            │           │ for writing new data.                │
           ├────────────┼───────────┼──────────────────────────────────────┤
    		.....
    ```

    

- 当然我在网上看其它文章里面也有写到可以通过增加 `net.core.netdev_max_backlog` 参数来解决，但是有些文章却不是这样认为 [日常排障：UDP 丢包问题 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/617397417)

## 思考🤔

为什么tcp下的链接就不会在出现服务器丢包捏

> [Unlike TCP, UDP protocol does not have built-in flow-control capabilities](https://linux-tips.com/t/udp-packet-drops-and-packet-receive-error-difference/237#:~:text=Unlike%20TCP%2C%20UDP%20protocol%20does%20not%20have%20built%2Din%20flow%2Dcontrol%20capabilities)

为了验证这个说法，我写了两个文件来测试

[/tcp/server.cc](https://github.com/kehaha-5/kehaha-5.github.io/blob/main/source/_posts/2024-03-21-udp%E4%B8%A2%E5%8C%85%E6%8E%92%E6%9F%A5/udp_drop_packets_test/tcp/server.cc)

[/tcp/client.cc](https://github.com/kehaha-5/kehaha-5.github.io/blob/main/source/_posts/2024-03-21-udp%E4%B8%A2%E5%8C%85%E6%8E%92%E6%9F%A5/udp_drop_packets_test/tcp/client.cc)

同样是客户端每秒发送指定包，最后客户端统计所有发送的数据大小，服务端统计接收到的数据大小

- 测试前统计网卡信息

  ```text
  kehaha@DESKTOP-4GO6ORF:~$ netstat -i
  Kernel Interface table
  Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
  eth0      1432      686      0      0 0           682      0      0      0 BMRU
  lo       65536   914976      0      0 0        914976      0      0      0 LRU
  ```

- 开始测试

  ```text
  kehaha@DESKTOP-4GO6ORF:~/projects/study/blogsDemos/udp_drop_packets_test$ ./build/TCP_SER
  tcp bind is listenning ... port 9999
  hasRevc:10016 hasRevcSize:655070000
  hasRevc:20075 hasRevcSize:1310140000
  hasRevc:30127 hasRevcSize:1965210000
  hasRevc:40184 hasRevcSize:2620280000
  hasRevc:50215 hasRevcSize:3275350000
  hasRevc:60246 hasRevcSize:3930420000
  hasRevc:70293 hasRevcSize:4585490000
  hasRevc:80335 hasRevcSize:5240560000
  hasRevc:90338 hasRevcSize:5895630000
  hasRevc:100351 hasRevcSize:6550700000
  ```

  ```text
  kehaha@DESKTOP-4GO6ORF:~/projects/study/blogsDemos/udp_drop_packets_test$ ./build/TCP_CLI 10000
  hasSendPacketsNum:10000
  hasSendPacketsNum:20000
  hasSendPacketsNum:30000
  hasSendPacketsNum:40000
  hasSendPacketsNum:50000
  hasSendPacketsNum:60000
  hasSendPacketsNum:70000
  hasSendPacketsNum:80000
  hasSendPacketsNum:90000
  hasSendPacketsNum:100000
  total hasSendPacketsNum:100000 hasSendSize:6550700000
  ```

- 查看网卡信息

  ```text
  kehaha@DESKTOP-4GO6ORF:~$ netstat -i
  Kernel Interface table
  Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
  eth0      1432      696      0      0 0           695      0      0      0 BMRU
  lo       65536  1139780      0      0 0       1139780      0      0      0 LRU
  ```

- 查看`sar信息`

  ```text
  kehaha@DESKTOP-4GO6ORF:~$ sar -n TCP 1
  Linux 5.15.133.1-microsoft-standard-WSL2 (DESKTOP-4GO6ORF)      03/22/24        _x86_64_        (8 CPU)
  
  17:11:11     active/s passive/s    iseg/s    oseg/s
  17:11:12         0.00      0.00     12.00     12.00
  17:11:13         0.00      0.00      6.00      6.00
  17:11:14         1.00      1.00  18462.00  18462.00 //开始传送数据
  17:11:15         0.00      0.00  17274.00  17274.00
  17:11:16         0.00      0.00  15761.00  15761.00
  17:11:17         0.00      0.00  24753.00  24753.00
  17:11:18         0.00      0.00  20243.00  20243.00
  17:11:19         0.00      0.00  15196.00  15196.00
  17:11:20         0.00      0.00  18340.00  18340.00
  17:11:21         0.00      0.00  19880.00  19880.00
  17:11:22         0.00      0.00  28062.00  28062.00
  17:11:23         0.00      0.00  18363.00  18363.00 //结束传送数据
  17:11:24         0.00      0.00     13.00     13.00
  17:11:25         0.00      0.00     10.00     10.00
  ^C
  
  Average:         0.07      0.07  14026.79  14026.79
  ```

  由此可知，即使client每秒发送10000个包，但是因为tcp拥有流控制和窗口滑动机制（根据可用的win size来调整每个包的大小），所以不像udp真就每秒发送10000个64k的包导致接收端的buff不足从而引起丢包




